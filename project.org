-*- mode: org; -*-

* Meta

#+TITLE: seh.pathway_hopping
#+AUTHOR: Samuel D. Lotz
#+EMAIL: samuel.lotz@salotz.info
#+STARTUP: overview inlineimages
#+TODO: TODO | INPROGRESS WAIT | DONE CANCELLED


* Scratch

For scratch work. Ephemeral.


** Debugging configurations for new dashboards

#+begin_src python

  from wepy.orchestration.orchestrator import Orchestrator

  with open('TaskMapper_CUDA_1-workers_lig-10.config.dill.pkl', 'rb') as rf:
      config = Orchestrator.deserialize(rf.read())

  dash_reporter = config.reporters[1]
#+end_src

* Configurations

The place to put all configuration type data that will go into
parameterizing other productions and analyses.


** Ligand Simulations

*** Docking models

Results and specification of which docking models were used for each
ligand.

docking_model_idx is base zero indexing

| ligand_id | docking_model_idx | starting snapshot                |
|-----------+-------------------+----------------------------------|
|         3 |                 8 | 3a3eed6bb613a042d089547bcab8b1e6 |
|        10 |                 0 | 6593f31b3db4442a0444c368157b2199 |
|        17 |                   | a1312cee6c9fdf771e110958bf0ed062 |
|        18 |                 0 | 6812d64700106bd832351b5e0a1ffa53 |
|        20 |                 0 | 2fcfc4f36e674ba42edfa71ced4a8595 |


Probably the starting snapshot doesn't make sense anymore.


*** Run Settings

The settings which will be applied to all generated runs from slurmify:

#+BEGIN_SRC toml :tangle hpcc/run_settings.toml
  # test run value
  #walltime = "4:00:00"
  walltime = "168:00:00"


  memory = "100gb"

  num_cpus = 4

  # test run value
  #num_gpus = 4
  num_gpus = 8

  constraint = "[intel18|intel16]"

#+END_SRC


*** epilog script

I am not using this anymore since all scripts will be done in place on
scratch now.

#+begin_src bash hpcc/scripts/epilog.sh
  #!/bin/sh -login

  echo ""
  echo "** Running epilog"
  echo ""

  echo "SLURM as epilog sees it"
  echo "SLURM_JOB_ID: ${SLURM_JOB_ID}"
  echo "SLURM_JOB_USER: ${SLURM_JOB_USER}"
  echo "SLURM_CLUSTER_NAME: ${SLURM_CLUSTER_NAME}"

  echo "globals from the main script:"
  echo "EXECDIR: ${EXECDIR}"
  echo "JOBDIR: ${JOBDIR}"
  echo "OUTPUTDIR: ${OUTPUTDIR}"
  echo "STDOUT: ${STDOUT}"
  echo "STDERR: ${STDERR}"

  echo "Copy the contents of the execdir to the job dir in the outputs subdir"

  echo "------------"
  echo "moving EXECDIR ${EXECDIR} to OUTPUTDIR ${OUTPUTDIR}"
  echo "------------"
  mv -f ${EXECDIR}/* ${OUTPUTDIR}
  echo ""


  # copy the logs to the outputdir
  echo "------------"
  echo "Copying logs to the outputdir"
  echo "------------"
  echo ""
  cp ${STDOUT} ${OUTPUTDIR}/
  cp ${STDERR} ${OUTPUTDIR}/

  # print to the main terminal that we are done
  echo "Done with job"
#+end_src



*** Context settings

Example:

#+BEGIN_SRC toml
  [context]

  # its okay to leave out the exec dir, but you can specify this and the
  #run will get executed here and then be moved back
  # exec_dir = "$SCRATCH/exec"


  # epilog is something that can be run after the run, if it is empty
  # here that is okay, this is really only important for moving data
  # back after being executed somewhere else
  epilog = ""

  # this section is for the setup of the run in the script
  [context.setup]

  # this is the parent dir for the jobs, there should be a directory
  # here called 'jobs' and each job will will get it's own unique
  # directory
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/3_simulations"

  # will load these modules from lmod i.e. 'module load <modname>'
  lmod_modules = ["GNU/4.8.3", "CUDA/9.2.88"]

  # everything in here is translated directly into environment
  # variables, e.g. "export ANACONDA_DIR=path/to/anaconda"
  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"


#+END_SRC



**** 3

#+BEGIN_SRC toml :tangle hpcc/simulations/3_simulations/context_settings.toml
  [context]

  # its okay to leave out the exec dir
  #exec_dir = "$SCRATCH/users/lotzsamu/jobs/seh.pathway_hopping"

  epilog = ""

  [context.setup]


  # home
  # goal_dir_path = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/3_simulations"

  # scratch
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/3_simulations"

  lmod_modules = ["GNU/4.8.3", "CUDA/9.2.88"]

  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"

#+END_SRC


**** 10

#+BEGIN_SRC toml :tangle hpcc/simulations/10_simulations/context_settings.toml
  [context]

  epilog = ""

  [context.setup]

  # HOME
  #goal_dir_path = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations"

  # scratch
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations"

  lmod_modules = ["GCC/8.3.0", "CUDA/10.1.243"]

  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"

#+END_SRC

**** 17

#+BEGIN_SRC toml :tangle hpcc/simulations/17_simulations/context_settings.toml
  [context]

  epilog = ""

  [context.setup]

  # HOME
  #goal_dir_path = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/17_simulations"

  # scratch
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/17_simulations"

  lmod_modules = ["GNU/4.8.3", "CUDA/9.2.88"]

  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"


#+END_SRC


**** 18

#+BEGIN_SRC toml :tangle hpcc/simulations/18_simulations/context_settings.toml
  [context]

  epilog = ""

  [context.setup]

  # HOME
  # goal_dir_path = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/18_simulations"

  # scratch
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/18_simulations"

  lmod_modules = ["GNU/4.8.3", "CUDA/9.2.88"]

  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"

#+END_SRC



**** 20

#+BEGIN_SRC toml :tangle hpcc/simulations/20_simulations/context_settings.toml
  [context]

  epilog = ""

  [context.setup]


  # HOME
  # goal_dir_path = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/20_simulations"

  # scratch
  goal_dir_path = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/20_simulations"

  lmod_modules = ["GNU/4.8.3", "CUDA/9.2.88"]

  [context.setup.env_vars]

  ANACONDA_DIR = "$HOME/.pyenv/versions/miniconda3-latest"
  
#+END_SRC


*** Task script templates

For our simulations we provide two task templates (using jinja) that
are parametrized for the specific run we are doing.

Keys are:
- num_gpus
- n_steps
- run_time
- checkpoint_freq
- orch_name
- config_name
- start_hash
- task_name
- conda_env
- log_level


Make sure yuo put the shebang in the options since if you don't it
will put a line before it at the top.

#+BEGIN_SRC bash :shebang "#!/bin/bash --login" :tangle hpcc/templates/production_template.sh.j2
  num_gpus={{ num_gpus }}
  n_steps={{ n_steps }}
  run_time={{ run_time }}
  checkpoint_freq={{ checkpoint_freq }}
  orch_name="{{ orch_name }}"
  config_name="{{ config_name }}"
  start_hash="{{ start_hash }}"
  log_level="{{ log_level }}"
  task_name="{{ task_name }}"

  scripts_dir="{{ scripts_dir }}"
  patches="{{ patches }}"

  # environment stuff
  conda_env="{{ conda_env }}"

  gnu_version="{{gnu_version}}"
  cuda_version="{{cuda_version}}"

  echo "Modules to start"
  module list

  echo "purging lmod modules"
  module purge

  echo "modules after purge"
  module list

  echo "loading specified lmod modules"
  echo "GNU: $gnu_version"
  echo "CUDA: $cuda_version"

  module load $gnu_version
  module load $cuda_version

  module list

  # functions for exiting on errors
  yell() { echo "$0: $*" >&2; }
  die() { yell "$*"; exit 1; }
  try() { "$@" || die "cannot $*"; }

  echo "----------------------"
  echo "Running on host: $(hostname)"
  echo "----------------------"




  # use the ANACONDA_DIR which is set in the environment variables in
  # the setup to setup this shell for anaconda
  . ${ANACONDA_DIR}/etc/profile.d/conda.sh

  echo "DEBUG: before activating the simulation env:"
  echo "DEBUG: current 'which python' is:"
  echo $(which python)
  echo ""

  echo "DEBUG: current 'pyenv which python' is:"
  echo $(pyenv which python)
  echo ""

  echo "DEBUG: current 'pyenv which conda' is:"
  echo $(pyenv which conda)
  echo ""


  echo "DEBUG: Trying to activate env: ${conda_env}"
  echo ""

  # activate the proper virtualenv
  try conda activate ${conda_env}

  echo "DEBUG: after activating the simulation env."
  echo "DEBUG: current 'which python' is:"
  echo $(which python)
  echo ""

  echo "DEBUG: current 'pyenv which python' is:"
  echo $(pyenv which python)
  echo ""

  echo "Testing OpenMM Installation"
  python -m simtk.testInstallation


  echo ""
  echo "Testing wepy installation"

  echo "which wepy"
  echo $(which wepy)

  echo ""

  start_snapshot_name="${start_hash}.snap.dill.pkl"

  # grab the snapshot out of the master orchestrator, so we have it for
  # provenance sake
  try wepy get snapshot -O "${start_snapshot_name}" "${orch_name}" "${start_hash}"

  # then copy it and rename it so it doesn't have to get overwritten
  start_snapshot_backup="${start_snapshot_name}.backup"

  try cp $start_snapshot_name $start_snapshot_backup

  # apply patches to snapshot
  echo "Running patch script for the patches ${patches[@]}"
  try python $scripts_dir/patch_snapshot.py $start_snapshot_name $start_snapshot_name ${patches[@]}
  echo "Finished with patch script"


  ## for launching parallel exporters

  # only launch right before the main job so we don't have to clean up
  # for all the other commands I run above

  # this helps us teardown these things upon exit
  trap teardown SIGINT

  teardown () {
      echo "caught signal tearing processes down"

      kill "$node_exporter_pid"
      echo "Killed node_exporter proc"

      kill "$nvidia_exporter_pid"
      echo "Killed nvidia_exporter proc"

      exit 1
  }

  # run the node exporters

  # TODO get the right path to the exporter binaries

  ./node_exporter &
  node_exporter_pid="$(pidof node_exporter)"
  echo "Started the node_exporter as PID: $node_exporter_pid"

  ./nvidia_gpu_prometheus_exporter &
  nvidia_exporter_pid="$(pidof nvidia_gpu_prometheus_exporter)"
  echo "Started the nvidia_gpu_prometheus_exporter as PID: $nvidia_exporter_pid"

  ### run main job

  echo "tagging as ${task_name}"

  # run the simulation from that snapshot
  wepy run snapshot \
       --tag ${task_name} \
       --job-name ${task_name} \
       --n-workers ${num_gpus} \
       --log ${log_level} \
       --checkpoint-freq ${checkpoint_freq} \
       ${start_snapshot_name} \
       ${config_name} \
       ${run_time} ${n_steps} || teardown
#+END_SRC


*** Run Specs

***** 3

#+BEGIN_SRC toml :tangle hpcc/simulations/3_simulations/run_spec.toml
  [defaults]

  num_gpus = 8
  n_steps = 10000
  run_time = 590400
  checkpoint_freq = 25
  orch_name = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/3_simulations/orchs/master_sEH_lig-3.orch.sqlite"
  config_name = "WorkerMapper_CUDA_1-workers_lig-10.config.dill.pkl"
  log_level = "DEBUG"

  # the root hash is the default
  start_hash = "8e158c28c8c426f064d032823bb9aaf9"

  # this really should have been deprecated since there is only one run
  # type now, but we keep it just as this for all of them
  task_type="production"

  # the specific anaconda environment to use
  conda_env = "seh.pathway_hopping.sims"

  scripts_dir = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/scripts"

  # patches to be applied at runtime, can be overridden at the run
  # definition
  patches = 'OpenMMRunner.getState_kwargs UnbindingBC._mdj_top'

  # array of tables where each one is a specific segment; the index of
  # the table will be the uniquely identifying run index


  # any value from the defaults can be overridden inside

  [[runs]]

  task_name = "lig-3_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/3_simulations/orchs/master_sEH_lig-3.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2

  [[runs]]

  task_name = "lig-3_test-local-hpcc-scratch"
  num_gpus = 8
  n_steps = 1000
  run_time = 3600
  checkpoint_freq = 2

  [[runs]]

  task_name = "lig-3_test-submit"
  num_gpus = 4
  n_steps = 10000
  run_time = 3600
  checkpoint_freq = 2



  [[runs]]

  task_name = "lig-3_contig-0-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"

  [[runs]]

  task_name = "lig-3_contig-0-1_production"
  task_type = "production"
  start_hash = "de390f00dac09f52f97d82ca5b8789fa"

  [[runs]]

  task_name = "lig-3_contig-0-2_production"
  task_type = "production"
  start_hash = "2a2888da6c4733be0328b22eb4b7af26"


  [[runs]]

  task_name = "lig-3_contig-0-3_production"
  task_type = "production"
  start_hash = "d1317b41436d5d9c48b79c1b500d759f"

  [[runs]]

  task_name = "lig-3_contig-0-4_production"
  task_type = "production"
  start_hash = "055e58cbe616fc271fb25d3daa78b471"

  [[runs]]

  task_name = "lig-3_contig-1-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"

  [[runs]]

  task_name = "lig-3_contig-1-1_production"
  task_type = "production"
  start_hash = "bb621a3fa5b36b7954a3d21745f203aa"

  [[runs]]

  task_name = "lig-3_contig-2-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"


  [[runs]]

  task_name = "lig-3_contig-2-1_production"
  task_type = "production"
  start_hash = "12f792ca1ec034cac0de7d9ff6517c91"

  [[runs]]

  task_name = "lig-3_contig-3-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"

  [[runs]]

  task_name = "lig-3_contig-3-1_production"
  task_type = "production"
  start_hash = "9b027c1488717712905b00fe3e8cb7a4"

  [[runs]]

  task_name = "lig-3_contig-4-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"

  [[runs]]

  task_name = "lig-3_contig-4-1_production"
  task_type = "production"
  start_hash = "eaab5d019e061a229dd07e387a242ffc"

  [[runs]]

  task_name = "lig-3_contig-5-0_production"
  task_type = "production"
  start_hash = "ed11f6caeabd40a7cf5d687a98f5a6b2"

  [[runs]]

  task_name = "lig-3_contig-5-1_production"
  task_type = "production"
  start_hash = "ff4b43ac2bb8c4277fbc0cb3b08496c4"


  # new batch

  [[runs]]

  task_name = "lig-3_contig-0-5_production"
  task_type = "production"
  start_hash = "0cf1c1dbbdb8a8e91f0bfc5c090a0b74"


  [[runs]]

  task_name = "lig-3_contig-1-2_production"
  task_type = "production"
  start_hash = "8e97b2d28e2afd49db6b2e1c18d48608"


  [[runs]]

  task_name = "lig-3_contig-2-2_production"
  task_type = "production"
  start_hash = "1dde5be5baf0faef2233201b68832245"


  [[runs]]

  task_name = "lig-3_contig-3-2_production"
  task_type = "production"
  start_hash = "1ff9228c6da5cd22713a9d8ec42351ad"


  [[runs]]

  task_name = "lig-3_contig-4-2_production"
  task_type = "production"
  start_hash = "5756b809fe2f60269f46eff3407d76db"


  [[runs]]

  task_name = "lig-3_contig-5-2_production"
  task_type = "production"
  start_hash = "18dcfb9749be00a67e22f85105ee378d"


#+END_SRC

***** 10

#+BEGIN_SRC toml :tangle hpcc/simulations/10_simulations/run_spec.toml
  [defaults]

  num_gpus = 8

  # SNIPPET: when using 2 fs timesteps this gives you 10 picosecond cycles
  # n_steps = 10000

  # for 1 fs timesteps gives you 10 picosecond cycles
  n_steps = 20000

  run_time = 590400
  checkpoint_freq = 25
  orch_name = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite"

  config_name = "TaskMapper_CUDA_8-workers_lig-10_base.config.dill.pkl"

  log_level = "DEBUG"

  # start_hash = "6a33f4a4746dc7a1e83296a727c1f8b5"
  # WARN: the new hash for the new step time
  start_hash = "d0cb2e6fbcc8c2d66d67c845120c7f6b"
  task_type = "production"

  conda_env = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev"

  gnu_version = "GCC/8.3.0"
  cuda_version = "CUDA/10.1.243"

  scripts_dir = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/scripts"

  # patches to be applied at runtime, can be overridden at the run
  # definition. Must do the 'WepySimApparatus' patch first so the other
  # ones target the correct attributes
  patches = 'WepySimApparatus OpenMMRunner.platform_kwargs OpenMMRunner.getState_kwargs UnbindingBC._mdj_top UnbindingBC._periodic'

  # array of tables where each one is a specific segment; the index of
  # the table will be the uniquely identifying run index

  # any value from the defaults can be overridden inside

  [[runs]]

  task_name = "lig-10_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2

  [[runs]]

  task_name = "lig-10_test-local-hpcc-scratch_TaskMapper"
  config_name = "TaskMapper_CUDA_8-workers_lig-10_base.config.dill.pkl"

  [[runs]]

  task_name = "lig-10_test-local-hpcc-scratch_WorkerMapper"
  config_name = "WorkerMapper_CUDA_8-workers_lig-10_base.config.dill.pkl"


  [[runs]]

  task_name = "lig-10_test-submit"
  num_gpus = 4
  n_steps = 10000
  run_time = 3600
  checkpoint_freq = 2

  ## BATCH 1

  [[runs]]
  task_name = "lig-10_contig-0-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-10_contig-1-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-10_contig-2-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-10_contig-3-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-10_contig-4-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-10_contig-5-0_production"
  task_type = "production"


  ## BATCH 2

  [[runs]]
  task_name = "lig-10_contig-0-1_production"
  task_type = "production"
  start_hash = "b4b96580ae57f133d5f3b6ce25affa6d"

  [[runs]]
  task_name = "lig-10_contig-1-1_production"
  task_type = "production"
  start_hash = "12b9c5a180a98408fa2c234ffe59eebd"

  [[runs]]
  task_name = "lig-10_contig-2-1_production"
  task_type = "production"
  start_hash = "0bfd845cc0320fd13bec1643f3bcbae3"

  [[runs]]
  task_name = "lig-10_contig-3-1_production"
  task_type = "production"
  start_hash = "1b2c3187aff093ae84f7d3028840c20b"

  [[runs]]
  task_name = "lig-10_contig-4-1_production"
  task_type = "production"
  start_hash = "2ad5ec161f863dbed3c553805dfc2f17"

  [[runs]]
  task_name = "lig-10_contig-5-1_production"
  task_type = "production"
  start_hash = "6002c812ed001eaaa022aa429290193e"


  ## BATCH 3

  [[runs]]
  task_name = "lig-10_contig-0-2_production"
  task_type = "production"
  start_hash = "6184f17f627cab5741553985f3575eec"

  [[runs]]
  task_name = "lig-10_contig-1-2_production"
  task_type = "production"
  start_hash = "79dd6ed530ede77d15fcb34de5882bb2"

  [[runs]]
  task_name = "lig-10_contig-2-2_production"
  task_type = "production"
  start_hash = "e857df19793ec1a64e1aba7a7ad823e4"

  [[runs]]
  task_name = "lig-10_contig-3-2_production"
  task_type = "production"
  start_hash = "b15381dbf86aa763db7d80bbc3b4cfb7"

  [[runs]]
  task_name = "lig-10_contig-4-2_production"
  task_type = "production"
  start_hash = "7973d300b135003c6c22c838101e7eb7"

  [[runs]]
  task_name = "lig-10_contig-5-2_production"
  task_type = "production"
  start_hash = "5809528296b02b840f6c3376a5b7f431"
#+END_SRC


****** old

******* V1

#+BEGIN_SRC toml

  # these runs are from old stuff and we want to kind of ignore them


  [[runs]]

  task_name = "lig-10_contig-0-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"


  [[runs]]

  task_name = "lig-10_contig-0-1_production"
  task_type = "production"
  start_hash = "5ff88e59dff891973bd459224813a539"

  [[runs]]

  task_name = "lig-10_contig-0-2_production"
  task_type = "production"
  start_hash = "56ed345df9cfd1bef9c6ee646bcf2b5b"

  [[runs]]

  task_name = "lig-10_contig-1-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"

  [[runs]]

  task_name = "lig-10_contig-1-1_production"
  task_type = "production"
  start_hash = "5a9ca6bae5b48dad00ee452e896aa61b"

  [[runs]]

  task_name = "lig-10_contig-2-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"


  [[runs]]

  task_name = "lig-10_contig-2-1_production"
  task_type = "production"
  start_hash = "b2303d633641de4b6cb12eed548f87dd"

  [[runs]]

  task_name = "lig-10_contig-3-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"

  [[runs]]

  task_name = "lig-10_contig-4-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"

  [[runs]]

  task_name = "lig-10_contig-4-1_production"
  task_type = "production"
  start_hash = "34e64c97669757a13440f19b394622c1"

  [[runs]]

  task_name = "lig-10_contig-5-0_production"
  task_type = "production"
  start_hash = "1b1ef610fddb4fd86e514eb1acd96a3f"

  [[runs]]

  task_name = "lig-10_contig-5-1_production"
  task_type = "production"
  start_hash = "4ecde27bd33391126d5706041b28e550"


  # next batch

  [[runs]]

  task_name = "lig-10_contig-1-2_production"
  task_type = "production"
  start_hash = "7125793c770e23d596bb82973bf5a26b"

  [[runs]]

  task_name = "lig-10_contig-2-2_production"
  task_type = "production"
  start_hash = "f43da7bcb5ec077e1d71a58efb59b275"

  [[runs]]

  task_name = "lig-10_contig-4-2_production"
  task_type = "production"
  start_hash = "342f7011e584f7a5db3428acb04b32ea"



  # next batch


  [[runs]]

  task_name = "lig-10_contig-0-3_production"
  task_type = "production"
  start_hash = "a4bd4a55dd4fb99fe1ad86002911945f"

  [[runs]]

  task_name = "lig-10_contig-1-3_production"
  task_type = "production"
  start_hash = "2e6c8ad8f3bc58dbb935f1c91b32c345"

  [[runs]]

  task_name = "lig-10_contig-2-3_production"
  task_type = "production"
  start_hash = "5f0b05d01960da72b3c1a5a55bf408fd"

  [[runs]]

  task_name = "lig-10_contig-3-1_production"
  task_type = "production"
  start_hash = "0e081557e6e3082183a14c671f76ad86"

  [[runs]]

  task_name = "lig-10_contig-4-3_production"
  task_type = "production"
  start_hash = "fa74ecc7bd2e1d8ba50bb4d566003742"

  [[runs]]

  task_name = "lig-10_contig-5-2_production"
  task_type = "production"
  start_hash = "efd25ab7fe51c42551e754d9f6a593cd"


#+END_SRC



******* V2

#+begin_src toml
  ## BATCH 2

  [[runs]]
  task_name = "lig-10_contig-0-1_production"
  task_type = "production"
  start_hash = "6d2c47827d6c6bfcc7974fda080f4378"

  [[runs]]
  task_name = "lig-10_contig-1-1_production"
  task_type = "production"
  start_hash = "e160f24a6155bf7664b2e6a41b8b2da7"

  [[runs]]
  task_name = "lig-10_contig-2-1_production"
  task_type = "production"
  start_hash = "af45df1ec10ad6286f3c7ca37401ddef"

  [[runs]]
  task_name = "lig-10_contig-3-1_production"
  task_type = "production"
  start_hash = "8d219d1788b9c8d3c5f8f649755f1221"

  [[runs]]
  task_name = "lig-10_contig-4-1_production"
  task_type = "production"
  start_hash = "1bf3e875f58e8ea7996efb2a3dbce966"


  ## BATCH 3

  [[runs]]
  task_name = "lig-10_contig-0-2_production"
  task_type = "production"
  start_hash = "d6c98acf0b786c8dd94cf957469d0dcf"

  [[runs]]
  task_name = "lig-10_contig-5-1_production"
  task_type = "production"
  start_hash = "d6c98acf0b786c8dd94cf957469d0dcf"

  [[runs]]
  task_name = "lig-10_contig-2-2_production"
  task_type = "production"
  start_hash = "f9d613627fdd07fd714afb6a025084e8"

  [[runs]]
  task_name = "lig-10_contig-3-2_production"
  task_type = "production"
  start_hash = "64c6723e262b80a4a99b4f69a2ebabb9"

  [[runs]]
  task_name = "lig-10_contig-4-2_production"
  task_type = "production"
  start_hash = "7f24b6fe68739af9536e1eebf14d870f"



  ## BATCH 4

  [[runs]]
  task_name = "lig-10_contig-0-3_production"
  task_type = "production"
  start_hash = "978f37c41a8d880a45e8a35b91ab5be3"

  [[runs]]
  task_name = "lig-10_contig-1-2_production"
  task_type = "production"
  start_hash = "e4c77e375e5bc2bc8e9d7357d3c16ce7"

  [[runs]]
  task_name = "lig-10_contig-2-3_production"
  task_type = "production"
  start_hash = "e4c77e375e5bc2bc8e9d7357d3c16ce7"

  [[runs]]
  task_name = "lig-10_contig-3-3_production"
  task_type = "production"
  start_hash = "dc74d28358ef2361690698ff966b96db"

  [[runs]]
  task_name = "lig-10_contig-4-3_production"
  task_type = "production"
  start_hash = "f54d00819eb8ff9db67336ac198853a2"

  [[runs]]
  task_name = "lig-10_contig-5-2_production"
  task_type = "production"
  start_hash = "cca05a1850609c6c488b295a36604317"

#+end_src

***** 17

#+BEGIN_SRC toml :tangle hpcc/simulations/17_simulations/run_spec.toml
  [defaults]

  num_gpus = 8
  # SNIPPET: when using 2 fs timesteps this gives you 10 picosecond cycles
  # n_steps = 10000

  # for 1 fs timesteps gives you 10 picosecond cycles
  n_steps = 20000

  run_time = 590400
  checkpoint_freq = 25
  orch_name = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/17_simulations/orchs/master_sEH_lig-17.orch.sqlite"
  config_name = "TaskMapper_CUDA_8-workers_lig-17_base.config.dill.pkl"
  log_level = "DEBUG"

  start_hash = "9096091b25b2c978354b32f348fb2c71"
  task_type = "production"

  conda_env = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims"

  gnu_version = "GCC/8.3.0"
  cuda_version = "CUDA/10.1.243"

  scripts_dir = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/scripts"

  # patches to be applied at runtime, can be overridden at the run
  # definition
  patches = 'WepySimApparatus OpenMMRunner.platform_kwargs OpenMMRunner.getState_kwargs UnbindingBC._mdj_top UnbindingBC._periodic WExplorePmin'

  # array of tables where each one is a specific segment; the index of
  # the table will be the uniquely identifying run index

  # any value from the defaults can be overridden inside

  [[runs]]

  task_name = "lig-17_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/17_simulations/orchs/master-v3_sEH_lig-17.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2

  [[runs]]

  task_name = "lig-17_test-local-hpcc-scratch_TaskMapper"
  config_name = "TaskMapper_CUDA_8-workers_lig-17_base.config.dill.pkl"

  [[runs]]

  task_name = "lig-17_test-local-hpcc-scratch_WorkerMapper"
  config_name = "WorkerMapper_CUDA_8-workers_lig-17_base.config.dill.pkl"


  [[runs]]

  task_name = "lig-17_test-submit"
  num_gpus = 4
  n_steps = 10000
  run_time = 3600
  checkpoint_freq = 2

  ## Batch 1

  [[runs]]
  task_name = "lig-17_contig-0-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-17_contig-1-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-17_contig-2-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-17_contig-3-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-17_contig-4-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-17_contig-5-0_production"
  task_type = "production"

  ## Batch 2

  [[runs]]
  task_name = "lig-17_contig-0-1_production"
  task_type = "production"
  start_hash = "bb375a77d0065cdd4e8b86eb05a2cdde"

  [[runs]]
  task_name = "lig-17_contig-1-1_production"
  task_type = "production"
  start_hash = "f97c14eb333f86e24d92f760780294af"

  [[runs]]
  task_name = "lig-17_contig-2-1_production"
  task_type = "production"
  start_hash = "2451389c1a1d8af39fea7f16977f4e13"

  [[runs]]
  task_name = "lig-17_contig-3-1_production"
  task_type = "production"
  start_hash = "5a5ceb0e0dc581460d4d36f9accc2451"

  [[runs]]
  task_name = "lig-17_contig-4-1_production"
  task_type = "production"
  start_hash = "1bdd219873e7163dccfb9f97be1e92eb"

  [[runs]]
  task_name = "lig-17_contig-5-1_production"
  task_type = "production"
  start_hash = "f2cb52a2072e7e9ecb8097de1262c435"


  ## Batch 3

  [[runs]]
  task_name = "lig-17_contig-0-2_production"
  task_type = "production"
  start_hash = "381489ed16d72d74c9a3a30ea1d6f2b7"

  [[runs]]
  task_name = "lig-17_contig-1-2_production"
  task_type = "production"
  start_hash = "165370e5502ff14a337fb9b1014b4136"

  [[runs]]
  task_name = "lig-17_contig-2-2_production"
  task_type = "production"
  start_hash = "d9dda1af96355f23dc8f3e5996512f5a"

  [[runs]]
  task_name = "lig-17_contig-3-2_production"
  task_type = "production"
  start_hash = "ec044c9a6cbef04a9ee75e2b36d2ff7a"

  [[runs]]
  task_name = "lig-17_contig-4-2_production"
  task_type = "production"
  start_hash = "780a49884be084bcb2328298da863806"

  [[runs]]
  task_name = "lig-17_contig-5-2_production"
  task_type = "production"
  start_hash = "6268c249d2c5a88255b6b45ff8ff2e4d"

#+END_SRC




***** 18

#+BEGIN_SRC toml :tangle hpcc/simulations/18_simulations/run_spec.toml
  [defaults]

  num_gpus = 8


  # SNIPPET: when using 2 fs timesteps this gives you 10 picosecond cycles
  # n_steps = 10000

  # for 1 fs timesteps gives you 10 picosecond cycles
  n_steps = 20000

  run_time = 590400
  checkpoint_freq = 25
  orch_name = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/18_simulations/orchs/master_sEH_lig-18.orch.sqlite"
  config_name = "TaskMapper_CUDA_8-workers_lig-18_base.config.dill.pkl"
  log_level = "DEBUG"

  task_type = "production"

  # start_hash = "72af38a7e982615cc0bf1bc70b501494"
  # WARN: the new hash for the new step time
  start_hash = "2348d3b3c4c34fe405282157614d7bb8"

  conda_env = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims"

  gnu_version = "GCC/8.3.0"
  cuda_version = "CUDA/10.1.243"

  scripts_dir = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/scripts"

  # patches to be applied at runtime, can be overridden at the run
  # definition
  patches = 'WepySimApparatus OpenMMRunner.platform_kwargs OpenMMRunner.getState_kwargs UnbindingBC._mdj_top UnbindingBC._periodic WExplorePmin'


  # array of tables where each one is a specific segment; the index of
  # the table will be the uniquely identifying run index

  # any value from the defaults can be overridden inside

  [[runs]]

  task_name = "lig-18_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/18_simulations/orchs/master_sEH_lig-18.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2


  [[runs]]

  task_name = "lig-18_test-local-hpcc-scratch_TaskMapper"
  config_name = "TaskMapper_CUDA_8-workers_lig-18_base.config.dill.pkl"

  [[runs]]

  task_name = "lig-18_test-local-hpcc-scratch_WorkerMapper"
  config_name = "WorkerMapper_CUDA_8-workers_lig-18_base.config.dill.pkl"


  [[runs]]

  task_name = "lig-18_test-submit"
  num_gpus = 4
  n_steps = 10000
  run_time = 3600
  checkpoint_freq = 2


  ## BATCH 1

  [[runs]]
  task_name = "lig-18_contig-0-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-18_contig-1-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-18_contig-2-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-18_contig-3-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-18_contig-4-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-18_contig-5-0_production"
  task_type = "production"

  ## Batch 2

  [[runs]]
  task_name = "lig-18_contig-0-1_production"
  task_type = "production"
  start_hash = "5ec12b6684505a507c2b349b9909d62d"

  [[runs]]
  task_name = "lig-18_contig-1-1_production"
  task_type = "production"
  start_hash = "235f74cd69e33796c55e22bd25a13074"

  [[runs]]
  task_name = "lig-18_contig-2-1_production"
  task_type = "production"
  start_hash = "e25400cdcdcf1b1a6edfd53cd14ac8cf"

  [[runs]]
  task_name = "lig-18_contig-3-1_production"
  task_type = "production"
  start_hash = "16d9df4ba299d6a0fa35bca0f6a19a5d"

  [[runs]]
  task_name = "lig-18_contig-4-1_production"
  task_type = "production"
  start_hash = "c7c1b71673e8aa6cf46d3acd9472a16c"

  [[runs]]
  task_name = "lig-18_contig-5-1_production"
  task_type = "production"
  start_hash = "b66aa93210241e691dd0159645a6724f"


  ## Batch 3

  [[runs]]
  task_name = "lig-18_contig-0-2_production"
  task_type = "production"
  start_hash = "5d4dd14c7746f87632393e5f664802f1"

  [[runs]]
  task_name = "lig-18_contig-1-2_production"
  task_type = "production"
  start_hash = "0a570c199fca47986fb613709cf8b597"

  [[runs]]
  task_name = "lig-18_contig-2-2_production"
  task_type = "production"
  start_hash = "f15c52ea4f35e1171d45423eb32e30e4"

  [[runs]]
  task_name = "lig-18_contig-3-2_production"
  task_type = "production"
  start_hash = "1bc951bb16776609ff4c3643d2a2d9bf"

  [[runs]]
  task_name = "lig-18_contig-4-2_production"
  task_type = "production"
  start_hash = "9529a931658f358b902cded4d34d56e5"

  [[runs]]
  task_name = "lig-18_contig-5-2_production"
  task_type = "production"
  start_hash = "2e1dd92c6ccf3de5ca04461822ce0b51"
#+END_SRC


****** old

#+BEGIN_SRC toml
  # next batch



  [[runs]]

  task_name = "lig-18_contig-0-4_production"
  task_type = "production"
  start_hash = "dacddf49609e3cc47e29daca52caa5d8"



  [[runs]]

  task_name = "lig-18_contig-2-2_production"
  task_type = "production"
  start_hash = "71dfce792cff9046ebd99e7f4e10a448"


  [[runs]]

  task_name = "lig-18_contig-3-2_production"
  task_type = "production"
  start_hash = "07614b623256fa7c1e54aa99d2b4c2d3"


  [[runs]]

  task_name = "lig-18_contig-4-2_production"
  task_type = "production"
  start_hash = "7646fd603713084c822060f9adcb011c"


  # next batch

  [[runs]]

  task_name = "lig-18_contig-0-5_production"
  task_type = "production"
  start_hash = "4bc9e83f424237c023a0cc868face740"

  [[runs]]

  task_name = "lig-18_contig-1-1_production"
  task_type = "production"
  start_hash = "91bbd7a0cc82a270123004b7f9479c9d"

  [[runs]]

  task_name = "lig-18_contig-2-3_production"
  task_type = "production"
  start_hash = "96b540d815a500374bd66d20fd7d18af"

  [[runs]]

  task_name = "lig-18_contig-3-3_production"
  task_type = "production"
  start_hash = "23a311b9d9cc45f5df44b929916e566a"

  [[runs]]

  task_name = "lig-18_contig-4-3_production"
  task_type = "production"
  start_hash = "b94290d18cf7c9282d7518a56fe6bd66"

  [[runs]]

  task_name = "lig-18_contig-5-2_production"
  task_type = "production"
  start_hash = "bd192c5c4da3973076411f37ea1a044b"


#+END_SRC


***** 20

#+BEGIN_SRC toml :tangle hpcc/simulations/20_simulations/run_spec.toml
  [defaults]

  num_gpus = 8

  # SNIPPET: when using 2 fs timesteps this gives you 10 picosecond cycles
  # n_steps = 10000

  # for 1 fs timesteps gives you 10 picosecond cycles
  n_steps = 20000

  run_time = 590400
  checkpoint_freq = 25
  orch_name = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/20_simulations/orchs/master_sEH_lig-20.orch.sqlite"
  config_name = "TaskMapper_CUDA_8-workers_lig-20_base.config.dill.pkl"
  log_level = "DEBUG"

  start_hash = "46edaa6b9b187fac7e6d5426962c97d1"
  task_type = "production"  

  conda_env = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims"

  gnu_version = "GCC/8.3.0"
  cuda_version = "CUDA/10.1.243"

  scripts_dir = "$SCRATCH/tree/lab/projects/seh.pathway_hopping/hpcc/scripts"

  # patches to be applied at runtime, can be overridden at the run
  # definition
  patches = 'WepySimApparatus OpenMMRunner.platform_kwargs OpenMMRunner.getState_kwargs UnbindingBC._mdj_top UnbindingBC._periodic WExplorePmin'

  # array of tables where each one is a specific segment; the index of
  # the table will be the uniquely identifying run index

  # any value from the defaults can be overridden inside

  [[runs]]

  task_name = "lig-20_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/20_simulations/orchs/master_sEH_lig-20.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2

  [[runs]]

  task_name = "lig-20_test-local-hpcc-scratch_TaskMapper"
  config_name = "TaskMapper_CUDA_8-workers_lig-20_base.config.dill.pkl"

  [[runs]]

  task_name = "lig-20_test-local-hpcc-scratch_WorkerMapper"
  config_name = "WorkerMapper_CUDA_8-workers_lig-20_base.config.dill.pkl"

  [[runs]]

  task_name = "lig-20_test-submit"
  num_gpus = 4
  n_steps = 10000
  run_time = 3600
  checkpoint_freq = 2

  ## BATCH 1

  [[runs]]
  task_name = "lig-20_contig-0-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-20_contig-1-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-20_contig-2-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-20_contig-3-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-20_contig-4-0_production"
  task_type = "production"

  [[runs]]
  task_name = "lig-20_contig-5-0_production"
  task_type = "production"

  ## BATCH 2

  [[runs]]
  task_name = "lig-20_contig-0-1_production"
  task_type = "production"
  start_hash = "67607f3a767374eca477fa18c64370a2"

  [[runs]]
  task_name = "lig-20_contig-1-1_production"
  task_type = "production"
  start_hash = "20e9b79e88a7066b5292e1d32c50ba89"

  [[runs]]
  task_name = "lig-20_contig-2-1_production"
  task_type = "production"
  start_hash = "09012ee1199004878490ff6798a9226f"

  [[runs]]
  task_name = "lig-20_contig-3-1_production"
  task_type = "production"
  start_hash = "c722976b2739f7326b72e9ee42ac88aa"

  [[runs]]
  task_name = "lig-20_contig-4-1_production"
  task_type = "production"
  start_hash = "0cc674b19036ca12d4a43e3b65026174"

  [[runs]]
  task_name = "lig-20_contig-5-1_production"
  task_type = "production"
  start_hash = "7e61c0c04e9f9eeca4bfb675f213bb59"

  ## BATCH 3

  [[runs]]
  task_name = "lig-20_contig-0-2_production"
  task_type = "production"
  start_hash = "92047914ba43c1dfadd28a33289c4a48"

  [[runs]]
  task_name = "lig-20_contig-1-2_production"
  task_type = "production"
  start_hash = "193c7c5d6d62695149b64ddfa5d87aee"

  [[runs]]
  task_name = "lig-20_contig-2-2_production"
  task_type = "production"
  start_hash = "6ebbfda79da92c41c5f3aef37309bce9"

  [[runs]]
  task_name = "lig-20_contig-3-2_production"
  task_type = "production"
  start_hash = "e383051d3d434282121381dbd3cad3c8"

  [[runs]]
  task_name = "lig-20_contig-4-2_production"
  task_type = "production"
  start_hash = "880e5cf325bf636555c5d5b545ff4713"

  [[runs]]
  task_name = "lig-20_contig-5-2_production"
  task_type = "production"
  start_hash = "a6e8df9432b76bda26da1743da700c4a"
#+END_SRC


****** old

#+BEGIN_SRC toml


  [[runs]]

  task_name = "lig-20_contig-0-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"

  [[runs]]

  task_name = "lig-20_contig-0-1_production"
  task_type = "production"
  start_hash = "18f87753e7646610f97b3ad801adde5b"

  [[runs]]

  task_name = "lig-20_contig-0-2_production"
  task_type = "production"
  start_hash = "3ed328662a6729de4828df4b6c25d915"

  [[runs]]

  task_name = "lig-20_contig-1-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"

  [[runs]]

  task_name = "lig-20_contig-1-1_production"
  task_type = "production"
  start_hash = "21d236bee8e1bac6c79af23029a8b772"

  [[runs]]

  task_name = "lig-20_contig-2-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"

  [[runs]]

  task_name = "lig-20_contig-3-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"

  [[runs]]

  task_name = "lig-20_contig-3-1_production"
  task_type = "production"
  start_hash = "59200b09f7acdb3b54085b9565def2aa"

  [[runs]]

  task_name = "lig-20_contig-4-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"


  [[runs]]

  task_name = "lig-20_contig-5-0_production"
  task_type = "production"
  start_hash = "243924f8b642a1b69e9ff83ef466e2d2"

  [[runs]]

  task_name = "lig-20_contig-5-1_production"
  task_type = "production"
  start_hash = "6a058bdf469dd0bde2d121de461f28ad"


  # new batch
  [[runs]]

  task_name = "lig-20_contig-0-3_production"
  task_type = "production"
  start_hash = "79248ae08705a5e9dc524331d5b1879a"


  [[runs]]

  task_name = "lig-20_contig-1-2_production"
  task_type = "production"
  start_hash = "db0db3d06713accc3e5e3befb6aa0ac9"


  [[runs]]

  task_name = "lig-20_contig-3-2_production"
  task_type = "production"
  start_hash = "fce412e170a916dd34938ef682fdad82"


  [[runs]]

  task_name = "lig-20_contig-4-2_production"
  task_type = "production"
  start_hash = "17e8193ab5ecb9db4949a53389df5303"


  [[runs]]

  task_name = "lig-20_contig-5-2_production"
  task_type = "production"
  start_hash = "bfcac3fcea48ad651ce7aa783c10b9ad"


  # new batch

  [[runs]]

  task_name = "lig-20_contig-0-4_production"
  task_type = "production"
  start_hash = "77925f2d0870263ee41aad604ecaa285"


  [[runs]]

  task_name = "lig-20_contig-1-3_production"
  task_type = "production"
  start_hash = "65c6fc950375b4de5be05c1a81fbe777"


  [[runs]]

  task_name = "lig-20_contig-2-1_production"
  task_type = "production"
  start_hash = "c5682961b7fd63b2b1d66b7601d0b49e"


  [[runs]]

  task_name = "lig-20_contig-3-3_production"
  task_type = "production"
  start_hash = "4c381d53f1e8b432f11c7ed3595f9d5e"


  [[runs]]

  task_name = "lig-20_contig-4-1_production"
  task_type = "production"
  start_hash = "b42c0899f742b53f0bfa00a3ab756369"


  [[runs]]

  task_name = "lig-20_contig-5-3_production"
  task_type = "production"
  start_hash = "f6aa72c339471089ebf3829a103eab56"



#+END_SRC


** Analysis

*** Dask Cluster Job Script

#+begin_src bash :tangle hpcc/analysis/dask_server/submissions/start_server.sbatch
  #!/bin/sh  -login

  # run from the hpcc/analysis/dask_server directory

  #SBATCH -e logs/scheduler.%J.err
  #SBATCH -o logs/scheduler.%J.out
  #SBATCH -J scheduler

  #SBATCH --time=168:00:00
  #SBATCH --nodes=1
  #SBATCH --ntasks=1
  #SBATCH --cpus-per-task=4

  #SBATCH --mem=16gb

  #SBATCH --mail-type="BEGIN,END,FAIL"

  adapt_max=40
  dash_port=45708
  scheduler_port=42805

  echo "Adapt Max: $adapt_max"
  echo "Dashboard Port: $dash_port"
  echo "Scheduler Port: $scheduler_port"

  conda activate $SCRATCH/tree/lab/projects/seh.pathway_hopping/_conda_envs/common

  # make a job dir and start server from there
  mkdir -p jobs/${SLURM_JOB_ID}
  cd jobs/${SLURM_JOB_ID}

  python -m seh_pathway_hopping.scheduler \
         --adapt-max "$adapt_max" \
         --dash-port "$dash_port" \
         --scheduler-port "$scheduler_port"
#+end_src

* Workflow

A (mostly) ordered outline of how to proceed from beginning to end.

** Utilities

*** HPCC Interactive Jobs

#+begin_src bash
srun -N 1 -c 1 --mem=15G --time=48:00:00 -C intel16 --gres=gpu:1 --pty "/bin/bash"
#+end_src


*** COMMENT project data push

#+BEGIN_SRC bash

  push_analysis_data () {
    hpcc_url="lotzsamu@rsync.hpcc.msu.edu"
    hpcc_data_dir="/mnt/scratch/lotzsamu/seh.pathway_hopping"
    projects_dir="$HOME/tree/lab/projects"
    project_dir="${projects_dir}/seh.pathway_hopping"

    # first sync the data, excluding not useful stuff
    rsync -rav \
          --include="cache" --include="db" --include='data' \
          --exclude=".git" --exclude="__pycache__" \
          "${project_dir}/" "${hpcc_url}:${hpcc_data_dir}/analysis"

    # then manipulate some stuff so it is the same on HPCC

    # change the symlinks for the results HDF5s from simulations
    for lig_id in 3; do
        ssh "$hpcc_url" "ln -sfrT ${hpcc_data_dir}/results/${lig_id}/all_results.wepy.h5 ${hpcc_data_dir}/analysis/data/results/${lig_id}/all_results_lig-${lig_id}.wepy.h5"
    done
  }
#+END_SRC


*** project data pull


Pull all the analysis data which has been updated prolly.

#+BEGIN_SRC bash

  pull_analysis_data () {
    hpcc_url="lotzsamu@rsync.hpcc.msu.edu"
    hpcc_data_dir="/mnt/scratch/lotzsamu/seh.pathway_hopping"
    projects_dir="$HOME/tree/lab/projects"
    project_dir="${projects_dir}/seh.pathway_hopping"

    # only retrieve the data, not the cache or anything
    rsync -rav \
          "${hpcc_url}:${hpcc_data_dir}/analysis/data/" "${project_dir}/data/"

    # then manipulate some stuff so it is the same on HPCC

    # change the symlinks for the results HDF5s from simulations
    for lig_id in 3; do

        ln -sfr "$HOME/tree/lab/projects/seh.pathway_hopping/results/3/all_results.wepy.h5" \
                 "$HOME/tree/lab/projects/seh.pathway_hopping/data/results/3/all_results_lig-3.wepy.h5"
    done
  }
#+END_SRC


Get just the simulations results from HPCC which are stored
differently.

#+NAME: pull_results
#+BEGIN_SRC bash
  pull_result () {

      lig_id="$1"

      local_results="$HOME/tree/lab/projects/seh.pathway_hopping/results/${lig_id}"
      echo $local_results

      mkdir -p "${local_results}"

      hpcc_url="lotzsamu@rsync.hpcc.msu.edu"
      scratch="/mnt/gs18/scratch/users/lotzsamu"
      result_file="${scratch}/seh.pathway_hopping/results/${lig_id}/all_results.wepy.h5"
      echo ${hpcc_url}:${results_file}

      rsync -rav "${hpcc_url}:${result_file}" "${local_results}/"

  }
#+END_SRC



*** wepy sync

rsync
#+BEGIN_SRC bash
hpcc_url="lotzsamu@hpcc.msu.edu"
hpcc_devel_dir="/mnt/home/lotzsamu/devel"
hpcc_wepy_dir="${hpcc_devel_dir}/wepy"
wepy_dir="$HOME/devel/wepy"
rsync_repo ${wepy_dir} ${hpcc_url}:${hpcc_devel_dir}
#+END_SRC


mutagen
#+BEGIN_SRC bash
hpcc_url="lotzsamu@hpcc.msu.edu"
hpcc_devel_dir="/mnt/home/lotzsamu/devel"
hpcc_wepy_dir="${hpcc_devel_dir}/wepy"
wepy_dir="$HOME/tree/lab/alt/wepy"
mutagen create ${wepy_dir} ${hpcc_url}:${hpcc_wepy_dir}
#+END_SRC


*** seh.pathway_hopping module

- mutagen
#+BEGIN_SRC bash
  hpcc_url="lotzsamu@hpcc.msu.edu"
  hpcc_devel_dir="/mnt/home/lotzsamu/devel"
  hpcc_seh_pathway_hopping_dir="${hpcc_devel_dir}/seh.pathway_hopping"
  seh_pathway_hopping_dir="$HOME/tree/lab/projects/seh.pathway_hopping"
  mutagen create -m one-way-safe \
          --ignore-vcs \
          -i "build*" -i "cache" -i "data" -i "db" -i "dist" -i "seh_prep" -i "troubleshoot" -i "*__pycache__*" -i "*egg*" \
          ${seh_pathway_hopping_dir} ${hpcc_url}:${hpcc_seh_pathway_hopping_dir}
#+END_SRC


*** sync scripts to hpcc


#+begin_src bash
rsync -ravvhhiz $HOME/tree/lab/projects/seh.pathway_hopping/hpcc/scripts/ \
  lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/scripts
#+end_src


#+BEGIN_SRC bash
hpcc_projects_dir="/mnt/home/lotzsamu/projects"
hpcc_project_dir="${hpcc_projects_dir}/seh.pathway_hopping"
hpcc_script_dir="${hpcc_project_dir}/scripts"
source ${hpcc_script_dir}/bash_funcs.sh
#+END_SRC



*** Set up mutagen mirroring of tangled scripts

Mutagen should be running for this to work:

#+BEGIN_SRC bash
mutagen daemon start
#+END_SRC

rsync
#+BEGIN_SRC bash
hpcc_url="lotzsamu@hpcc.msu.edu"
hpcc_projects_dir="/mnt/home/lotzsamu/projects"
hpcc_project_dir="${hpcc_projects_dir}/seh.pathway_hopping"
hpcc_script_dir="${hpcc_project_dir}/scripts"
projects_dir="$HOME/tree/lab/projects"
project_dir="${projects_dir}/seh.pathway_hopping"
project_scripts_dir="${project_dir}/run"
rsync -rav ${project_scripts_dir} ${hpcc_url}:${hpcc_script_dir}
#+END_SRC


Then just set up the connection. Should only need to be done once and
then mutagen saves it:

#+BEGIN_SRC bash
hpcc_url="lotzsamu@hpcc.msu.edu"
hpcc_projects_dir="/mnt/home/lotzsamu/projects"
hpcc_project_dir="${hpcc_projects_dir}/seh.pathway_hopping"
hpcc_script_dir="${hpcc_project_dir}/scripts"
projects_dir="$HOME/tree/lab/projects"
project_dir="${projects_dir}/seh.pathway_hopping"
project_scripts_dir="${project_dir}/run"
mutagen create ${project_scripts_dir} ${hpcc_url}:${hpcc_script_dir}
#+END_SRC




** Preparing Molecular Systems

*** virtualenvs

Using conda for making virtual environments. We will need a couple
different ones because different packages are incompatible. This
mostly involves openbabel, but we separate them just because it is
cleaner and matches what I was already doing.


**** seh_prep

I provide a small package for preparing the sEH system that is
reusable for equilibrating and setting up the orchestrator for the
system.

It comes with a definition of the environment it needs that can be
created with conda:

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/seh_prep
conda env create -n seh_prep -f seh_prep.env.yaml
# install it as an editable package in case you need to tweak it
pip install -e .
#+END_SRC

Otherwise you can install from the source distribution archive file:
#+BEGIN_SRC bash
pip install seh_prep-0.1.tar.gz
#+END_SRC

**** openbabel

To get the desired behavior from openbabel we have to have a new
version which has incompatible dependencies with other packages so we
provide another environment:

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/seh_prep
conda env create -n openbabel -f openbabel.env.yaml
#+END_SRC

**** COMMENT seh_pathway_hopping

This is the environment in which the simulations will be run.

This expects that the tarball source distribution of the seh_prep
modules is in the same directory as it.

#+BEGIN_SRC yaml
  name: seh_pathway_hopping
  channels:
    - omnia
    - defaults
  dependencies:
    - blas=1.0=mkl
    - ca-certificates=2018.03.07=0
    - certifi=2018.10.15=py36_0
    - intel-openmp=2019.1=144
    - libedit=3.1.20170329=h6b74fdf_2
    - libffi=3.2.1=hd88cf55_4
    - libgcc-ng=8.2.0=hdf63c60_1
    - libgfortran-ng=7.3.0=hdf63c60_0
    - libstdcxx-ng=8.2.0=hdf63c60_1
    - mkl=2018.0.3=1
    - mkl_fft=1.0.6=py36h7dd41cf_0
    - mkl_random=1.0.1=py36h4414c95_1
    - ncurses=6.1=hf484d3e_0
    - numpy=1.15.4=py36h1d66e8a_0
    - numpy-base=1.15.4=py36h81de0dd_0
    - openssl=1.1.1a=h7b6447c_0
    - pip=18.1=py36_0
    - python=3.6.7=h0371630_0
    - readline=7.0=h7b6447c_5
    - setuptools=40.6.2=py36_0
    - sqlite=3.25.3=h7b6447c_0
    - tk=8.6.8=hbc83047_0
    - wheel=0.32.3=py36_0
    - xz=5.2.4=h14c3975_4
    - zlib=1.2.11=h7b6447c_3
    - fftw3f=3.3.4=2
    - openmm=7.3.0=py36_cuda92_rc_1
    - pip:
      - click==7.0
      - cycler==0.10.0
      - cython==0.29.1
      - decorator==4.3.0
      - dill==0.2.8.2
      - geomm==0.1.5
      - h5py==2.8.0
      - kiwisolver==1.0.1
      - matplotlib==3.0.2
      - mdtraj==1.9.2
      - networkx==2.2
      - pandas==0.23.4
      - pyparsing==2.3.0
      - python-dateutil==2.7.5
      - pytz==2018.7
      - scipy==1.1.0
      - six==1.11.0
      - wepy==0.10.2
      - seh_prep-0.1.tar.gz
#+END_SRC

**** sim_prep

#+BEGIN_SRC bash
  env_name='sim_prep'
  conda create -n "$env_name"
  conda activate "$env_name"
  conda install -y -c conda-forge \
        numpy scipy pandas lxml \
        parmed mdtraj
  conda install -y -c omnia openmm

  pip install -e ~/devel/wepy
#+END_SRC

*** Simplifying and cleaning RCSB PDB file

We first should prepare the protein for simulation.

Here is the protocol described in the paper:

#+BEGIN_QUOTE
Initial atomic coordinates were taken from the Protein Data Bank (PDB)
entry 4OD0,(15) which has two domains with few interdomain contacts
and are connected by a flexible linker. We removed the domain which
does not bind to the ligand of interest, including the associated
crystallographic waters, magnesium ion, and phosphate ion, retaining
residues Ser231 to Arg546 (PDB serial numbers), and 10
crystallographic waters. To solvate the system, a box of TIP3P water
molecules were placed around the protein and ligand with a 12 
cutoff. The completed system included 316 amino acids (5052 atoms) for
the protein, 45 ligand atoms, 16831 water molecules, and 7
neutralizing sodium ions for a total of 21935 atoms.

The protein is parametrized by the CHARMM36 force field,(47) and the
ligand force field was derived using the homology based CHARMM
Generalized Force Field (CGENFF) algorithm.(48) Before dynamics the
solvent molecules are minimized using 500 steps of steepest descent
followed by 500 steps of the adopted NewtonRaphson method. The whole
system is then minimized in the same way.
#+END_QUOTE


First we want to take the original structure and fix some things
associated with the crystallographic data shortcomings. We will use
PDBFixer for this.

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  from setup_paths import *

  from pdbfixer import PDBFixer

  from simtk.openmm.app import PDBFile

  pdb_id = "4od0"

  pdb_path = osp.join(rcsb_dir, "{}.pdb".format(pdb_id))

  fixer = PDBFixer(filename=pdb_path)

  fixer.findMissingResidues()

  print(fixer.missingResidues)
#+END_SRC

We see from this that we are missing residues at the end of the
chain. These were not included in our previous simulations. And I see
no reason to include them now but just to note this.

So the last residue 546 is not the last residue overall and there
should be the following residues after it:
'ASN', 'PRO', 'PRO', 'VAL', 'VAL', 'SER', 'LYS', 'MET'

However we don't want to add in those missing resides because we don't
want to simulate them.

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  # stole this from the PDBFixer manual
  chains = list(fixer.topology.chains())
  keys = list(fixer.missingResidues.keys())
  for key in keys:
      chain = chains[key[0]]
      if key[1] == 0 or key[1] == len(list(chain.residues())):
          del fixer.missingResidues[key]
#+END_SRC

We also want to add in the missing atoms.

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  fixer.findMissingAtoms()

  print(fixer.missingAtoms)
#+END_SRC

And we see that we are missing:

#+BEGIN_EXAMPLE
{<Residue 0 (MET) of chain 0>: [<Atom 3 (CG) of chain 0 residue 0 (MET)>,
  <Atom 4 (SD) of chain 0 residue 0 (MET)>,
  <Atom 5 (CE) of chain 0 residue 0 (MET)>]}
#+END_EXAMPLE

Now we actually want to add the missing atoms along with missing
hydrogens for the pH of the simulations:

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  fixer.addMissingAtoms()
#+END_SRC


Now I want to get rid of the rest of the protein that I don't want to
simulate:

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  # this is the residue ID of the first residue to keep
  first_keep_res_id = 231

  last_keep_res_idx = 546

  # the index of that residue is actually minus 1 since IDs are just
  # base 1 indices, so we make a list of residues to get rid of ending
  # at that

  num_residues = len(list(chain.residues()))

  # a list of the residue idxs not to keep
  remove_res_nums = list(range(0, first_keep_res_id - 1))

#+END_SRC


Now save the prepared PDB system.

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  full_system_fixed_path = osp.join(pdbfixer_dir, '4od0_simplified.pdbfixer.pdb')

  PDBFile.writeFile(fixer.topology, fixer.positions, open(full_system_fixed_path, 'w'))
#+END_SRC


We also want to split it up into multiple parts for the solvent,
protein, and ligand.

#+BEGIN_SRC python :tangle prep/fix_4od0_pdb.py
  import mdtraj as mdj

  # load the pdb with mdtraj
  pdb = mdj.load_pdb(full_system_fixed_path)

  # get the topology as a dataframe
  top_df = pdb.top.to_dataframe()[0]

  # we use this to manually get the indices of the atoms for each
  # segmentation

  # we want three segmentations:
  # - protein
  # - ligand
  # - solvent: water, ions

  # through manual inspection we find that 'chainID' == 0 is the protein
  # and 'chainID' == 1 is everything else, so we get the protein that way
  prot_idxs = top_df[top_df['chainID'] == 0].index.values
  prot_pdb = pdb.atom_slice(prot_idxs)

  # and we write that out
  prot_pdb_path = osp.join(pdbfixer_dir, '4od0_protein.pdbfixer.pdb')
  prot_pdb.save_pdb(prot_pdb_path)

  # for the ligand we use the residue name which is '2RV'
  lig_idxs = top_df[ top_df['resName'] == '2RV'].index.values
  lig_pdb = pdb.atom_slice(lig_idxs)
  lig_pdb_path = osp.join(pdbfixer_dir, '4od0_2rv.pdbfixer.pdb')
  lig_pdb.save_pdb(lig_pdb_path)

  # now for the solvent which is just everything that isn't the other ones
  all_idxs = top_df.index.values
  solvent_idxs = list(set(all_idxs).difference(set(prot_idxs), set(lig_idxs)))
  solvent_pdb = pdb.atom_slice(solvent_idxs)
  solvent_pdb_path = osp.join(pdbfixer_dir, '4od0_solvent.pdbfixer.pdb')
  solvent_pdb.save_pdb(solvent_pdb_path)


  # we also want just the water
  water_idxs = top_df[ top_df['resName'] == 'HOH'].index.values
  water_pdb = pdb.atom_slice(water_idxs)
  water_pdb_path = osp.join(pdbfixer_dir, '4od0_water.pdbfixer.pdb')
  water_pdb.save_pdb(water_pdb_path)

  # and a separate one for just the ions
  ions_idxs = top_df[ (top_df['resName'] == 'MG') | (top_df['resName'] == 'PO4') ].index.values
  ions_pdb = pdb.atom_slice(ions_idxs)
  ions_pdb_path = osp.join(pdbfixer_dir, '4od0_ions.pdbfixer.pdb')
  ions_pdb.save_pdb(ions_pdb_path)


  # we also only want the crystallographic waters from the domain of
  # interest (DOI), we manually get the idxs for those by other means
  # and make just a pdb of that here

  # to do this we visualized the waters from the waters only file made
  # above and got the indices from it of which waters were used
  DOI_water_idxs = [15, 7, 4, 6, 12, 5, 9, 11, 1]

  # so we use this to generate another file
  DOI_water_pdb = water_pdb.atom_slice(DOI_water_idxs)
  DOI_water_pdb_path = osp.join(pdbfixer_dir, '4od0_DOI_water.pdbfixer.pdb')
  DOI_water_pdb.save_pdb(DOI_water_pdb_path)

  # we also combine this with the protein to make a file of just the
  # protein and the DOI waters
  prot_DOI_water_pdb = prot_pdb.stack(DOI_water_pdb)
  prot_DOI_water_pdb_path = osp.join(pdbfixer_dir, '4od0_prot_DOI_water.pdbfixer.pdb')
  prot_DOI_water_pdb.save_pdb(prot_DOI_water_pdb_path)
#+END_SRC

*** Preparing files for docking with smina

To do docking we need files in the PDBQT format which is a specialized
version of the PDB format that just adds columns for atom partial
charge and type.

Its such a silly thing but we have to do it. I have an issue with how
partial charges are actually calculated and how they migh tbe
different depending on different algorithms. However, I am constrained
by which programs I can get to run that produce correct outputs.

I have to convert the following files:
- protein
- reference ligand (TPPU)
- each test ligand

Using openbabel the file format adds in these strange rows referring
to branches and other things. I don't really know why this is but it
seems suspicious and probably for some feature of AutoDock that I
don't know how to use and probably don't want to use. 

I tried using MDAnalysis but it doesn't write the partial charges and
it still works in smina with all of these things in it.

#+BEGIN_SRC bash :tangle prep/make_receptor_pdbqts.bash
  obabel -ipdb 4od0_protein.pdbfixer.pdb -opdbqt -O4od0_protein.pdbfixer.obabel.pdbqt
  obabel -ipdb 4od0_2rv.pdbfixer.pdb -opdbqt -O4od0_2rv.pdbfixer.obabel.pdbqt
#+END_SRC

*** Setup environment on HPCC

We first generate a template script and directory structure for single
simulation tasks.

DO this by pulling the job template and customizing. This will then be
copied and further customized for each ligand.

Once this is done proceed with the rest.


Organization:
- seh.pathway_hopping
  - simulations
    - X_simulations (job_template)
  - root_orchs
  - slurm_jobdir_template
  - analysis
    - X_analysis

#+BEGIN_SRC bash
PROJECTS_DIR="/mnt/home/lotzsamu/projects"
PROJECT_DIR="${PROJECTS_DIR}/seh.pathway_hopping"
mkdir -p ${PROJECT_DIR}
cd ${PROJECT_DIR}
TEMPLATE_DIR="${PROJECT_DIR}/job_submission_template"
ORCH_DIR="${PROJECT_DIR}/root_orchs"
SIM_DIR="${PROJECT_DIR}/simulations"
ANALYSIS_DIR="${PROJECT_DIR}/analysis"
ENV_DIR="${PROJECT_DIR}/env"
TEST_DIR="${PROJECT_DIR}/test_run"
mkdir -p ${ORCH_DIR}
mkdir -p ${SIM_DIR}
mkdir -p ${ANALYSIS_DIR}
mkdir -p ${ENV_DIR}
mkdir -p ${TEST_DIR}
for LIG_ID in 3 10 18 20; do
  mkdir -p "${SIM_DIR}/${LIG_ID}_simulations"
  mkdir -p "${ANALYSIS_DIR}/${LIG_ID}_analysis"
  cp -rf ${TEMPLATE_DIR}/* "${SIM_DIR}/${LIG_ID}_simulations/"
done
#+END_SRC


Then we need to make a reproducible virtual environment. We have
developed the environment we need in the form of a YAML file that
conda can read. This is declared above. So we copy that and run it.


From the staging repo computer:

#+BEGIN_SRC bash
STAGING_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/sim_staging"
HPCC_URL="lotzsamu@hpcc.msu.edu"
PROJECTS_DIR="/mnt/home/lotzsamu/projects"
PROJECT_DIR="${PROJECTS_DIR}/seh.pathway_hopping"
ENV_DIR="${PROJECT_DIR}/env"
scp ${STAGING_DIR}/* "${HPCC_URL}:${ENV_DIR}/"
#+END_SRC

From the cluster:
#+BEGIN_SRC bash
PROJECTS_DIR="/mnt/home/lotzsamu/projects"
PROJECT_DIR="${PROJECTS_DIR}/seh.pathway_hopping"
ENV_DIR="${PROJECT_DIR}/env"
ENV_NAME="seh_pathway_hopping"
cd $ENV_DIR
conda env remove -y -n seh_pathway_hopping
conda env create -y -n "${ENV_NAME}" -f seh_pathway_hopping.env.yaml
#+END_SRC


*** Scripts for ligand systems setup workflow:


**** Generate 3D ligand coordinates

#+BEGIN_SRC python :tangle prep/embed_ligand_3d.py
  import sys
  import os
  import os.path as osp

  from rdkit import Chem
  from rdkit.Chem import AllChem

  # argument of file to use
  mol2_path = sys.argv[1]

  # read
  mol = Chem.MolFromMol2File(mol2_path, removeHs=False, sanitize=True)

  # embed in 3D using Distance Geometry
  AllChem.EmbedMolecule(mol)

  # optimize using the UFF force field
  AllChem.UFFOptimizeMolecule(mol)

  # make a new path adding the new processing segment on
  new_suffix_component = 'rdkit_DG-embed_UFF-optimize'
  filename = mol2_path.split('/')[-1]
  f_segs = filename.split('.')
  prefix = f_segs[0]
  ext = f_segs[-1]
  process_segs = f_segs[1:-1]
  process_segs.append(new_suffix_component)
  new_filename = "{}.{}.{}".format(prefix, '.'.join(process_segs), 'pdb')
  new_path = osp.join(osp.realpath(osp.curdir), new_filename)

  # write
  Chem.MolToPDBFile(mol, new_path)
#+END_SRC

**** Assemble receptor and ligand after docking and normalizing

#+BEGIN_SRC python :tangle prep/assemble_receptor_ligand.py
  import os
  import os.path as osp
  import sys

  from setup_paths import *

  import mdtraj as mdj

  lig_path = sys.argv[1]

  receptor_pdb_path = osp.join(pdbfixer_dir, '4od0_prot_DOI_water.pdbfixer.pdb')

  no_lig_sys = mdj.load_pdb(receptor_pdb_path)

  lig_filename = osp.basename(lig_path)
  prefix = lig_filename.split('.')[0]
  docked_lig_filename = "{}_4od0_docked.pdb".format(prefix)
  docked_lig_path = osp.join(osp.realpath(osp.curdir), docked_lig_filename)

  # we load it and take just the first frame
  lig = mdj.load_pdb(lig_path)[0]

  # combine them into the same trajectory
  docked_lig = no_lig_sys.stack(lig)

  # then write it out
  docked_lig.save_pdb(docked_lig_path)
#+END_SRC



**** Clean Topologies from PSF

These files are really stinky bad and we need to make them better.

So we are going to put new serial numbers on them and reindex the residue serial numbers

#+BEGIN_SRC python :tangle prep/fix_pdb_psf.py
  import sys
  from copy import copy, deepcopy
  import itertools as it
  import json

  import numpy as np
  import pandas as pd

  import parmed as pmd
  import simtk.openmm.app as omma
  import simtk.openmm as omm
  import mdtraj as mdj

  from wepy.util.mdtraj import mdtraj_to_json_topology, json_to_mdtraj_topology
  from wepy.util.json_top import json_top_subset


  # ff names, these are always the same
  prot_rtf = "top_all36_prot.rtf"
  prot_prm = "par_all36m_prot.prm"

  cgenff_rtf = "top_all36_cgenff.rtf"
  cgenff_prm = "par_all36_cgenff.prm"

  solvent_str = "toppar_water_ions.str"

  lig_rtf = "unl.rtf"
  lig_prm = "unl.prm"

  # get inputs
  pdb_path = sys.argv[1]
  psf_path = sys.argv[2]
  lig_id = sys.argv[3]
  pdb_output_path = sys.argv[4]
  json_output_path = sys.argv[5]
  coords_output_path = sys.argv[6]

  csv_template = "sEH_lig-{}_system.{}.csv".format(lig_id, '{}')

  # automatically generate the output files for the CSVs
  master_atom_csv_path = csv_template.format('atoms')
  master_bond_csv_path = csv_template.format('bonds')

  # load the PSF
  omm_psf = omma.CharmmPsfFile(psf_path)
  pmd_psf = pmd.charmm.CharmmPsfFile(psf_path)


  # load the parameters with the openmm reader, the Parmed one won't do
  # it
  omm_params = omma.CharmmParameterSet(prot_rtf, prot_prm,
                                       cgenff_rtf, cgenff_prm,
                                       solvent_str,
                                       lig_rtf, lig_prm)

  omm_psf.loadParameters(omm_params)

  # load the PDB file with parmed and mdtraj
  pmd_pdb = pmd.load_file(pdb_path)
  mdj_pdb = mdj.load_pdb(pdb_path)

  # for discovery etc. lets make dataframes of each of these:
  psf_df = pmd_psf.to_dataframe()
  pmd_pdb_df = pmd_pdb.to_dataframe()
  mdj_pdb_df = mdj_pdb.top.to_dataframe()[0]

  # to make visualization for indexing grouping easier
  relevant_columns = ['number', 'atomic_number', 'name', 'type', 'mass', 'charge',
                      'resname', 'resid', 'resnum', 'chain', 'segid']

  # set which fields to choose from each of the different data
  # sources. We use parmed for some things and mdtraj for other things
  pmd_pdb_fields = ('atomic_number', 'mass', 'xx', 'xy', 'xz')
  mdj_pdb_fields = ('serial', 'chainID')

  # these aren't used but for reference
  psf_fields = ('name', 'type', 'charge', 'resname', 'resid', 'segid', 'tree')

  # for all the other fields, if I find I need them I will get them from
  # the PSF

  # for this we use the openmm psf because it can read my parameters

  # so copy the PSF and then swap out the columns for the Parmed PDB
  # ones
  struct_df = copy(psf_df)

  # parmed gets the atomic number wrong for SOD sodiums so we correct it
  atomic_numbers = pmd_pdb_df['atomic_number'].values
  for row_idx, row in pmd_pdb_df[pmd_pdb_df['resname'] == "SOD"].iterrows():
      atomic_numbers[row_idx] = 11

  # set the columns
  struct_df['atomic_number'] = atomic_numbers
  struct_df['mass'] = pmd_pdb_df['mass']
  struct_df['xx'] = pmd_pdb_df['xx']
  struct_df['xy'] = pmd_pdb_df['xy']
  struct_df['xz'] = pmd_pdb_df['xz']

  # so copy the PSF and then swap out the columns for the mdtraj ones
  struct_df['chain'] = mdj_pdb_df['chainID']
  struct_df['pdb_serial'] = mdj_pdb_df['serial']

  # so we make a new column in the dataframe from the index
  struct_df['charmm-gui_psf_idx'] = struct_df.index

  # we also want to make the element symbol column, and the symbols
  # arent in the PSF and mdtraj reads them wrong because we told it not
  # to infer for other reasons

  # so convert the atomic numbers to element symbols using MDTraj
  element_symbols_col = []
  for atomic_number in struct_df['atomic_number']:
      element = mdj.element.Element.getByAtomicNumber(atomic_number)
      element_symbols_col.append(element.symbol)
  struct_df['element'] = element_symbols_col

  # we also collect the bond, angle etc tables
  bond_table = []
  bond_columns = ['atom_1', 'atom_2', 'force_constant', 'equilibrium_distance']
  for bond in omm_psf.bond_list:

      # because for some reason the PSF has bonds between TIP3 hydrogens
      # we need to remove these manually

      # make sure we have the right residue
      if bond.atom1.residue.resname == "TIP3":

          # then check if both atoms are the hydrogens
          if bond.atom1.type.name == 'HT' and bond.atom2.type.name == 'HT':

              # don't add this bond
              continue

      row = (bond.atom1.idx, bond.atom2.idx,
             bond.bond_type.k, bond.bond_type.req)

      bond_table.append(row)

  bond_df = pd.DataFrame(bond_table, columns=bond_columns)


  # we also want to renumber the: 'number', 'resid', and 'resnum' and we
  # also want to organize the file as a whole: ligand, crystallographic
  # waters, ions, solvent water, so we collect the rows which correspond
  # to those groups


  segments_order = ('protein', 'xray_waters', 'ions', 'ligand', 'solvent')

  lig_resname = "UNL"
  xray_segid = "WATA"
  ions_resname = "SOD"
  prot_segid = "PROA"
  solvent_segid = 'SOLV'


  segments_rows = {'ligand' : [], 'xray_waters' : [],
                   'ions' : [], 'protein' : [], 'solvent' : []}

  for row_idx, row in struct_df.iterrows():

      if row['resname'] == lig_resname:
          segments_rows['ligand'].append(row)

      elif row['resname'] == ions_resname:
          segments_rows['ions'].append(row)

      elif row['segid'] == xray_segid:
          segments_rows['xray_waters'].append(row)

      elif row['segid'] == prot_segid:
          segments_rows['protein'].append(row)

      elif row['segid'] == solvent_segid:
          segments_rows['solvent'].append(row)

      else:
          raise ValueError()

  # combine them into a dataframe in the order
  new_rows = []
  for seg_key in segments_order:
      seg_rows = segments_rows[seg_key]
      new_rows.extend([list(row) for row in seg_rows])

  grouped_df = pd.DataFrame(new_rows, columns=struct_df.columns)

  # now that we have the major sections in a good order, we need to
  # reindex everything

  # start with the atoms by resetting the actual DF index and the
  # 'number' column
  atom_index = range(grouped_df.shape[0])
  grouped_df.index = atom_index
  grouped_df['number'] = atom_index

  # we also need to get the correct residue numbering so we use a key of
  # (segid, resname, residue number)
  residue_idxs_col = []
  curr_res_idx = 0
  last_reskey = None
  for atom_idx, row in grouped_df.iterrows():

      reskey = (row['segid'], row['resname'], row['resnum'])

      if last_reskey is None:
          last_reskey = reskey
          residue_idxs_col.append(curr_res_idx)
          continue

      if reskey != last_reskey:
          curr_res_idx += 1

      last_reskey = reskey
      residue_idxs_col.append(curr_res_idx)

  # add the column as the residue_idx
  grouped_df['residue_idx'] = residue_idxs_col

  grouped_df.to_csv(master_atom_csv_path)

  # want a good alias for this
  atom_df = grouped_df

  # make an easy mapping of indices
  atom_idx_map = {row['charmm-gui_psf_idx'] : row_idx
                  for row_idx, row in grouped_df.iterrows()}

  # now we want to make the dataframes for the forcefield groupings but
  # we need to translate their atom indices to the new ones
  column_names = [column_name for column_name in bond_df.columns
                  if column_name.startswith('atom_')]

  # for each column we translate all the indices
  for column_name in column_names:
      new_idxs = [atom_idx_map[i] for i in bond_df[column_name]]
      bond_df[column_name] = new_idxs


  # generate an MDTraj topology from the atom and bonds



  # for the atoms dataframe we need to have certain columns
  mdj_atom_columns = ('serial', 'name', 'element', 'resSeq', 'resName', 'chainID', 'segmentID')

  new_mdj_top_df = pd.DataFrame()

  # the new serial numbers are the numbering we gave
  new_mdj_top_df['serial'] = atom_df['number']
  # the atom names
  new_mdj_top_df['name'] = atom_df['name']
  # atom symbols
  new_mdj_top_df['element'] = atom_df['element']
  # residue numbers
  new_mdj_top_df['resSeq'] = atom_df['resid']
  # residue names
  new_mdj_top_df['resName'] = atom_df['resname']
  # chain IDs are not used so they all should be 0, this is becuause PDB
  # doesn't have enough space for all the chains and so we just use the
  # segids for the big sections
  new_mdj_top_df['chainID'] = atom_df['chain']
  # segids are used
  new_mdj_top_df['segmentID'] = atom_df['segid']

  # and we need the bonds as an array
  bond_array = bond_df[['atom_1', 'atom_2']].values

  # NOTE this has the effect of reindexing things making all our work
  # for nought so we just make the topology manually, using the table
  #mdj_fixed_top = mdj.Topology.from_dataframe(new_mdj_top_df, bond_array)
  mdj_fixed_top = mdj.Topology()

  # the hierarchy is chain -> residue -> atom, and then bonds are
  # separate and point to the atoms, so we start by making a single
  # chain to put everything in
  chain = mdj_fixed_top.add_chain()

  # then we will just go through the table and make new residues when we
  # need to

  # so we keep track of which residues are already made, the residue
  # keys are (segmentID, name, resSeq)
  reskeys = {}
  atoms = {}
  for row_idx, row in new_mdj_top_df.iterrows():

      reskey = (row['segmentID'], row['resName'], row['resSeq'])

      if not reskey in reskeys:
          # generate the residue
          residue = mdj_fixed_top.add_residue(row['resName'], chain, row['resSeq'], row['segmentID'])
          # and add it to the dictionary tracking residues
          reskeys[reskey] = residue

      # otherwise get the residue for this atom
      else:
          residue = reskeys[reskey]

      element = mdj.element.Element.getBySymbol(row['element'])
      atom = mdj_fixed_top.add_atom(row['name'], element, residue, row['serial'])

      # add it to a dictionary so we can get them for the bonds
      atoms[row_idx] = atom

  # now add all the bonds
  for a_idx, b_idx in bond_array:
      bond = mdj_fixed_top.add_bond(atoms[a_idx], atoms[b_idx])


  # serialize to json
  json_top_str = mdtraj_to_json_topology(mdj_fixed_top)
  with open(json_output_path, 'w') as wf:
      wf.write(json_top_str)


  # get the coordinates from the dataframe
  coords = atom_df[['xx', 'xy', 'xz']].values * .1

  fixed_mdj_traj = mdj.Trajectory([coords], topology=mdj_fixed_top)

  fixed_mdj_traj.save_pdb(pdb_output_path)

  # also write out a numpy format coordinate file
  np.savetxt(coords_output_path, coords)


  # DEBUG subset

  # json_top = json.loads(json_top_str)

  # # define the methods for getting ligand and proteins
  # def ligand_idxs(mdtraj_topology, ligand_resid):
  #     return mdtraj_topology.select('resname "{}"'.format(ligand_resid))

  # def protein_idxs(mdtraj_topology):
  #     return np.array([atom.index for atom in mdtraj_topology.atoms if atom.residue.is_protein])

  # lig_idxs = ligand_idxs(mdj_fixed_top, 'UNL')
  # prot_idxs = protein_idxs(mdj_fixed_top)

  # # then take a subset of the topology with the combination of the idxs,
  # # in both ways
  # sela_idxs = np.concatenate((lig_idxs, prot_idxs))
  # selb_idxs = np.concatenate((prot_idxs, lig_idxs))

  # # topologies
  # sela_json_top = json_top_subset(json_top_str, sela_idxs)
  # selb_json_top = json_top_subset(json_top_str, selb_idxs)

  # sela_mdj_top = json_to_mdtraj_topology(sela_json_top)
  # selb_mdj_top = json_to_mdtraj_topology(selb_json_top)

  # # coordinates
  # sela_coords = coords[sela_idxs]
  # selb_coords = coords[selb_idxs]

  # # write pdbs with both
  # mdj.Trajectory([sela_coords], topology=sela_mdj_top).save_pdb('sela_debug.FIXED.pdb')
  # mdj.Trajectory([selb_coords], topology=selb_mdj_top).save_pdb('selb_debug.FIXED.pdb')


  # # test whether reading and writing the JSON file has any effect on
  # # writing a correct PDB
  # mdj.Trajectory([coords], topology=debug_mdj_top).save_pdb('debug.FIXED.pdb')

  # with open(json_output_path, 'r') as rf:
  #     debug_json_top_str = rf.read()

  # debug_json_top = json.loads(debug_json_top_str)
  # debug_mdj_top = json_to_mdtraj_topology(debug_json_top_str)

  # # go through each json dict and get the atom dictionaries
  # atom_dicts = []
  # debug_atom_dicts = []

  # for chain_i, chain in enumerate(json_top['chains']):
  #     chain_idx = chain['index']
  #     for res_i, residue in enumerate(chain['residues']):
  #         residue_idx = residue['index']
  #         for atom_i, atom in enumerate(residue['atoms']):
  #             atom_idx = atom['index']
  #             atom_dicts.append(atom)

  # for chain_i, chain in enumerate(debug_json_top['chains']):
  #     chain_idx = chain['index']
  #     for res_i, residue in enumerate(chain['residues']):
  #         residue_idx = residue['index']
  #         for atom_i, atom in enumerate(residue['atoms']):
  #             atom_idx = atom['index']
  #             debug_atom_dicts.append(atom)



  # make a trajectory and write to PDB for visual checking of
  # correctness
#+END_SRC



**** Generate a ForceField from Charmm36 XML and infer atom types without PSF

#+BEGIN_SRC python :tangle prep/ffxml_infer_types.py

  import sys
  from copy import copy, deepcopy
  import itertools as it
  import json
  from collections import OrderedDict

  import pandas as pd

  import parmed as pmd
  import simtk.openmm.app as omma
  import simtk.openmm as omm
  import mdtraj as mdj

  from wepy.util.mdtraj import json_to_mdtraj_topology

  # ff names, these are always the same
  prot_rtf = "top_all36_prot.rtf"
  prot_prm = "par_all36m_prot.prm"

  cgenff_rtf = "top_all36_cgenff.rtf"
  cgenff_prm = "par_all36_cgenff.prm"

  solvent_str = "toppar_water_ions.str"

  lig_rtf = "unl.rtf"
  lig_prm = "unl.prm"

  # get inputs
  json_input_path = sys.argv[1]


  # load in the ligand topology
  lig_params = pmd.charmm.CharmmParameterSet(lig_rtf, lig_prm)

  # load the parameters into an OpenMMParameterSet from the Charmm
  # one. For my system at least I could ignore the warning about
  # dropping an atom because it isn't in the parameters. This is because
  # the atom types in the ligand topology come from teh charmm36 cgenff
  # parameters and those aren't loaded here. I tried loading them but
  # then there is an error when reading in the two xmls together about
  # multiple definitions for certain atoms. I found after testing that
  # the warning actually has no effect, so I am ignoring it.
  params = pmd.openmm.OpenMMParameterSet.from_parameterset(lig_params)

  # here you have an opportunity to rename the ligand if you choose
  params.residues['UNL'] = lig_params.residues["UNL"]

  # print("writing the xml")
  params.write('unl.xml')

  # generate the force field for CHARMM36
  ff = omma.ForceField('charmm36.xml', 'charmm36_solvent.xml', 'unl.xml')

  # debugging the matching of residues

  # load the json and conver to mdtraj
  with open(json_input_path, 'r') as rf:
      json_top_str = rf.read()

  mdj_top = json_to_mdtraj_topology(json_top_str)
  omm_top = mdj_top.to_openmm()


  unmatched_residues = ff.getUnmatchedResidues(omm_top)
  print("Unmatched residues", unmatched_residues)

  system = ff.createSystem(omm_top)

#+END_SRC

*** Workflow Template: Docking and generating the system files

**** Choose ligand to prepare

We have Ids for each ligand so we just set a shell variable for it so
we can make these commands general

#+BEGIN_SRC bash
LIG_ID="17"
#+END_SRC

**** Canonicalize ligand MOL2 file

This is necessary so that we can upload files to CHARMM-GUI in a
consisten atom ordering. THe openbabel canonical ordering suffices for
this.

We need to make sure we are using the right openbabel version for
which I have set up a virtualenv for it.

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/ligands
conda activate openbabel
obabel -i mol2 $LIG_ID.marvin_Hs.obabel.mol2 -o mol2 -O $LIG_ID.marvin_Hs.obabel_canonical.mol2 --canonical
cp $LIG_ID.marvin_Hs.obabel_canonical.mol2 ../ligands_3D/
#+END_SRC

**** Embed the ligand in 3D and optimize 3D coordinates

Generate the 3D coordinates from the mol2 using RDKit:

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/ligands_3D
conda activate omnia
python ../../embed_ligand_3d.py $LIG_ID.marvin_Hs.obabel_canonical.mol2
#+END_SRC

**** Prepare docking (smina) input file for ligand

Convert the PDB of the 3D coordinates to a PDBQT with canonical atom
ordering that preserves the hydrogens (-xh). Make sure you do not use
the -xn option at this point or the charges will all be set to 0. We
also do not use the -xr for rigid structure because smina needs the
flexible chains for it to work. Make sure you are using an up to date
version of openbabel I am using v2.4.0 for this to work and has been
known not to work with v2.3.2 (which is the package on debian at time
of writing):

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/ligands_3D
conda activate openbabel
obabel -i pdb $LIG_ID.marvin_Hs.obabel_canonical.rdkit_DG-embed_UFF-optimize.pdb -o pdbqt -O $LIG_ID.marvin_Hs.obabel_canonical.rdkit_DG-embed_UFF-optimize.obabel.pdbqt -xh
cp $LIG_ID.marvin_Hs.obabel_canonical.rdkit_DG-embed_UFF-optimize.obabel.pdbqt /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking/ligand_inputs/
#+END_SRC


To do docking we will need the appropriate input files which we will
assume have been made and discussed elseqhere.

**** Dock with smina

Then we perform docking with smina:

#+BEGIN_SRC bash
  cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking
  conda activate openbabel


  smina -r 4od0_protein.pdbfixer.obabel.pdbqt \
      -l ligand_inputs/$LIG_ID.marvin_Hs.obabel_canonical.rdkit_DG-embed_UFF-optimize.obabel.pdbqt \
      --autobox_ligand 4od0_2rv.obabel.pdbqt \
      --cpu 6 \
      -o ligand_outputs/$LIG_ID.smina.pdb
#+END_SRC

**** Choose model and Normalize results

Just look at them in VMD and choose a model to use.

Now we need to normalize this data and choose a model. We know the
first model is good so we just use it. To normalize we just read and
write with mdtraj.

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking
conda activate omnia
mdconvert ./ligand_outputs/$LIG_ID.smina.pdb -o ./ligand_outputs/$LIG_ID.smina.mdtraj.pdb
#+END_SRC

Just so you don't forget to read the text here.
#+BEGIN_SRC bash
vmd
#+END_SRC


For this one we choose the first model.

For this we just choose it with mdconvert
#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking
conda activate omnia
MODEL_IDX="0"
mdconvert ./ligand_outputs/$LIG_ID.smina.pdb -o ./assemblies/$LIG_ID.smina.mdtraj_best-model.pdb -i $MODEL_IDX
#+END_SRC

Then canonicalize the ligand that we just made a PDB for.

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking
obabel -i pdb ./assemblies/$LIG_ID.smina.mdtraj_best-model.pdb -o pdb -O ./assemblies/$LIG_ID.smina.mdtraj_best-model.obabel_canonical.pdb --canonical
#+END_SRC

**** Generate Assembly

Now that we have normalized them we need to combine the PDB of the
docked ligand with the protein.

#+BEGIN_SRC bash
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/docking/assemblies
conda activate omnia
python ../../../assemble_receptor_ligand.py $LIG_ID.smina.mdtraj_best-model.obabel_canonical.pdb
#+END_SRC


**** CHARMM-GUI: Solvation, Truncation, and output files

Next we upload to CHARMM-GUI giving it this docked file and the
canonicalized mol2 file:
- $LIG_ID_4od0_docked.pdb
- $LIG_ID.marvin_Hs.obabel_canonical.mol2

And truncate the protein going from residue: 231 to 547

And we also add the solvent.

For this we need to know the box size.

I tried doing the cutoff distance like convpdb but the results were
different and CHARMM-GUI produced something that was smaller than the
convpdb results.

So I measured the results of the water box made with convpdb and input
that directly for the length of the box for CHARMM-GUI to make.

THe lengths of the box were 84 Angstroms or 8.4 nanometers.

We also add 7 sodium ions (residue name: SOD).

Save to:

/home/salotz/tree/lab/projects/seh.pathway_hopping/data/charmm-gui_solvated_assemblies

We need to repair the mistakes in these files so we move them to
another folder (the force fields and topology: ff_top) to do this.

Namely we need to get the relevant CHARMM36 parameter files from the
general use toppar dir, the ligand parameters from cgenff, the
starting coordinates PDB file, and the topology PSF file (specifically
the step2_solvator.psf because it has an exteded column format and the
step2_solvator.xplor.psf doesn't and this causes an error in the
residue numbering for having a number of residues over 10,000, due to
column width constraints or something).

Then we copy the necessary parts and rename them if needed:

#+BEGIN_SRC bash
TOPPAR_DIR=/home/salotz/tree/lab/projects/seh.pathway_hopping/data/ff_top/toppar
FF_TOP_DIR=/home/salotz/tree/lab/projects/seh.pathway_hopping/data/ff_top/${LIG_ID}
mkdir -p $FF_TOP_DIR
cp ${TOPPAR_DIR}/* ${FF_TOP_DIR}/
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/charmm-gui_solvated_assemblies/$LIG_ID
cp unl/unl.prm ${FF_TOP_DIR}/
cp unl/unl.rtf ${FF_TOP_DIR}/
cp step2_solvator.pdb ${FF_TOP_DIR}/sEH_lig-"$LIG_ID"_system.pdb
cp step2_solvator.psf ${FF_TOP_DIR}/sEH_lig-"$LIG_ID"_system.psf
#+END_SRC


**** Normalize the topologies and generate ligand forcefield

Because the files we get from CHARMM-GUI have numerous issues with
them making them unsuitable for civil simulations we have to work
them.

The first step is to read in atom and topology information from both
the PDB and the PSF. These will be read by different programs in order
to extract all of the different information available. The atom and
residue indexing will be normalized to ensure no problems can happen
in the future and a separate CSV format for both atoms and bonds will
be written for cross-reference if needed. Further, initial coordinates
will be written to a numpy binary file for easy and PDB-parsing
experiences.

Problems with the input files that must be dealt with:
- indexing leading to ambiguous (in the eyes of some parsers)
  topologies that make conjoined molecules
- incorrect bonding of water hydrogens in the PSF topology
- incorrect recognition of the ion elements

#+BEGIN_SRC bash
FF_TOP_DIR=/home/salotz/tree/lab/projects/seh.pathway_hopping/data/ff_top/${LIG_ID}
mkdir -p ${FF_TOP_DIR}
cd ${FF_TOP_DIR}
cp ../charmm36.xml ./
cp ../charmm36_solvent.xml ./
conda activate sim_prep
python ../../../prep/fix_pdb_psf.py sEH_lig-${LIG_ID}_system.pdb sEH_lig-${LIG_ID}_system.psf ${LIG_ID} \
                                    sEH_lig-${LIG_ID}_system.FIXED.pdb \
                                    sEH_lig-${LIG_ID}_system.top.json \
                                    sEH_lig-${LIG_ID}_system.coords.txt
python ../../../prep/ffxml_infer_types.py sEH_lig-${LIG_ID}_system.top.json
EQUIL_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/equilibration/${LIG_ID}"
mkdir -p ${EQUIL_DIR}
cp sEH_lig-${LIG_ID}_system.coords.txt ${EQUIL_DIR}/
cp sEH_lig-${LIG_ID}_system.top.json ${EQUIL_DIR}/
cp charmm36_solvent.xml ${EQUIL_DIR}/
cp charmm36.xml ${EQUIL_DIR}/
cp unl.xml ${EQUIL_DIR}/
#+END_SRC

**** Minimize and Equilibrate

Before we set up the simulation with the initial conditions we want to
take our assembly and prepare it for production simulations by
minimizing and equilibrating it.

For this we use just plain openmm and produce a file that can then be
read for the initial positions needed to create the orchestrator.

#+BEGIN_SRC bash
EQUIL_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/equilibration/${LIG_ID}"
mkdir -p ${EQUIL_DIR}
cd ${EQUIL_DIR}
conda activate seh_pathway_hopping
RESID="UNL"
python -m seh_prep.equilibrate "sEH_lig-${LIG_ID}_system.coords.txt" "sEH_lig-${LIG_ID}_system.top.json" $RESID "charmm36.xml" "charmm36_solvent.xml" "unl.xml" $LIG_ID
MD_SYSTEMS_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_systems/${LIG_ID}"
mkdir -p $MD_SYSTEMS_DIR
# copy the things needed for generating the orchestrator
cp "sEH_lig-${LIG_ID}_system.top.json" ${MD_SYSTEMS_DIR}/
cp "sEH_lig-${LIG_ID}_equilibrated.state.pkl" ${MD_SYSTEMS_DIR}/
# force fields
cp charmm36_solvent.xml ${MD_SYSTEMS_DIR}/
cp charmm36.xml ${MD_SYSTEMS_DIR}/
cp unl.xml ${MD_SYSTEMS_DIR}/
#+END_SRC

Make sure to check the equilibrated state visually to see that it is
okay and nothing crazy happened!


#+BEGIN_SRC bash
vmd "sEH_lig-${LIG_ID}_equilibrated.pdb"
#+END_SRC

**** Building the Orchestrator

Now we want to make an orchestrator for the ligand to be able to run
simulations:

#+BEGIN_SRC bash
RESID="UNL"
MD_SYSTEMS_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_systems/${LIG_ID}"
NUM_WALKER=48
cd ${MD_SYSTEMS_DIR}
#conda activate seh_prep
#conda activate common_dev
python -m seh_prep.gen_orchestrator sEH_lig-${LIG_ID}_equilibrated.state.pkl sEH_lig-${LIG_ID}_system.top.json ${RESID} charmm36.xml charmm36_solvent.xml unl.xml ${LIG_ID} ${NUM_WALKERS}

TEST_RUN_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/$LIG_ID"
mkdir -p $TEST_RUN_DIR
cp sEH_lig-$LIG_ID.orch.sqlite $TEST_RUN_DIR/root.orch.sqlite

# move one to the root_orch dir
cp sEH_lig-$LIG_ID.orch.sqlite "$HOME/tree/lab/resources/project-resources/seh.pathway_hopping/hpcc/root_orchs/sEH_lig-$LIG_ID.orch.sqlite"
#+END_SRC


**** [[*Generate Configurations][Generate Configurations]]

**** Patching an Orchestrator

During development (or with bugfixes) you might run into a scenario
where you want to run new code that requires the addition of some
attributes to the objects that you pickled.

***** COMMENT Patch for v2

Apply the patching function to an orchestrator by calling the function
defined below:


****** local tests

Apply it to the old_orchs for the test run.

#+BEGIN_SRC bash
conda activate seh_pathway_hopping
old_orch_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
for lig_id in 3 10 18 20; do
    old_orch_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
    orch_path="${old_orch_dir}/sEH_lig-${lig_id}.orch"
    new_orch_path="${old_orch_dir}/sEH_lig-${lig_id}_v012-PATCH.orch"
    echo "Patching root orch ${lig_id}"
    python patch_orch_v012.py $orch_path $new_orch_path
    echo "done"
done
#+END_SRC



Test it on one that has a run actually in it:

#+BEGIN_SRC bash
LIG_ID=3
conda activate seh_pathway_hopping
TMP_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/tmp"
ORCH_PATH="${TMP_DIR}/sEH_lig-${LIG_ID}.orch"
NEW_ORCH_PATH="${TMP_DIR}/sEH_lig-${LIG_ID}_v012-PATCH.orch"
python patch_orch_v012.py $ORCH_PATH $NEW_ORCH_PATH
#+END_SRC

****** HPCC

Update the script on HPCC:


#+BEGIN_SRC bash
hpcc_url="lotzsamu@hpcc.msu.edu"
projects_dir="/mnt/home/lotzsamu/projects"
project_dir="${projects_dir}/seh.pathway_hopping"
hpcc_script_dir="${project_dir}/scripts"
rsync -avz "patch_orch_v012.py" "${hpcc_url}:${hpcc_script_dir}/patch_orch_v012.py" 2> /dev/null
#+END_SRC

Apply it to the root orchs:

#+NAME: patch-root-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch_root_orchs () {
      conda activate seh_pathway_hopping
      projects_dir="/mnt/home/lotzsamu/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"
      hpcc_script_dir="${project_dir}/scripts"
      root_orch_dir="${project_dir}/root_orchs"
      for lig_id in 3 10 18 20; do
          orch_path="${root_orch_dir}/sEH_lig-${lig_id}.orch"
          new_orch_path="${root_orch_dir}/sEH_lig-${lig_id}_v012-PATCH.orch"
          echo "Patching root orch ${lig_id}"
          python "${hpcc_script_dir}/patch_orch_v012.py" $orch_path $new_orch_path
          echo "done"
      done
  }
#+END_SRC

Patch each job's orchestrator:

#+NAME: patch-job-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-job-orchs () {

       conda activate seh_pathway_hopping
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       sim_dir="${project_dir}/simulations"
       scripts_dir="${project_dir}/scripts"

       for lig_id in 3 10 18 20; do
           lig_sim_dir="${sim_dir}/${lig_id}_simulations"
           lig_job_dir="${lig_sim_dir}/jobs"

           # patch all of them in the folder
           nargs=$#
           let "njobargs = $nargs - 1"
           job_ids=("${@:2:$njobargs}")

           # then loop over them to get the paths to the orchestrators from
           # them
           echo ""
           echo "Getting Paths from ${lig_sim_dir}/jobs"
           echo $(ls "${lig_sim_dir}/jobs")
           orch_paths=()
           job_counter=0
           for jdir in $(ls "${lig_sim_dir}/jobs"); do
               job_dir="${lig_sim_dir}/jobs/${jdir}"

               # we need to rename them to the original folder that the
               # orchestrator is expecting. Instead of the name of the output
               # folder my job submission system renames it to. If this was
               # already done ignore these and their messages.
               output_dir="${job_dir}/output"
               if ! (mv ${output_dir} ${job_dir}/exec); then echo "${job_dir} already renamed"; fi
               output_dir="${job_dir}/exec"

               # from the results folder for this job
               sim_results_dir=(${output_dir}/*-${lig_id})

               # find the orchestrator the simulation produced (if at all)
               # this will either be a finished one or a checkpoint
               test_orch=(${sim_results_dir}/*.orch)
               if [[ -e $test_orch ]]; then
                   echo "Found an orchestrator for $job_dir"

                   # if the job succeeded there will be a file generated which is
                   # the orchestrator for the end of that simulation
                   sim_orch_path=$test_orch

               elif [[ -e "${sim_results_dir}/checkpoint.chk" ]]; then
                   echo "Found a checkpoint for $job_dir"
                   # if it failed we want to use the checkpoint for this job
                   # (which is also an orchestrator)
                   sim_orch_path=${sim_results_dir}/checkpoint.chk

               else
                   echo "No orchestrator found for JOB ${job_dir}";
                   sim_orch_path="None"

               fi

               # and save it in the array of the orchestrator paths
               orch_paths[job_counter]=${sim_orch_path}
               job_counter=$((job_counter + 1))
           done

           echo ""
           echo "orchestrators being patched"
           for orch in ${orch_paths[@]}; do
               echo "$orch"
           done
           echo "fin"

           sleep 15

           echo "patching orchestrators"
           for orch in ${orch_paths[@]}; do
               new_orch="${orch}.PATCH_V2"
               echo "patching $orch to ${new_orch}"
               python ${scripts_dir}/patch_orch_v012.py ${orch} ${new_orch}
           done

       done


  }
#+END_SRC

Apply patch to the master orchs.

#+NAME: patch-master-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-master-orchs () {
      conda activate seh_pathway_hopping

      projects_dir="/mnt/home/lotzsamu/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"
      sim_dir="${project_dir}/simulations"
      for lig_id in 3 10 18 20; do
          lig_sim_dir="${sim_dir}/${lig_id}_simulations"
          input_dir="${lig_sim_dir}/input"
          orch_dir="${lig_sim_dir}/orchs"
          mkdir -p "${orch_dir}"
          # make a patched orchestrator, under a different name
          orch_path="${input_dir}/sEH_lig-${lig_id}.orch"
          new_orch_filename="sEH_lig-${lig_id}_v012-PATCH.orch"
          new_orch_path="${input_dir}/${new_orch_filename}"
          echo "Patching master orch ${lig_id}"
          python "${hpcc_script_dir}/patch_orch_v012.py" $orch_path $new_orch_path
          echo "done"
          backup_old_orch_filename="sEH-lig-${lig_id}_OLD_BACKUP.orch"
          echo "copying the old orchestrator to ${orch_dir} as ${backup_old_orch_filename}"
          cp ${orch_path} ${orch_dir}/${backup_old_orch_filename}
          new_orch_backup_filename="sEH_lig-${lig_id}_v012-PATCH_BACKUP.orch"
          echo "copying explicitly named patched orch to ${orch_dir} as ${new_orch_backup_filename}"
          cp ${new_orch_path} ${orch_dir}/${new_orch_backup_filename}
          echo "renaming the new patched orch to the master orch name in the inputs"
          mv ${new_orch_path} ${orch_path}
      done
  }
#+END_SRC


****** script


Patching in old cycle_idxs for runs

In order to get correct reconiliation of the HDF5 files in contigtrees
we need to account for the fact that more data is collected in the
HDF5s after the generation of a checkpoint. So when we reconcile the
HDF5s we are just going to clip out the data after the checkpoint that
ended up getting restarted.

However, simulations I ran did not have the cycle indices saved into
the orchestrators as run metadata.

The mechanism to do this in the future has been implemented, but old
orchestrators will have to be patched.

The first step is to figure out exactly which cycles correspond to
each run in the orchestrators.

Using the number of cycles for each run and the checkpoint frequency
we figure this out for each run.

So first we collect the information about the number of cycles and
relate that to the end hash for each run for each ligand.

#+NAME: patch_orch_v012
#+BEGIN_SRC python :tangle prep/patch_orch_v012.py
  from warnings import warn
  from wepy.reporter.restree import ResTreeReporter

  # the frequency checkpoints were made
  CHECKPOINT_FREQ = 25

  # the number of cycles that would appear in the HDF5 for each run and
  # the hash for that run
  lig_run_full_n_cycles = {3 : [(241, "1b0a5c2dc8d36ab234935a7c6b3cfc44"),
                           (155, "608107bdef0980187daa7c66c125e854"),
                           (677, "8cd97e8583f682845106ff1456916318"),
                           (685, "b6119f14a8656e9b7099865a6bf443bd"),
                           (714, "d005a8ff3725cd638d8d5f64d0106f39"),
                           (739, "81ec62755ca3eb30c38390eab5c33b2b"),
                           (693, "25b4dd9712ae9d113514d31bc4a69ade"),
                           (688, "8725e396deb0cad3904b05f64a73fe8f"),
                           (725, "b43cc17d8c17d88be1167be2515bd8ba")],
                      10 : [(236, "ce2a2734998ccf791b4215a9445c2b6a"),
                            (721, 'c9ec78f6860688ae95c975843ba5f140'),
                            (667, 'f8747a492a5290bda59a38de9839e018'),
                            (699, '91ab23ca0bc0309a1ce83ad88835d1a0'),
                            (727, 'ce87c6ceb1ea44bb833af5a94c286cc8'),
                            (645, 'ab3a8db0ae33557ac935c3928ed65287'),
                            (725, 'fa1d93b340c79c40fcc5a89608362f38')],
                      18 : [(105, '9146738e382e8bd242e2c3e0a697405e'),
                            (29, '6f312aec9af87c012bffd75184947923'),
                            (295, '616d9451c3ad5325dcf672545a14c169'),
                            (404, '00a9082b272c62d28a5c724548f0583a'),
                            (727, '72fdb473eb9eb511dc69328caf35281a'),
                            (738, '9198cc6c498ea77037f64353beebfee5'),
                            (733, '5e0c9be0d6e118a897b525b4924339b9'),
                            (11, 'e886aa40fcb5f78d8bbe521dd6769c0d')],
                      20 : [(232, '46d469e48c99453e826f324ee36bee2d'),
                            (756, '488bf6840eb0c3e07d313e3d735a84e9'),
                            (710, '40b780f64c331e9d736a8f30a05c473b'),
                            (743, '2fa1e6b19b3ca5b89c56d5534ed02bbc'),
                            (747, '53e0a9f0d952864d7003d64121feb082'),
                            (724, '8dcee38988335bf42a3ef9825d166166'),
                            (730, '36cac1e918ff24123c8371c4330c579b')]}



  # which runs had a failure and were continued with a checkpoint
  lig_recovered_runs = {3 : ['608107bdef0980187daa7c66c125e854'],
                        10 : ['f8747a492a5290bda59a38de9839e018',
                              'ce87c6ceb1ea44bb833af5a94c286cc8'],
                        18 : ['9146738e382e8bd242e2c3e0a697405e',
                              '6f312aec9af87c012bffd75184947923',
                              '616d9451c3ad5325dcf672545a14c169',
                              '00a9082b272c62d28a5c724548f0583a',
                              'e886aa40fcb5f78d8bbe521dd6769c0d'],
                        20 : ['40b780f64c331e9d736a8f30a05c473b']}

  # determine the cycle_idx of each of the runs
  run_n_cycles = {}
  for lig_id, run_tups in lig_run_full_n_cycles.items():

      for n_cycles, run_hash in run_tups:

          # if it was a run that failed and will (or has been) recovered
          # from a checkpoint set the number of cycles to the correct
          # value that would be in the recovered continuation
          if run_hash in lig_recovered_runs[lig_id]:
              run_n_cycles[run_hash] = n_cycles - (n_cycles % CHECKPOINT_FREQ)

          # otherwise just set it to what it already was
          else:
              run_n_cycles[run_hash] = n_cycles


  RUNNER_FILER_IDX = 0
  BC_FILTER_IDX = 1
  RESAMPLER_FILTER_IDX = 2

  def patch_bc(bc):
      # retrieve the topology that is already stored in the unbdinding
      # bc and just put it where it needs to be in the new module
      bc._topology = bc.__dict__['topology']
      bc._ligand_idxs = bc.__dict__['ligand_idxs']
      bc._receptor_idxs = bc.__dict__['receptor_idxs']
      bc._initial_states = [bc.__dict__['initial_state']]
      bc._cutoff_distance = bc.__dict__['cutoff_distance']
      bc._initial_weights = [1/len(bc._initial_states) for _ in bc._initial_states]
      return bc

  def patch_apparatus(app, patch_specs):
      app.filters[BC_FILTER_IDX] = patch_bc(app.filters[BC_FILTER_IDX])
      return app

  def patch_config(config, patch_specs):

      for i, reporter in enumerate(config._reporters):

          if isinstance(reporter, ResTreeReporter):
              del reporter._ResamplingRecord
              del reporter._WarpingRecord

          config.reporters[i] = reporter

      return config

  def patch_orch(orch, patch_func, patch_specs):
      """General purpose patching function that just applies the patch
      function in-place to all of the apparatuses in the orchestrator"""


      # patch the run information in the orchestrator
      # first add an attribute for the run cycles dictionary
      orch._run_cycles = {}

      # then for each run we use the precomputed data on the number of
      # cycles for the run and set it for each run
      for start_hash, end_hash in orch.runs:
          orch._run_cycles[(start_hash, end_hash)] = run_n_cycles[end_hash]

      # patch the configurations if they are not to be ignored
      try:
          orch._configuration = patch_config(orch._configuration, patch_specs)
      except AttributeError:
          warn("skipping configuration {} since it has no reporters")

      for key, config in orch._run_configurations.items():
          try:
              orch._run_configurations[key] = patch_config(config, patch_specs)
          except AttributeError:
              warn("skipping configuration {} since it has no reporters")

      # patch the default apparatus
      orch._apparatus = patch_func(orch.default_apparatus, patch_specs)


      # patch the rest of the snapshots
      for key, snapshot in orch._snapshots.items():
          snapshot._apparatus = patch_func(snapshot.apparatus, patch_specs)
          orch._snapshots[key] = snapshot

      return orch



  if __name__ == "__main__":
      import sys
      import pickle

      orch_path = sys.argv[1]
      if orch_path == "--help":
          print("inputs: orch_path, new_orch_path")
      new_orch_path = sys.argv[2]

      with open(orch_path, 'rb') as rf:
          orch = pickle.load(rf)

      patch_specs = {}

      orch = patch_orch(orch, patch_apparatus, patch_specs)

      # save the orch in the new place
      with open(new_orch_path, 'wb') as wf:
          pickle.dump(orch, wf)
#+END_SRC



***** Patch for v3

For patching you need a special branch that has both of the
orchestrators in it:

#+BEGIN_SRC bash
git checkout old_orch_archive
#+END_SRC


****** HPCC


Patch a specific set of orchestrators.

#+NAME: patch-specific-lig-job-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-specific-lig-job-orchs () {

      lig_id=$1

       conda activate seh_pathway_hopping
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       sim_dir="${project_dir}/simulations"
       scripts_dir="${project_dir}/scripts"

       lig_sim_dir="${sim_dir}/${lig_id}_simulations"
       lig_job_dir="${lig_sim_dir}/jobs"

       log_file="${lig_sim_dir}/lig-${lig_id}_job_patch_v3.log"

       # get just the job ids as an array
       nargs=$#
       let "njobargs = $nargs - 1"
       job_ids=("${@:2:$njobargs}")


       # then loop over them to get the paths to the orchestrators from
       # them
       echo ""
       echo "patching these job ids: ${job_ids[@]}"
       orch_paths=()
       job_counter=0
       for i in ${!job_ids[@]}; do

           job_id=${job_ids[$i]}
           echo $job_id

           # get the directory for the job
           job_dir="${lig_job_dir}/${job_id}"
           echo $job_dir

           # we need to rename them to the original folder that the
           # orchestrator is expecting. Instead of the name of the output
           # folder my job submission system renames it to. If this was
           # already done ignore these and their messages.
           output_dir="${job_dir}/output"
           if ! (mv ${output_dir} ${job_dir}/exec); then echo "${job_dir} already renamed"; fi
           output_dir="${job_dir}/exec"

           # from the results folder for this job
           sim_results_dir=(${output_dir}/*${lig_id}*/)
           # again remove trailing slash
           sim_results_dir=${sim_results_dir%/}

           # we want to choose the V2 patched orches to patch if they
           # exist. If they don't then they were generated in V2 and we
           # just use the base ones

           # these are the paths to test for in order of precedence

           patched_orch=(${sim_results_dir}/*.orch.PATCH_V2)
           patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_V2)
           orch=(${sim_results_dir}/*.orch)
           checkpoint_orch=(${sim_results_dir}/checkpoint.chk)


           if [[ -e "$patched_orch" ]]; then
               echo "Found a patched orchestrator for $job_dir"

               # if the job succeeded there will be a file generated which is
               # the orchestrator for the end of that simulation
               sim_orch_path="$patched_orch"

           elif [[ -e "$patched_checkpoint_orch" ]]; then
               echo "Found a patched checkpoint for $job_dir"
               # if it failed we want to use the checkpoint for this job
               # (which is also an orchestrator)
               sim_orch_path="$patched_checkpoint_orch"

           elif [[ -e "$orch" ]]; then
               echo "Found an unpatched orchestrator for $job_dir"

               # if the job succeeded there will be a file generated which is
               # the orchestrator for the end of that simulation
               sim_orch_path="$orch"

           elif [[ -e "$checkpoint_orch" ]]; then
               echo "Found an unpatched checkpoint for $job_dir"
               # if it failed we want to use the checkpoint for this job
               # (which is also an orchestrator)
               sim_orch_path="$checkpoint_orch"

           else
               echo "No orchestrator found for JOB ${job_dir}";
               sim_orch_path="None"

           fi


           # and save it in the array of the orchestrator paths
           orch_paths[job_counter]=${sim_orch_path}
           job_counter=$((job_counter + 1))
       done

       echo ""
       echo "orchestrators being patched"
       for orch in ${orch_paths[@]}; do
           echo "$orch"
       done
       echo "fin"

       sleep 15


       patch_tag='v3'
       for orch in ${orch_paths[@]}; do
           new_orch="${orch}.PATCH_${patch_tag}.sqlite"
           echo "patching $orch to ${new_orch}"  1>> $log_file
           python ${scripts_dir}/patch_orch_${patch_tag}.py ${orch} ${new_orch} 1>> $log_file
       done

  }
#+END_SRC


To do it for just one ligand set so we can parallelize over different jobs.

#+NAME: patch-lig-job-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-lig-job-orchs () {

      lig_id=$1

       conda activate seh_pathway_hopping
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       sim_dir="${project_dir}/simulations"
       scripts_dir="${project_dir}/scripts"

       lig_sim_dir="${sim_dir}/${lig_id}_simulations"
       lig_job_dir="${lig_sim_dir}/jobs"

       log_file="${lig_sim_dir}/lig-${lig_id}_job_patch_v3.log"

       # patch all of them in the folder

       # then loop over them to get the paths to the orchestrators from
       # them
       echo ""
       echo "Getting Paths from ${lig_sim_dir}/jobs"
       echo $(ls "${lig_sim_dir}/jobs")

       job_counter=0
       for job_dir in ${lig_sim_dir}/jobs/* ; do

           # remove the trailing slash from the job dir, since it is
           # confusing to me
           job_dir=${job_dir%/}

           # collect all of the job ids in the jobs directory
           job_id=$(basename "$job_dir")
           job_ids[$job_counter]="$job_id"
           let "job_counter = $job_counter + 1"
       done

       echo "patching orchestrators lig for ${lig_id}" 1> $log_file
       # then just call the shared function
       patch-specific-lig-job-orchs "$lig_id" ${job_ids[@]}

  }
#+END_SRC

****** script

#+BEGIN_SRC python :tangle prep/patch_orch_v3.py
  from wepy.orchestration.new_orchestrator import Orchestrator as NewOrchestrator



  def patch_orch(orch, patch_specs):

      # make the new orchestrator
      new_orch = NewOrchestrator(patch_specs['new_orch_path'], mode='w')

      # then get stuff out of the old one and feed them into the new one
      # for testing; these go in the metadata, which we set with
      # dedicated methods
      new_orch.set_default_sim_apparatus(orch.default_apparatus)
      new_orch.set_default_configuration(orch.default_configuration)
      # only get the default snapshot stuff if it has it
      if hasattr(orch, "_start_hash"):
          if orch._start_hash is not None:
              new_orch.set_default_init_walkers(orch.default_init_walkers)
              new_orch.set_default_snapshot(orch.default_snapshot)

      # for all of the old snapshots set them. Since we are changing
      # things here the hashes for the snapshots will be different so we
      # need a mapping of them
      snaphash_map = {}
      for old_snaphash, snapshot in orch._snapshots.items():
          new_snaphash = new_orch.add_snapshot(snapshot)
          snaphash_map[old_snaphash] = new_snaphash

      print("mapping of the old snapshots to the new")
      print(snaphash_map)

      # we also need the run data, we make a list of records to make a
      # table out of
      run_records = []

      for run_id, config in orch._run_configurations.items():

          # add the configuration
          config_hash = new_orch.add_configuration(config)

          cycle_idx = orch.run_cycles[run_id]


          # make the record for the run table, using the new snaphashes
          run_record = (snaphash_map[run_id[0]], snaphash_map[run_id[1]],
                        config_hash, cycle_idx)

          run_records.append(run_record)

      # then save the run records to the new orch
      for run_record in run_records:
          new_orch.register_run(*run_record)

      return new_orch



  if __name__ == "__main__":
      import sys
      import pickle

      orch_path = sys.argv[1]
      if orch_path == "--help":
          print("inputs: orch_path, new_orch_path")
      new_orch_path = sys.argv[2]

      with open(orch_path, 'rb') as rf:
          orch = pickle.load(rf)

      patch_specs = {'new_orch_path' : new_orch_path}

      orch = patch_orch(orch, patch_specs)
      orch.close()


#+END_SRC

****** COMMENT local tests

Apply it to the old_orchs for the test run.

#+BEGIN_SRC bash
conda activate seh_pathway_hopping
patch_tag='v3'
old_orch_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
for lig_id in 3 10 18 20; do
    old_orch_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
    orch_path="${old_orch_dir}/sEH_lig-${lig_id}.orch"
    new_orch_path="${old_orch_dir}/sEH_lig-${lig_id}_${patch_tag}-PATCH.orch"
    echo "Patching root orch ${lig_id}"
    python run/patch_orch_${patch_tag}.py $orch_path $new_orch_path
    echo "done"
done
#+END_SRC



Test it on one that has a run actually in it:

#+BEGIN_SRC bash
LIG_ID=3
conda activate seh_pathway_hopping
patch_tag='v3'
TMP_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/tmp"
ORCH_PATH="${TMP_DIR}/sEH_lig-${LIG_ID}.orch"
NEW_ORCH_PATH="${TMP_DIR}/sEH_lig-${LIG_ID}_${patch_tag}-PATCH.orch"
python run/patch_orch_${patch_tag}.py $ORCH_PATH $NEW_ORCH_PATH
#+END_SRC

****** COMMENT old stuff

Update the script on HPCC:


#+BEGIN_SRC bash
patch_tag='v3'
hpcc_url="lotzsamu@hpcc.msu.edu"
projects_dir="/mnt/home/lotzsamu/projects"
project_dir="${projects_dir}/seh.pathway_hopping"
hpcc_script_dir="${project_dir}/scripts"
rsync -avz "run/patch_orch_${patch_tag}.py" "${hpcc_url}:${hpcc_script_dir}/patch_orch_${patch_tag}.py" 2> /dev/null
#+END_SRC

Apply patch to the master orchs.

#+NAME: patch-master-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-master-orchs () {
      patch_tag='v3'
      conda activate seh_pathway_hopping

      projects_dir="/mnt/home/lotzsamu/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"
      hpcc_script_dir="${project_dir}/scripts"
      sim_dir="${project_dir}/simulations"
      for lig_id in 3 10 18 20; do
          lig_sim_dir="${sim_dir}/${lig_id}_simulations"
          input_dir="${lig_sim_dir}/input"

          master_orch_path="${lig_sim_dir}/old_master_orch/sEH_lig-${lig_id}.orch"

          # make a patched orchestrator, under a different name
          new_orch_filename="master_sEH_lig-${lig_id}.orch.sqlite"
          new_orch_path="${lig_sim_dir}/${new_orch_filename}"

          log_file="${lig_sim_dir}/patch_V3.log"

          echo $(date) 1>> $log_file
          echo "Patching master orch ${lig_id}" 1>> $log_file
          # capture output to a log file which will have the mapping of
          # old hashes to new ones
          python "${hpcc_script_dir}/patch_orch_${patch_tag}.py" $master_orch_path $new_orch_path #1>> ${log_file}
          echo "done" 1>> $log_file

      done
  }
#+END_SRC


Patch all of the orches for all of the existing jobs in one go.

#+NAME: patch-job-orchs
#+BEGIN_SRC bash :tangle prep/bash_funcs.sh
  patch-job-orchs () {

       conda activate seh_pathway_hopping
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       sim_dir="${project_dir}/simulations"
       scripts_dir="${project_dir}/scripts"

       for lig_id in 3 10 18 20; do
           lig_sim_dir="${sim_dir}/${lig_id}_simulations"
           lig_job_dir="${lig_sim_dir}/jobs"

           # patch all of them in the folder

           # then loop over them to get the paths to the orchestrators from
           # them
           echo ""
           echo "Getting Paths from ${lig_sim_dir}/jobs"
           echo $(ls "${lig_sim_dir}/jobs")
           orch_paths=()
           job_counter=0
           for job_dir in ${lig_sim_dir}/jobs/* ; do

               # remove the trailing slash from the job dir, since it is
               # confusing to me
               job_dir=${job_dir%/}

               # we need to rename them to the original folder that the
               # orchestrator is expecting. Instead of the name of the output
               # folder my job submission system renames it to. If this was
               # already done ignore these and their messages.
               output_dir="${job_dir}/output"
               if ! (mv ${output_dir} ${job_dir}/exec); then echo "${job_dir} already renamed"; fi
               output_dir="${job_dir}/exec"

               # from the results folder for this job
               sim_results_dir=(${output_dir}/*${lig_id}*/)
               # again remove trailing slash
               sim_results_dir=${sim_results_dir%/}

               # find the orchestrator the simulation produced (if at all)
               # this will either be a finished one or a checkpoint
               test_orch=(${sim_results_dir}/*.orch)
               if [[ -e $test_orch ]]; then
                   echo "Found an orchestrator for $job_dir"

                   # if the job succeeded there will be a file generated which is
                   # the orchestrator for the end of that simulation
                   sim_orch_path=$test_orch

               elif [[ -e "${sim_results_dir}/checkpoint.chk" ]]; then
                   echo "Found a checkpoint for $job_dir"
                   # if it failed we want to use the checkpoint for this job
                   # (which is also an orchestrator)
                   sim_orch_path=${sim_results_dir}/checkpoint.chk

               else
                   echo "No orchestrator found for JOB ${job_dir}";
                   sim_orch_path="None"

               fi

               # and save it in the array of the orchestrator paths
               orch_paths[job_counter]=${sim_orch_path}
               job_counter=$((job_counter + 1))
           done

           echo ""
           echo "orchestrators being patched"
           for orch in ${orch_paths[@]}; do
               echo "$orch"
           done
           echo "fin"

           sleep 15

           echo "patching orchestrators"
           patch_tag='v3'
           for orch in ${orch_paths[@]}; do
               new_orch="${orch}.PATCH_${patch_tag}.sqlite"
               echo "patching $orch to ${new_orch}"
               python ${scripts_dir}/patch_orch_${patch_tag}.py ${orch} ${new_orch}
           done

       done


  }
#+END_SRC
**** Running a test simulation

This is just to make sure that it is runnable and no further
troubleshooting is needed.

We use a real number of integration steps and try to get 3 or 4 cycles
(2 hours each).

With 0.002 picosecond integrations steps and 20 picosecond cycles we
have 10000 steps per segment

First list the initial snapshot for the orchestrator:

#+BEGIN_SRC bashc
LIG_ID="3"
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/$LIG_ID
conda activate seh_pathway_hopping
python -m wepy.orchestration.cli ls-snapshots root.orch
START_HASH=""
#+END_SRC


Use the hash of the snapshot from that to do a run:
#+BEGIN_SRC bash
N_STEPS=10000
RUN_TIME=7200
cd /home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/$LIG_ID
TEST_RUN_NAME="pilot-run"
rm -rf ./${TEST_RUN_NAME}/*
mkdir -p ./${TEST_RUN_NAME}
conda activate seh_pathway_hopping
python -m wepy.orchestration.cli run --job-name pilot-run --n-workers 1 --log INFO root.orch $START_HASH $RUN_TIME $N_STEPS
#+END_SRC


If you want to use a different configuration we can call it like this:

#+BEGIN_SRC bash
START_HASH="3a3eed6bb613a042d089547bcab8b1e6"
N_STEPS=10000
RUN_TIME=7200
OLD_ORCH_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
ORCH_PATH="${OLD_ORCH_DIR}/sEH_lig-${LIG_ID}.orch"
MD_TEST_RUN_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/${LIG_ID}"
cd ${MD_TEST_RUN_DIR}
N_WORKERS=1
N_DEVICES=1
CONFIG_NAME="OpenMMGPUWorker_8-workers_lig-${LIG_ID}.config.dill.pkl"
CONFIG_PATH="${MD_TEST_RUN_DIR}/${CONFIG_NAME}"
TEST_RUN_NAME="UPDATED-config-run"
rm -rf ./${TEST_RUN_NAME}/*
mkdir -p ./${TEST_RUN_NAME}
conda activate seh_pathway_hopping
python -m wepy.orchestration.cli run --n-workers ${N_WORKERS} \
        --job-name ${TEST_RUN_NAME} --configuration ${CONFIG_PATH} --log INFO ${ORCH_PATH} \
        $START_HASH $RUN_TIME $N_STEPS
#+END_SRC


For running on a patched Orchestrator:

v0.12

#+BEGIN_SRC bash
START_HASH="3a3eed6bb613a042d089547bcab8b1e6"
N_STEPS=100
RUN_TIME=100
OLD_ORCH_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/old_orchs"
ORCH_PATH="${OLD_ORCH_DIR}/sEH_lig-${LIG_ID}_v012-PATCH.orch"
#ORCH_PATH="${OLD_ORCH_DIR}/sEH_lig-${LIG_ID}.orch"
MD_TEST_RUN_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/${LIG_ID}"
cd ${MD_TEST_RUN_DIR}
N_WORKERS=1
N_DEVICES=1
CONFIG_NAME="OpenMMGPUWorker_2-workers_lig-${LIG_ID}_V2.config.dill.pkl"
CONFIG_PATH="${MD_TEST_RUN_DIR}/${CONFIG_NAME}"
TEST_RUN_NAME="UPDATED-config-run"
rm -rf ./${TEST_RUN_NAME}/*
mkdir -p ./${TEST_RUN_NAME}
conda activate seh_pathway_hopping
python -m wepy.orchestration.cli run --n-workers ${N_WORKERS} \
        --job-name ${TEST_RUN_NAME} --configuration ${CONFIG_PATH} --log INFO ${ORCH_PATH} \
        $START_HASH $RUN_TIME $N_STEPS
cd -
#+END_SRC



*** multi-ligand pipelines
**** Equilibration Script

I want to make a pipeline for equilibrating each ligand as a batch for
my computer so I don't have to do it manually.

#+BEGIN_SRC bash :tangle prep/equilibrate_systems.sh
  conda activate seh_pathway_hopping

  project_data_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data"
  for LIG_ID in 3 10 17 18 20; do

      # equilibrate
      EQUIL_DIR="${project_data_dir}/equilibration/${LIG_ID}"
      mkdir -p ${EQUIL_DIR}

      # back up the stuff that was in there so we never lose an equilibration
      EQUIL_BACKUP_DIR="${project_data_dir}/equilibration_backup/${LIG_ID}"

      STORE_DIR=$EQUIL_BACKUP_DIR/$(python -c "from datetime import datetime; print(datetime.today().isoformat())")

      # make a directory for the stuff to go into
      mkdir -p $STORE_DIR

      # copy it if it is there
      cp -r ${EQUIL_DIR}/* ${STORE_DIR}

      pushd ${EQUIL_DIR}

      RESID="UNL"
      python -m seh_prep.equilibrate \
             "sEH_lig-${LIG_ID}_system.coords.txt" \
             "sEH_lig-${LIG_ID}_system.top.json" \
             $RESID \
             "charmm36.xml" \
             "charmm36_solvent.xml" \
             "unl.xml" \
             $LIG_ID

      MD_SYSTEMS_DIR="${project_data_dir}/md_systems/${LIG_ID}"
      mkdir -p $MD_SYSTEMS_DIR
      # copy the things needed for generating the orchestrator
      cp "sEH_lig-${LIG_ID}_system.top.json" ${MD_SYSTEMS_DIR}/
      cp "sEH_lig-${LIG_ID}_equilibrated.state.pkl" ${MD_SYSTEMS_DIR}/
      # force fields
      cp charmm36_solvent.xml ${MD_SYSTEMS_DIR}/
      cp charmm36.xml ${MD_SYSTEMS_DIR}/
      cp unl.xml ${MD_SYSTEMS_DIR}/

      popd

  done

#+END_SRC

***** DEBUGGING

#+BEGIN_SRC bash :tangle prep/local_bash_funcs.bash

  test-equilibrate () {
      conda activate seh_pathway_hopping
      lig_id=3

      # the directory to get the inputs from
      equil_inputs_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/equilibration/${lig_id}"

      # directory to run in
      equil_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/equilibration/tmp-${lig_id}"
      mkdir -p ${equil_dir}
      pushd ${equil_dir}

      resid="UNL"
      python -m seh_prep.equilibrate \
             "${equil_inputs_dir}/sEH_lig-${lig_id}_system.coords.txt" \
             "${equil_inputs_dir}/sEH_lig-${lig_id}_system.top.json" \
             "$resid" \
             "${equil_inputs_dir}/charmm36.xml" \
             "${equil_inputs_dir}/charmm36_solvent.xml" \
             "${equil_inputs_dir}/unl.xml" \
             "$lig_id"


      popd
  }
#+END_SRC

**** Validating Equilibrated Structure


Here is a script to generate simulation components from the final
equilibrated structures.

#+BEGIN_SRC bash
  conda activate seh_pathway_hopping

  project_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping"

  lig_id=3
  resid="UNL"
  platform="CUDA"

  md_systems_dir="${project_dir}/data/md_systems/${lig_id}"
  output_path="${md_systems_dir}/lig-${lig_id}_${platform}.sim.pkl"

  pushd ${md_systems_dir}

  python -m seh_prep.gen_sim \
         sEH_lig-${lig_id}_equilibrated.state.pkl \
         sEH_lig-${lig_id}_system.top.json \
         ${resid} \
         charmm36.xml charmm36_solvent.xml unl.xml \
         ${lig_id} \
         ${platform}\
         ${output_path}

  popd


#+END_SRC



**** Generate the initial snapshot

#+BEGIN_SRC bash
inv gen-init-snapshots
#+END_SRC

Manually for what you need:

#+BEGIN_SRC bash
bash -i prep/gen_init_snap.sh '10'
#+END_SRC


***** Script for doing so

#+NAME: gen_configuration
#+BEGIN_SRC bash :tangle prep/gen_init_snap.sh
  #!/bin/bash -l

  lig_id=$1

  project_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping"

  resid="UNL"
  num_walkers=48

  md_systems_dir="${project_dir}/data/md_systems/${lig_id}"

  pushd ${md_systems_dir}

  python -m seh_prep.gen_snapshot \
         sEH_lig-${lig_id}_equilibrated.state.pkl \
         sEH_lig-${lig_id}_system.top.json \
         ${resid} \
         charmm36.xml charmm36_solvent.xml unl.xml \
         ${lig_id} \
         ${num_walkers}

  popd
#+END_SRC


**** Generate Configurations

If you want to change your configuration while running a simulation we
can do that on its own. So tweak the parameters in the
gen_configuration module and then we run it. The main use for this is
to change the number of workers you are using (at least for us).

Use invoke since we can clean with it and make sure dependencies
happen:

#+BEGIN_SRC bash
inv gen-configurations
#+END_SRC

Manually for what you need:

#+BEGIN_SRC bash
bash prep/gen_configuration.sh 'TaskMapper' 'CUDA' 1 '10'
#+END_SRC


***** Script for doing so

#+NAME: gen_configuration
#+BEGIN_SRC bash :tangle prep/gen_configuration.sh
  #!/bin/bash

  work_mapper_type=$1
  platform=$2
  n_workers=$3
  lig_id=$4
  tag=$5

  resid="UNL"

  # env='seh.pathway_hopping.common'

  projects_dir="/home/salotz/tree/lab/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"

  # make the file name for the configuration
  if [[ $tag = "" ]]; then
      config_name="${work_mapper_type}_${platform}_${n_workers}-workers_lig-${lig_id}.config.dill.pkl"
  else
      config_name="${work_mapper_type}_${platform}_${n_workers}-workers_lig-${lig_id}_${tag}.config.dill.pkl"
  fi

  md_systems_dir="${project_dir}/data/md_systems/${lig_id}"
  configurations_dir="${project_dir}/data/configurations/${lig_id}"
  config_path="${configurations_dir}/${config_name}"

  mkdir -p $configurations_dir

  pushd ${md_systems_dir}
  rm -f ${config_path}
  # conda activate $env
  echo "making configuration for ligand ${lig_id}"
  python -m seh_prep.gen_configuration \
         --tag "${tag}" \
         sEH_lig-${lig_id}.snap.pkl \
         sEH_lig-${lig_id}_equilibrated.state.pkl \
         sEH_lig-${lig_id}_system.top.json \
         ${config_path} \
         ${resid} \
         ${n_workers} \
         ${platform} \
         ${work_mapper_type}

  echo "Made configuration at: $config_path"
  test_run_dir="${project_dir}/data/md_test_runs/${lig_id}"
  mkdir -p $test_run_dir
  cp ${config_path} $test_run_dir/${config_name}
  cp ${config_path} $project_dir/hpcc/simulations/${lig_id}_simulations/configurations/${config_name}
  cp ${config_path} $project_dir/hpcc/simulations/${lig_id}_simulations/input/${config_name}
  popd

#+END_SRC

***** COMMENT  Others
Make a configuration for local testing:

#+BEGIN_SRC bash
source run/bash_funcs.sh
gen_configuration Worker 1
#+END_SRC


One for running on GPUs on HPCC
#+BEGIN_SRC bash
source run/bash_funcs.sh
gen_configuration OpenMMGPUWorker 8
#+END_SRC



***** COMMENT I tried a xonsh version but it was too difficult

#+begin_src xonsh :tangle prep/gen_configuration.xsh
  #!/bin/xonsh -l -i

  worker_type = $ARGS[1]
  n_workers = $ARGS[2]

  if len($ARGS) > 3:
      tag = $ARGS[3]
  else:
      tag = None

  resid = "UNL"

  projects_dir = f"{$HOME}/tree/lab/projects"
  project_dir = f"{projects_dir}/seh.pathway_hopping"

  for lig_id in [3, 10, 18, 20]:

      # make the file name for the configuration
      if tag is None:
          config_name = f"{worker_type}_{n_workers}-workers_lig-{lig_id}.config.dill.pkl"
      else:
          config_name = f"{worker_type}_{n_workers}-workers_lig-{lig_id}_{tag}.config.dill.pkl"

      md_systems_dir = f"{project_dir}/data/md_systems/{lig_id}"
      configurations_dir = f"{project_dir}/data/configurations/{lig_id}"
      config_path = f"{configurations_dir}/{config_name}"

      pushd @(md_systems_dir)
      rm -f @(config_path)

      conda activate seh_pathway_hopping

      echo f"making configuration for ligand {lig_id}"
      python -m seh_prep.gen_configuration \
             sEH_lig-@(lig_id)_equilibrated.state.pkl \
             sEH_lig-@(lig_id)_system.top.json \
             @(resid) @(lig_id) \
             @(config_path) \
             @(n_workers) @(worker_type)

      # echo f"Made configuration at: {config_path}"

      # test_run_dir = f"{project_dir}/data/md_test_runs/{lig_id}"

      # mkdir -p @(test_run_dir)
      # cp @(config_path) f"{test_run_dir}/{config_name}"
      # popd


#+end_src


**** Generate Orchestrator Script

WARNING may be out of date.

#+BEGIN_SRC bash :shebang "#!/bin/bash -l" :tangle prep/generate_orchestrators.sh

  LIG_ID="$1"

  echo "LIG_ID: $LIG_ID"

  project_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping"
  root_orch_dir="${project_dir}/data/root_orchs"

  RESID="UNL"
  NUM_WALKERS=48

  MD_SYSTEMS_DIR="${project_dir}/data/md_systems/${LIG_ID}"

  pushd ${MD_SYSTEMS_DIR}
  #conda activate seh_prep
  python -m seh_prep.gen_orchestrator \
         sEH_lig-${LIG_ID}.snap.pkl \
         sEH_lig-${LIG_ID}_equilibrated.state.pkl \
         sEH_lig-${LIG_ID}_system.top.json \
         ${RESID} \
         charmm36.xml charmm36_solvent.xml unl.xml \
         ${LIG_ID} \
         ${NUM_WALKERS} \
      || exit 1

  # copy the root orch to the root orches directory
  cp sEH_lig-$LIG_ID.orch.sqlite $root_orch_dir/sEH_lig-${LIG_ID}.orch.sqlite

  # copy it to the test runs dir
  TEST_RUN_DIR="${root_orch_dir}/data/md_test_runs/$LIG_ID"
  mkdir -p $TEST_RUN_DIR
  cp sEH_lig-$LIG_ID.orch.sqlite $TEST_RUN_DIR/root.orch.sqlite
  popd


#+END_SRC


**** Test Simulations script

#+BEGIN_SRC bash :shebang "#!/bin/bash -l" :tangle-mode (identity #o755) :tangle prep/test_runs.sh

  conda activate seh_pathway_hopping
  SNAPHASHES[3]="8e158c28c8c426f064d032823bb9aaf9"
  SNAPHASHES[10]="6258387e90a2517b19adfd15ee4ba07f"
  SNAPHASHES[18]="84de06ee42824b533e2289f6962cf289"
  SNAPHASHES[20]="46dc5ffd2ea9c4616a9ce75c2d1501b0"

  for lig_id in 3 10 18 20; do

      N_STEPS=100
      RUN_TIME=10
      test_run_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/md_test_runs/$lig_id"
      pushd "$test_run_dir"

      TEST_RUN_NAME="pilot-run"
      # clean up the test run dir
      rm -rf ./${TEST_RUN_NAME}/*
      mkdir -p ./${TEST_RUN_NAME}

      # the start hash for this ligand
      START_HASH=${SNAPHASHES[$lig_id]}

      # the test configuration
      config="${test_run_dir}/OpenMMGPUWorker_8-workers_lig-${lig_id}.config.dill.pkl"

      # get the snapshot we need to run this
      wepy get snapshot -O start.snap.dill.pkl root.orch.sqlite $START_HASH

      # then run the simulation from that snapshot file
      wepy run snapshot\
           --job-name "$TEST_RUN_NAME" \
           --n-workers 1 \
           --log INFO \
           start.snap.dill.pkl ${config} \
           $RUN_TIME $N_STEPS

      popd

  done
#+END_SRC



*** Workflow: preparing the simulations on the cluster

**** Generate Task Scripts

#+BEGIN_SRC bash
inv hpcc_gen_tasks
#+END_SRC

***** manually

To generate the tasks we write a file that specifies all the run
scripts to make from the task templates for each goal.

The we use a script to read this and to generate the files.

Then use it for all of the ligands:

#+NAME: gen_lig_task_scripts
#+BEGIN_SRC bash :tangle prep/gen-lig-task-scripts.sh
  # projects_dir="/mnt/home/lotzsamu/projects"
  projects_dir="$HOME/tree/lab/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  scripts_dir="${project_dir}/hpcc/scripts"
  sim_dir="${project_dir}/hpcc/simulations"
  templates_dir="${project_dir}/hpcc/templates"
  conda activate seh_pathway_hopping
  for lig_id in 3 10 18 20; do
     lig_sim_dir="${sim_dir}/${lig_id}_simulations"
     rm -f "${lig_sim_dir}/tasks/*"
     python "${scripts_dir}/gen_tasks.py" "${templates_dir}" "${lig_sim_dir}/tasks" "${lig_sim_dir}/run_spec.toml"
  done

#+END_SRC

***** Script

#+BEGIN_SRC python :tangle hpcc/scripts/gen_tasks.py
  import os
  import os.path as osp
  import stat
  from copy import copy

  import toml
  import click
  from jinja2 import Environment, FileSystemLoader

  TASK_TEMPLATES = (
      ('production', 'production_template.sh.j2'),
  )

  EXECUTABLE_PERMISSIONS = (stat.S_IXUSR | stat.S_IRUSR | stat.S_IWUSR
                            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP
                            | stat.S_IROTH)

  @click.command()
  @click.argument("templates", type=click.Path(exists=True, file_okay=False))
  @click.argument("out_dir", type=click.Path(file_okay=False))
  @click.argument("spec", type=click.File())
  def main(templates, out_dir, spec):

      # load the templates
      templates = osp.abspath(osp.expanduser(templates))
      env = Environment(loader=FileSystemLoader(templates))

      task_templates = {}
      for key, template_key in dict(TASK_TEMPLATES).items():
          task_templates[key] = env.get_template(template_key)

      # load the specs
      spec_d = toml.load(spec)

      # create the output dir
      out_dir = osp.abspath(osp.expanduser(out_dir))
      os.makedirs(out_dir, exist_ok=True)

      # make the specs for each run combining it with the defaults
      defaults = spec_d.pop('defaults')
      run_specs = []
      for run_spec in spec_d.pop('runs'):

          run_spec_d = copy(defaults)
          run_spec_d.update(run_spec)

          run_specs.append(run_spec_d)

      # for each spec write a file in the outdir
      for run_spec in run_specs:

          task_type = run_spec.pop('task_type')
          template = task_templates[task_type]

          task_script = template.render(**run_spec)

          # name for the script
          script_name = "{}.sh".format(run_spec['task_name'])

          # path for the script
          script_path = osp.join(out_dir, script_name)

          click.echo("Writing task: {}".format(script_name))

          with open(script_path, 'w') as wf:
              wf.write(task_script)

          # change the permissions so it is executable
          os.chmod(script_path, EXECUTABLE_PERMISSIONS)



  if __name__ == "__main__":

      main()

#+END_SRC





**** Set up simulation submission directory

This is the initial setup for the directory and should only need to be
done once.

Copy the prototype submission script with directory over to the
simulation directory:


Copy the root orchestrators to the master orch position:

#+NAME: copy-root_orchs-to-master-positions
#+BEGIN_SRC bash :shebang "#!/bin/bash" :tangle prep/copy_root_orches_to_master_positions.sh
  project_dir="${HOME}/tree/lab/projects/seh.pathway_hopping"
  orch_dir="${project_dir}/hpcc/root_orchs"
  sim_dir="${project_dir}/hpcc/simulations"
  for lig_id in 3 10 17 18 20; do
    cp "${orch_dir}/sEH_lig-${lig_id}.orch.sqlite" "${sim_dir}/${lig_id}_simulations/orchs/master_sEH_lig-${lig_id}.orch.sqlite"
  done

#+END_SRC


WE parameters:
- MD step size :: 0.002 picoseconds
- cycle time :: 20 picoseconds
- num segment MD steps :: 10000
- num walkers :: 48
- checkpoint frequency :: 25


Fields:
- lig_id
- walltime (job)
- num-gpus
- run-time (wepy)

Constants:
- hours in week :: 168
- seconds in week :: 604800

Set it up so that the script just runs for a short time run it and
make sure everything works.

Test run Parameters:

- walltime :: 4:00:00
- num-gpus :: 1
- wepy-run-time :: 12600

Production run Parameters:

- num-gpus :: 8
- walltime :: 168:00:00
- wepy-run-time (4 hour cleanup buffer) :: 590400



**** Generate Submission scripts


***** COMMENT HOME

#+BEGIN_SRC sh :tangle prep/gen-lig-submission-scripts.sh
  projects_dir="$HOME/tree/lab/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  sim_dir="${project_dir}/hpcc/simulations"
  conda activate seh_pathway_hopping
  for lig_id in 3 10 18 20; do

     lig_sim_dir="${sim_dir}/${lig_id}_simulations"

     # clean
     rm -f "${lig_sim_dir}/submissions/*"

     # run slurmify
     slurmify --config "${project_dir}/run_settings.toml" \
                      --context "${lig_sim_dir}/context_settings.toml" \
                      --batch-in "${lig_sim_dir}/tasks" \
                      --batch-out "${lig_sim_dir}/submissions" \
                      "lig_${lig_id}_sims"
  done

#+END_SRC

***** SCRATCH

#+BEGIN_SRC sh :tangle prep/gen-lig-submission-scripts.sh
  projects_dir="$SCRATCH/tree/lab/projects"

  project_dir="${projects_dir}/seh.pathway_hopping"
  sim_dir="${project_dir}/hpcc/simulations"
  conda activate seh_pathway_hopping
  for lig_id in 3 10 17 18 20; do

     lig_sim_dir="${sim_dir}/${lig_id}_simulations"

     # clean
     rm -f "${lig_sim_dir}/submissions/*"

     # run slurmify
     slurmify --config "${project_dir}/hpcc/run_settings.toml" \
                      --context "${lig_sim_dir}/context_settings.toml" \
                      --batch-in "${lig_sim_dir}/tasks" \
                      --batch-out "${lig_sim_dir}/submissions" \
                      "lig_${lig_id}_sims"
  done

#+END_SRC


***** COMMENT invoke way (broken)
#+BEGIN_SRC bash
inv hpcc-gen-sumbissions
#+END_SRC


**** Test generated scripts


***** Locally

Just generate the scripts with one with a single GPU and a local
orchestrator:

#+BEGIN_SRC toml
  [[runs]]

  task_name = "lig-10_test-local"
  orch_name = "$HOME/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite"
  num_gpus = 1
  n_steps = 500
  run_time = 60
  checkpoint_freq = 2
  task_type = "production"
  start_hash = "83689c1333f61c7998110ad5994ccdbf"
#+END_SRC


#+BEGIN_SRC bash
inv hpcc-test-local
#+END_SRC


***** Interactively on HPCC

We need to get an interactive job to do this. THis is for SLURM.
Test with at least 2 GPUS to make sure the work mapper is working.

#+BEGIN_SRC bash
srun -N 1 -c 4 --mem=20G --time=7:00:00 -C intel16 --gres=gpu:8 --pty "/bin/bash -l"
#+END_SRC


#+BEGIN_SRC bash
  lig_id=10
  (cd hpcc/simulations/${lig_id}_simulations &&
      mkdir -p test_jobs/test_hpcc_interactive &&
      rm -rf test_jobs/test_hpcc_interactive/* &&
      cp input/* test_jobs/test_hpcc_interactive/ &&
      export ANACONDA_DIR="$HOME/.pyenv/versions/miniconda3-latest" &&
      pushd test_jobs/test_hpcc_interactive &&
      ../../tasks/lig-${lig_id}_test-local-hpcc-scratch.sh &&
      popd
  )
#+END_SRC


****** COMMENT 


Once we get this then we run a test simulation
#+BEGIN_SRC bash
  job_name="test_job_interactive"
  num_workers=8
  # should be placed in the test run dir
  lig_id='3'
  config_name="OpenMMGPUWorker_8-workers_lig-${lig_id}.config.dill.pkl"
  conda activate seh_pathway_hopping

  projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  test_dir="${project_dir}/test_run"
  root_orch="${project_dir}/root_orchs/sEH_lig-${lig_id}.orch.sqlite"


  pushd $test_dir

  # the new root snapshot
  start_hash="8e158c28c8c426f064d032823bb9aaf9"

  # if you want to use a config in the orchestrator
  # config_hash=""

  rm -rf ${job_name}
  module load CUDA/9.2.88

  # copy the snapshot and config we need for this run to the inputs dir
  wepy get snapshot -O ${start_hash}.snap.dill.pkl ${root_orch} ${start_hash}

  # run the job
  wepy run snapshot \
           --job-name ${job_name} \
           --n-workers ${num_workers} \
           --log DEBUG \
           --checkpoint-freq 1 \
           "${start_hash}.snap.dill.pkl" \
           ${config_name} \
           100 1000

  popd
#+END_SRC


***** SLURM submission

For an entry in the runs like this:

#+BEGIN_SRC toml
[[runs]]

task_name = "lig-10_test"
num_gpus = 2
n_steps = 1000
run_time = 3600
checkpoint_freq = 2
task_type = "production"
start_hash = "83689c1333f61c7998110ad5994ccdbf"

#+END_SRC


Generate the task and submission scripts. If you want to adjust the
submission parameters in the run_settings.toml file you can tag those
generated files here which won't get overwritten with the default name:

#+BEGIN_SRC bash
inv hpcc-gen-submissions --tag="test"
#+END_SRC

And then run all of the tests for each ligand on the scheduler:

#+BEGIN_SRC bash
inv hpcc-test-submit
#+END_SRC


manually:

Then run all of the test scripts
#+BEGIN_SRC bash
  project_dir="/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping"

  for lig_id in 3 10 18 20; do
      pushd "$project_dir/hpcc/simulations/${lig_id}_simulations"
      sbatch "submissions/lig-${lig_id}_test-submit.sh.slurm"
      popd
  done

#+END_SRC


**** Patching outdated pickles


Of course because the serialization format is not well defined and is
evolving we will need to "patch" snapshots on occasion.

Here we provide the functions for doing so and a wrapper script that
will apply the necessary ones.

Note that there were historical patches, but because it has become
such a persisten problem I decided to make a more extendable and
reusable solution that can be used at runtime instead of finding every
instance of something and changing it.


***** Patches

****** WepySimApparatus

Update them to the wepy sim apparatus which has specific methods for
the filters.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py


  def patch_wepy_sim_apparatus(snapshot):

      from wepy.orchestration.snapshot import WepySimApparatus

      if not issubclass(type(snapshot.apparatus), WepySimApparatus):
          app = WepySimApparatus(snapshot.apparatus._filters[0],
                           boundary_conditions=snapshot.apparatus._filters[1],
                           resampler=snapshot.apparatus._filters[2],
          )

          snapshot._apparatus = app

      return snapshot
#+end_src



****** OpenMMRunner.getState_kwargs

This is a new attribute and so it must be added if not present.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py


  def patch_openmmrunner_getstate_kwargs(snapshot):

      from wepy.runners.openmm import GET_STATE_KWARG_DEFAULTS

      if not hasattr(snapshot.apparatus.runner, 'enforce_box'):
          # we default to True, since that was the behavior by default
          # before and if it wasn't then it was a problem
          snapshot.apparatus.runner.enforce_box = True
          snapshot.apparatus.runner.getState_kwargs = dict(GET_STATE_KWARG_DEFAULTS)

          # redundant but explicit
          snapshot.apparatus.runner.getState_kwargs['enforcePeriodicBox'] = True
      else:
          assert hasattr(snapshot.apparatus.runner, 'getState_kwargs'), \
              "Inconsistency in snapshot runner. Has 'enforce_box' attribute but not 'getState_kwargs'"

      return snapshot
#+end_src



****** OpenMMRunner.platform_kwargs

This is a new attribute and so it must be added if not present.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py


  def patch_openmmrunner_platform_kwargs(snapshot):

      if not hasattr(snapshot.apparatus.runner, 'platform_kwargs'):
          snapshot.apparatus.runner.platform_kwargs = None

      return snapshot
#+end_src



****** UnbindingBC._mdj_top

THis was added as a hidden attribute for efficiency purposes.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py


  def patch_unbinding_bc_mdj_top(snapshot):

      from wepy.util.mdtraj import json_to_mdtraj_topology

      if not hasattr(snapshot.apparatus.boundary_conditions, '_mdj_top'):

          snapshot.apparatus.boundary_conditions._mdj_top = \
                          json_to_mdtraj_topology(snapshot.apparatus.boundary_conditions._topology)

      return snapshot
#+end_src


****** UnbindingBC._periodic


This was to specify if we are to use periodic boundary
conditions. Wasn't important for this simulation but just easier to
patch it and keep on the HEAD.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py

  def patch_unbinding_bc_periodic(snapshot):

      if not hasattr(snapshot.apparatus.boundary_conditions, '_periodic'):

          snapshot.apparatus.boundary_conditions._periodic = True


      return snapshot
#+end_src


****** WExplore.pmin

Set the pmin updating from the seh_setup package value.

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py

  def patch_wexplore_pmin(snapshot):

      from seh_prep.parameters import PMIN

      # this actually doesn't have any effect since the pmin in the
      # RegionTree is actually what governs this
      snapshot.apparatus.resampler._pmin = PMIN

      # so set the region tree too
      snapshot.apparatus.resampler._region_tree._pmin = PMIN

      return snapshot
#+end_src

***** Patch Script

#+begin_src python :tangle hpcc/scripts/patch_snapshot.py

  PATCHES = {
      'OpenMMRunner.platform_kwargs' : patch_openmmrunner_platform_kwargs,
      'OpenMMRunner.getState_kwargs' : patch_openmmrunner_getstate_kwargs,
      'UnbindingBC._mdj_top' : patch_unbinding_bc_mdj_top,
      'UnbindingBC._periodic' : patch_unbinding_bc_periodic,
      'WepySimApparatus' : patch_wepy_sim_apparatus,
      'WExplorePmin' : patch_wexplore_pmin,
  }

  def print_snapshot_params(snap):

      print("PMIN: ", snap.apparatus.resampler.pmin)


  if __name__ == '__main__':

      from wepy.orchestration.orchestrator import Orchestrator

      import sys

      snapshot_path = sys.argv[1]
      if snapshot_path == '--help' or snapshot_path == '-h':
          print("args: snapshot_input, snapshot_output, patch0, patch1,...")


      snapshot_output = sys.argv[2]
      patch_specs = sys.argv[3:]


      patch_funcs = []
      if len(patch_specs) < 1:
          print("No patch specified, applying none")
          exit

      # otherwise use the patch functions
      else:
          for patch_spec in patch_specs:
              patch_funcs.append(PATCHES[patch_spec])


      # get the snapshot
      print("Reading and deserializing snapshot")
      with open(snapshot_path, 'rb') as rf:
          orig_serial_snap = rf.read()

      snap = Orchestrator.deserialize(orig_serial_snap)

      # print the hash for posterity
      orig_snaphash = Orchestrator.hash_snapshot(orig_serial_snap)
      print("Original starting snapshot hashed as: {}".format(orig_snaphash))

      # apply patches
      for i, patch_func in enumerate(patch_funcs):
          print("Applying patch {}".format(patch_specs[i]))
          snap = patch_func(snap)


      print("Finished patching, serializing snapshot...")
      serial_snap = Orchestrator.serialize(snap)

      # get the hash of this snapshot
      snaphash = Orchestrator.hash_snapshot(serial_snap)
      print("Patched starting snapshot hash is: {}".format(snaphash))

      print("{} --> {}".format(orig_snaphash, snaphash))

      # write it out
      print("Finished. Serializing and writing output snapshot to: {}".format(snapshot_output))
      with open(snapshot_output, 'wb') as wf:
          wf.write(serial_snap)


      print("----------------------------------------")
      print("Selection of Snapshot Parameters after patching:")

      print_snapshot_params(snap)
      print("----------------------------------------")
#+end_src

**** Copy to HPCC

To the main directory:

#+BEGIN_SRC bash
inv refugue.rsync --source="$HOSTNAME" --target='icer'
#+END_SRC

Scratch:

#+BEGIN_SRC bash
inv refugue.rsync --source="$HOSTNAME" --target='icer/scratch'
#+END_SRC

Except that this is too much probably so I am just replicating a part of it

#TEMP
#+BEGIN_SRC bash
rsync -a -v -hh --stats -i   -z --delete  --include='lab/' --include='lab/projects/' --include='lab/projects/seh.pathway_hopping/' --include='lab/resources/' --include='lab/resources/project-resources/' --include='lab/resources/project-resources/seh.pathway_hopping/' --exclude='personal' --exclude='lab' --exclude='incoming' --exclude='outgoing' --exclude='reading' --exclude='mobile' --exclude='bin' --exclude='programs' --exclude='lab/*' --exclude='lab/projects/*' --exclude='lab/resources/*' --exclude='lab/resources/project-resources/*' --exclude='lab/resources/project-resources/**.wepy.h5' --exclude='lab/resources/project-resources/seh.pathway_hopping/hpcc/simulations/*/jobs/**wepy.h5' --exclude='*__pycache__*' --exclude='*.stversions' --exclude='*.rsync_backup' --exclude='.git/' '/home/salotz/tree/lab/projects/seh.pathway_hopping/' 'lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping'
#+END_SRC

New configurations and orchestrators:


Everything:

#+BEGIN_SRC 
rsync -n -a -v -hh --stats -i   -z '/home/salotz/tree/lab/resources/project-resources/seh.pathway_hopping/hpcc/simulations/' 'lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/project-resources/seh.pathway_hopping/hpcc/simulations'
#+END_SRC

Just the configurations:
#+BEGIN_SRC bash
  for lig_id in 10 17 18 20; do
      rsync -a -v -hh --stats -i   -z  \
           "/home/salotz/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations/" \
            "lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations"
  done
#+END_SRC

Just the orchestrators:
#+BEGIN_SRC bash
  for lig_id in 10 18 20; do
      rsync -a -v -hh --stats -i   -z  \
           "/home/salotz/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/orchs/" \
            "lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/orchs"
  done
#+END_SRC



***** COMMENT Old
We need to transfer the relevant materials to HPCC for simulations.


The orchestrators:

#+BEGIN_SRC bash
  root_orchs_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/root_orchs"
  hpcc_url="lotzsamu@hpcc.msu.edu"
  projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  orch_dir="${project_dir}/root_orchs"
  for lig_id in 3 10 18 20; do
    scp "${root_orchs_dir}/sEH_lig-${lig_id}.orch.sqlite" "${hpcc_url}:${orch_dir}/"
  done
#+END_SRC

Then any new configurations we have:

#+BEGIN_SRC bash
  local_configs_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping/data/configurations"

  hpcc_url="lotzsamu@hpcc.msu.edu"
  projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"

  for lig_id in 3 10 18 20; do
      config_dir="${project_dir}/simulations/${lig_id}_simulations/configurations"
      for config in "${local_configs_dir}/${lig_id}"/*; do
          scp "${config}" "${hpcc_url}:${config_dir}/"
      done
  done
#+END_SRC


Move the configurations from the main dir to the simulation inputs directories on HPCC:

#+BEGIN_SRC bash
project_dir="/mnt/home/lotzsamu/projects/seh.pathway_hopping"

for lig_id in 3 10 18 20; do
    lig_sim_dir="${project_dir}/simulations/${lig_id}_simulations"
    config_dir="$lig_sim_dir/configurations"
    cp ${config_dir}/*lig-${lig_id}*.config.dill.pkl "${lig_sim_dir}/input/"
done
#+END_SRC


The task templates:
#+BEGIN_SRC bash
  hpcc_url="lotzsamu@hpcc.msu.edu"
  local_project_dir="/home/salotz/tree/lab/projects/seh.pathway_hopping"
  projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  for template in "${local_project_dir}"/templates/* ; do
      scp ${template} "${hpcc_url}:${project_dir}/templates/"
  done
#+END_SRC

**** COMMENT TODO Test on scratch

#+BEGIN_SRC bash
  job_name="test_job"
  num_workers=8
  # should be placed in the test run dir
  lig_id='3'
  config_name="OpenMMGPUWorker_8-workers_lig-${lig_id}.config.dill.pkl"
  conda activate seh_pathway_hopping

  projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"

  root_orch="${project_dir}/root_orchs/sEH_lig-${lig_id}.orch.sqlite"

  scratch_dir="/mnt/gs18/scratch/users/lotzsamu/seh.pathway_hopping"

  test_dir="${scratch_dir}/test_run"
  pushd $test_dir

  # the new root snapshot
  start_hash="8e158c28c8c426f064d032823bb9aaf9"

  # if you want to use a config in the orchestrator
  # config_hash=""

  rm -rf ${job_name}
  module load CUDA/9.2.88

  # copy the snapshot and config we need for this run to the inputs dir
  wepy get snapshot -O ${start_hash}.snap.dill.pkl ${root_orch} ${start_hash}

  # run the job
  wepy run snapshot \
           --job-name ${job_name} \
           --n-workers ${num_workers} \
           --log DEBUG \
           --checkpoint-freq 1 \
           "${start_hash}.snap.dill.pkl" \
           ${config_name} \
           7200 1000

  popd
#+END_SRC



** Ligand Wepy Simulations

*** Common functions


**** Gather up a list of orchestrators to work on

Because there is a bit of a complicated history of folder formats
(path polymorphism) for how the simulations were saved, and different
versions of the orchestrators, we have a kind of complex procedure for
selecting the correct one. So we encapsulate this into a function to
gather them up.


To run it and derivatives of it:

#+begin_src bash
gather-orch-paths 3 jobid_0 jobid_1
#+end_src

For example the print function will just list the paths at the end:

E.g.:

#+begin_src bash
print-gathered-orchs 3 22160883 22073108 22073109 22073110 22073111 22073112

echo ${orch_paths[@]}
#+end_src

***** script

#+begin_src bash :tangle hpcc/scripts/bash_funcs.sh
  gather-checkpoint-paths () {
      shopt -s globstar
      # get the ligand id and set up paths for this ligand
      lig_id="$1"
      projects_dir="$TREE/lab/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"
      sim_dir="${project_dir}/hpcc/simulations"
      lig_sim_dir="${sim_dir}/${lig_id}_simulations"

      # get just the job ids as an array
      nargs=$#
      let "njobargs = $nargs - 1"
      job_ids=("${@:2:$njobargs}")

      jobs_dir="${lig_sim_dir}/jobs"

      # then loop over them to get the paths to the orchestrators from
      # them
      echo ""
      echo "Getting Paths"
      checkpoint_paths=()
      for i in ${!job_ids[@]}; do

          job_id=${job_ids[$i]}
          echo $job_id

          # get the directory for the job
          job_dir="${lig_sim_dir}/jobs/${job_id}"
          echo $job_dir

          checkpoint=$(echo ${job_dir}/**/checkpoint.orch.sqlite)

          echo "Found checkpoint: $checkpoint"

          # and save it in the array of the orchestrator paths
          checkpoint_paths[i]=${checkpoint[0]}
      done

      # use the $checkpoint_paths variable in a calling function
  }

  print-gathered-checkpoints () {

      # run the command to gather orches. They will be put in the
      # orch_paths array
      gather-checkpoint-paths "$@"

      echo "Printing out the orch paths that are gathered"
      for orch_path in ${checkpoint_paths[@]}; do
          echo "$orch_path"
      done

  }

  print-gathered-checkpoint-snaphashes () {
      gather-checkpoint-paths "$@"

      for idx in ${!checkpoint_paths[@]}; do
          checkpoint=${checkpoint_paths[idx]}
          snaps=$(wepy ls snapshots $checkpoint)
          # make an array out of it
          snaps=($snaps)
          # then get the last one and print
          echo "${checkpoint_paths[idx]} snapsots:"
          echo ${snaps[*]}
      done
  }
#+end_src

***** COMMENT old script
#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
  gather-orch-paths () {
      # get the ligand id and set up paths for this ligand
      lig_id="$1"
      projects_dir="$SCRATCH/tree/lab/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"
      sim_dir="${project_dir}/simulations"
      lig_sim_dir="${sim_dir}/${lig_id}_simulations"

      # get just the job ids as an array
      nargs=$#
      let "njobargs = $nargs - 1"
      job_ids=("${@:2:$njobargs}")

      # then loop over them to get the paths to the orchestrators from
      # them
      echo ""
      echo "Getting Paths"
      orch_paths=()
      output_dirs=()
      sim_results_dirnames=()
      for i in ${!job_ids[@]}; do

          job_id=${job_ids[$i]}
          echo $job_id

          # get the directory for the job
          job_dir="${lig_sim_dir}/jobs/${job_id}"
          echo $job_dir

          # we need to rename them to the original folder that the
          # orchestrator is expecting. Instead of the name of the output
          # folder my job submission system renames it to. If this was
          # already done ignore these and their messages.
          output_dir="${job_dir}/output"

          output_dirs[i]="$output_dir"

          # Note we run the localize_reporter_path.py script over
          # them with the localized paths we won't need to do
          # this. Localizing is the less confusing thing in the future
          # once we take on the burden of renaming paths in the
          # orchestrator

          # If ! (mv ${output_dir} ${job_dir}/exec); then echo "${jobid} already renamed"; fi
          # output_dir="${job_dir}/exec"


          # to get the paths to the orchestrators in the simulation
          # results dir we need to get the name of it which should be
          # the only dir
          sim_results_dir=$(ls -d ${output_dir}/*/)
          # remove trailing slash
          sim_results_dir=${sim_results_dir%/}

          # just the name of the sim results dir
          sim_results_dirname=$(basename ${sim_results_dir})
          sim_results_dirnames[i]="$sim_results_dirname"


          echo "The results dir is: ${sim_results_dir}"

          # find the orchestrator the simulation produced (if at all)
          # this will either be a finished one or a checkpoint, and
          # either of those maybe patched or not. If a patched one exist
          # we preferentially choose that.
          patched_orch_2=(${sim_results_dir}/*.orch.PATCH_V2.PATCH_v3.sqlite)
          patched_orch=(${sim_results_dir}/*.orch.PATCH_v3.sqlite)
          # checkpoint_orch=(${sim_results_dir}/checkpoint.chk)
          patched_checkpoint_orch_2=(${sim_results_dir}/checkpoint.chk.PATCH_V2.PATCH_v3.sqlite)
          patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_v3.sqlite)
          result_orch=(${sim_results_dir}/checkpoint.orch.sqlite)

          if [[ -e $patched_orch_2 ]]; then
              echo "Found a patched orchestrator V2+V3 for $job_id"

              # if the job succeeded there will be a file generated which is
              # the orchestrator for the end of that simulation
              sim_orch_path=$patched_orch_2

          elif [[ -e $patched_orch ]]; then

              echo "Found a patched orch V3 for $job_id"
              sim_orch_path=$patched_orch

          elif [[ -e $patched_checkpoint_orch_2 ]]; then
              echo "Found a patched V2+V3 checkpoint for $job_id"
              sim_orch_path=$patched_checkpoint_orch_2

          elif [[ -e $patched_checkpoint_orch ]]; then
              echo "Found a patched V3 orchestrator for $job_id"
              sim_orch_path=$patched_checkpoint_orch

          # for the newest simulations there is just the
          # checkpoint.orch.sqlite orch, the result orch
          elif [[ -e $result_orch ]]; then
              echo "Found a regular orch for $job_id"
              sim_orch_path=$result_orch

          # and report if none was found just in case
          else
              echo "No orchestrator found for JOB ${job_id}";
              sim_orch_path="None"

          fi

          # and save it in the array of the orchestrator paths
          orch_paths[i]=${sim_orch_path}
      done

      # use the $orch_paths variable in a calling function
  }

  print-gathered-orchs () {

      # run the command to gather orches. They will be put in the
      # orch_paths array
      gather-orch-paths "$@"

      echo "Printing out the orch paths that are gathered"
      for orch_path in ${orch_paths[@]}; do
          echo "$orch_path"
      done

  }

  print-gathered-output-dirs () {
      # run the command to gather orches. They will be put in the
      # orch_paths array
      gather-orch-paths "$@"

      echo "Printing out the output paths that are gathered"
      for output_path in ${output_dirs[@]}; do
          echo "$output_path"
      done

  }
#+end_src


*** Running Simulations

Once we have everything set up it is time to run simulations in
production.

Generate all the task scripts from the run_specs.toml and then
generate the slurm submission scripts.

[[file:analysis.org::gen_lig_task_scripts][gen_lig_task_scripts]]
[[file:analysis.org::gen_lig_submission_scripts][gen_lig_submission_scripts]]

#+BEGIN_SRC bash
gen-lig-task-scripts && gen-lig-submission-scripts
#+END_SRC

or with inv:

#+BEGIN_SRC bash
inv hpcc-gen-submissions
#+END_SRC

Then submit them all by choosing which ones to submit. You should
decide priorities here.

Example:
#+BEGIN_SRC bash
./hpcc/scripts/submit_contig_runs.sh 3 '0-1' '1-0'
#+END_SRC

This will automatically log all the JOB IDs and errors from the
process along with the dates submitted in an org mode file for record
keeping.


**** Script for automatically submitting them based on contig ids:

#+NAME: submit-contig-runs
#+BEGIN_SRC bash :shebang "#!/bin/bash" :tangle hpcc/scripts/submit_contig_runs.sh
  # used to submit jobs for a particular set of jobs based on the
  # contig ids e.g. 0-1, 1-0, 1-1, 2-1 etc. Just matches on the
  # submission script name since the contig_ids are unique to a job

  # get the ligand id and set up paths for this ligand
  lig_id="$1"

  #projects_dir="/mnt/home/lotzsamu/projects"
  projects_dir="/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects"

  project_dir="${projects_dir}/seh.pathway_hopping"
  sim_dir="${project_dir}/hpcc/simulations"
  lig_sim_dir="${sim_dir}/${lig_id}_simulations"

  submission_log_dir="${lig_sim_dir}/submission_logs"
  mkdir -p $submission_log_dir

  submission_dir="${lig_sim_dir}/submissions"

  # go to the ligand simulation directory to submit the jobs
  cd $lig_sim_dir

  # add to the submission log the datetime for this submission block
  submission_log=$submission_log_dir/"$(python -c "from datetime import datetime; print(datetime.today().isoformat())").org"
  touch "$submission_log"
  echo "BATCH: $(date)" >> $submission_log
  echo "" >> $submission_log

  # get just the contig_ids from the inputs as an array
  nargs=$#
  let "ncontigargs = $nargs - 1"
  contig_ids=("${@:2:$ncontigargs}")

  echo "Requested contigs:" >> $submission_log

  for contig_id in ${contig_ids[@]}; do
      echo "- ${contig_id}" >> $submission_log
  done

  echo "" >> $submission_log
  echo "Submitting scripts:" >> $submission_log
  echo "" >> $submission_log


  for contig_id in ${contig_ids[@]}; do
      echo "*** Contig ${contig_id}" >> $submission_log
      echo "$(date)" >> $submission_log
      echo "" >> $submission_log

      # get the submission script by pattern matching the filename and expanding
      sub_script=$(echo "${submission_dir}/*${contig_id}*")

      # check that there is only one script that was matched
      if [[ ${#sub_script[@]} -gt 1 ]]; then
          echo "Multiple scripts found, aborting" >> $submission_log
          echo "" >> $submission_log
          echo "${sub_script[@]}" >> $submission_log
      else
          echo "Script: $(echo ${sub_script})" >> $submission_log
          echo "" >> $submission_log
          echo "submission output: " >> $submission_log
          echo "" >> $submission_log
          sbatch "$(echo ${sub_script})" &>> $submission_log

      fi

  done
#+END_SRC


*** Monitor Simulations

**** Prometheus & Grafana Dashboard

***** TODO Setup the cluster

See the ~prom~ jig.


***** Setup Tunnel to the Prometheus cluster

#+begin_src bash
  start_prom_tunnel () {

      host="$1"

      autossh \
          -M 50100 -- \
          -N \
          -F $HOME/.ssh/config \
          -L 9090:localhost:9090 \
          -L 9093:localhost:9093 \
          -L 3000:localhost:3000 \
          "$host"

  }
#+end_src

***** Setup Tunnels to the job nodes


Use the bash function below to set up the tunnels and specify which
nodes you want and in which order. You need to create all of the
tunnels at once to get proper matching of port ranges.

#+begin_src bash
start_node_tunnels lac-025 lac-026
#+end_src

lac-025 will be job0 and lac-026 will be job1 on prometheus/grafana
etc. 

If you want to keep the same numbering of the jobs if other nodes go
offline use the 'None' string:

#+begin_src bash
start_node_tunnels None lac-026
#+end_src


To list the processes:

#+begin_src bash
ps aux | grep 'ssh -N -F /home/salotz/.ssh/config -J hpcc.dev'
#+end_src

To kill them all, this is done by the func:

#+begin_src bash
pkill -e -f 'ssh -N -F /home/salotz/.ssh/config -J hpcc.dev'
#+end_src

****** Bash Function

#+begin_src bash :tangle scripts/start_node_tunnels.sh

  start_node_tunnels () {
      # give it the nodes to make tunnels to in order of the jobs

      nodes=("${@}")

      echo "Killing existing tunnels"
      pkill -e -f 'ssh -N -F /home/salotz/.ssh/config -J hpcc.dev'

      echo "Getting info for: $nodes"

     for i in ${!nodes[@]}; do

         node="${nodes[$i]}"

         # pad with zeros so we can do more than 10 tunnels
         idx=$(printf "%02d\n" $i)

         if [[ "$node" == 'None' ]]; then

             echo "No node for job $idx, skipping"
         else
             echo "job: $idx JOBID: $node"

             tunn_command="$(cat << EOF
             autossh \
                 -M 501${idx} \
                 -- \
                 -N \
                 -F $HOME/.ssh/config -J hpcc.dev \
                 -L 50${idx}0:localhost:9100 \
                 -L 50${idx}1:localhost:9445 \
                 -L 50${idx}2:localhost:9001 \
                 lotzsamu@$node
  EOF
  )"

             echo "Running this command:"
             echo $tunn_command

             $tunn_command &

             echo "PID: $!"
         fi

     done

     echo "Processes"
     echo "----------------------------------------"
     echo $(ps aux | grep 'ssh -N -F /home/salotz/.ssh/config -J hpcc.dev')
  }
#+end_src


**** squeue command

Good slurm command for viewing jobs:

#+BEGIN_SRC bash
squeue -u $USER -O "jobid,partition,name,username,state,timeused,timelimit,nodelist,reason"
#+END_SRC


**** Copy reporter materials


Go through all the ligands and all the jobs for the ligands and copy
the files from those job outputs and the logs back to their monitoring
folder locally.

#+begin_src bash
bash hpcc/scripts/monitor_sims.sh
#+end_src

This will download the materials to ~data/sim_monitoring~

**** batch script

#+BEGIN_SRC bash :shebang "#!/bin/bash" :tangle hpcc/scripts/monitor_sims.sh
  data_dir="$HOME/tree/lab/projects/seh.pathway_hopping/data"
  monitor_dir="${data_dir}/sim_monitoring"
  mkdir -p "$monitor_dir"

  hpcc_url="lotzsamu@rsync.hpcc.msu.edu"

  scratch_dir="/mnt/gs18/scratch/users/lotzsamu"
  proj_dir="$scratch_dir/tree/lab/projects/seh.pathway_hopping"

  # for lig_id in 3 10 17 18 20; do
  for lig_id in 10; do

      lig_jobs_dir="$proj_dir/hpcc/simulations/${lig_id}_simulations/jobs"

      echo ""
      echo "monitoring Ligand: ${lig_id}"
      echo ""
      echo "### getting all jobs monitoring data for $lig_jobs_dir"

      rsync -ravhhiz --stats \
            --exclude='*.pkl*' \
            --exclude="core.*" \
            --exclude='*.h5' \
            --exclude='*.sqlite' \
            --exclude='*.orch*' \
            --exclude='*.chk*' \
            "${hpcc_url}:${lig_jobs_dir}"/* \
                "${monitor_dir}/${lig_id}/"
  done
#+END_SRC


**** Check last walkers for blowups


Script to run for the working jobs
#+BEGIN_SRC bash :shebang "#!/bin/bash" :tangle hpcc/scripts/check_blowups.sh
  conda activate seh_pathway_hopping

  projects_dir="$SCRATCH/tree/lab/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"
  sim_dir="${project_dir}/hpcc/simulations"
  scripts_dir="${project_dir}/hpcc/scripts"

  log_file="$project_dir/hpcc/blowup_report.log.txt"


  echo "writing results to log file"
  echo $log_file

  date &> $log_file
  echo "Checking for blowups in scratch jobs" &>> $log_file

  for lig_id in 3 10 17 18 20; do
      echo "LIGAND: $lig_id" &>> $log_file
      for job_dir in "${project_dir}/hpcc/simulations/${lig_id}_simulations/jobs"/*; do

          echo "Lig $lig_id Job $job_dir"

          job_name=$(basename $job_dir)
          echo "Checking for job $job_name" &>> $log_file
          echo "" &>> $log_file

          output_dir=(${job_dir}/lig-${lig_id}*/)
          output_dir=${output_dir%/}

          h5_path=$output_dir/*.wepy.h5

          echo "checking h5 file: $h5_path"

          python "$scripts_dir/check_run_for_blowup.py" $h5_path 1>> $log_file

          echo "" &>> $log_file
          echo "" &>> $log_file

      done

  done
#+END_SRC


***** Python Script

Script to open up the HDF5 and look at the last cycle of walkers and
get their positions

#+BEGIN_SRC python :tangle hpcc/scripts/check_run_for_blowup.py
  from wepy.hdf5 import WepyHDF5

  def main(h5_path):
      wepy_h5 = WepyHDF5(h5_path, mode='r')
      wepy_h5.open()

      last_cycle_idx = wepy_h5.num_run_cycles(0) - 1

      num_run_trajs = wepy_h5.num_run_trajs(0)

      trace = [(traj_idx, last_cycle_idx) for traj_idx in range(num_run_trajs)]

      last_cycle_fields = wepy_h5.get_run_trace_fields(0, trace,
                                                       ['positions', 'box_vectors'])

      # get the index of the walker with the maximum value along any dimension

      # the max values for each dimension of each walker state
      max_walkers_dims = last_cycle_fields['positions'].max(axis=1)
      max_walker_idx = max_walkers_dims.max(axis=1).argmax()
      max_walker_max_values = max_walkers_dims[max_walker_idx]

      # the min values for each dimension of each walker state
      min_walkers_dims = last_cycle_fields['positions'].min(axis=1)
      min_walker_idx = min_walkers_dims.min(axis=1).argmin()
      min_walker_min_values = min_walkers_dims[min_walker_idx]

      print("Maximum positions (walker {}): {}".format(max_walker_idx, max_walker_max_values))
      print("Minimum positions (walker {}): {}".format(min_walker_idx, min_walker_min_values))


  if __name__ == "__main__":

      import sys

      h5_path = sys.argv[1]

      try:
          main(h5_path)
      except Exception as e:
          print("Error occured not printing")
#+END_SRC



*** Logging Simulations


You need to log the simulations by job id and get the hashes of the
ends of each of them. You can either look at the logs, or you can run
this script to get them. This is useful because the runs that fail
don't print it out nicely for you.

e.g.:

#+begin_src bash
source ./hpcc/scripts/bash_funcs.sh
print-gathered-checkpoint-snaphashes 10 49459813 49459812
#+end_src


*** Reconciling runs

When a simulation finishes and we want to continue it we start by
reconciling the orchestrators into a new master orchestrator.

We take the opportunity to also combine the HDF5 files for analysis
later.

We need to rename the file back to 'exec' because that is what is
stored in the orchestrator.

We also take note of the hashes of the finished job in our records for
this look at the end of the log.

Then to continue the simulation copy the template for production runs
and just change the start hash.

Job submission for an interactive job to make this faster and less
greedy on HPCC:


For running a bunch of them on the same node we will need a core for
each bash job and a bunch of memory

#+BEGIN_SRC bash
srun -N 1 -c 8 --mem=25G --time=72:00:00 --pty /bin/bash
#+END_SRC


**** batch script: without HDF5 reconciliation


#+NAME: reconcile-run_orchs
#+BEGIN_SRC bash :shebang #!/bin/bash :tangle hpcc/scripts/reconcile_runs_orch.sh
  # env='seh.pathway_hopping.common'

  # get the ligand id and set up paths for this ligand
  lig_id="$1"
  projects_dir="$SCRATCH/tree/lab/projects"
  #projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"

  # make sure we have the bash funcs
  source $project_dir/hpcc/scripts/bash_funcs.sh

  sim_dir="${project_dir}/hpcc/simulations"
  lig_sim_dir="${sim_dir}/${lig_id}_simulations"

  # conda activate $env

  # get the orchestrator paths from the job ids
  #checkpoint_paths=
  gather-checkpoint-paths "$@"

  echo ""
  echo "orchestrators being reconciled"
  for orch in ${checkpoint_paths[@]}; do
      echo "$orch"
  done

  # do the reconciliation

  echo "Reconciling"

  # clean this up in advance

  master_orch_path="${lig_sim_dir}/orchs/master_sEH_lig-${lig_id}.orch.sqlite"
  echo "Reconciling snapshots to: ${master_orch_path}"

  wepy reconcile orch \
       "$master_orch_path" \
       ${checkpoint_paths[*]}
#+END_SRC


If everything goes well then remove the old file
#+BEGIN_SRC bash
cd ${LIG_RESULTS_DIR}
rm all_results_old.wepy.h5
#+END_SRC





**** Patch in Continuations if necessary

There is an issue with connecting the start hashes since they get
patched and change value. (it really is unnecessary but its there so
we gotta deal with it).

So what we do is just figure them out from the jobs table which has
the patched hashes in there.

#+begin_src bash
python -m seh_pathway_hopping.execution.patch_in_continuations $LIG_ID
#+end_src

**** Reconcile with job dirs

We need a special script that will truncate simulations if they
terminated without a final checkpoint, this script makes sure that
this occurs.

Basically you tell it which ligand you want to do this for, then you
provide a list of the JOBID and the run ID (snapshot hash pairs) you
want to do it for.

***** Batch script

E.g.:

#+begin_src bash
  python ./hpcc/scripts/reconcile_hdf5s.py \
            10 \
                64809231      d0cb2e6fbcc8c2d66d67c845120c7f6b,b4b96580ae57f133d5f3b6ce25affa6d \
                64809232      d0cb2e6fbcc8c2d66d67c845120c7f6b,12b9c5a180a98408fa2c234ffe59eebd \
                64809233      d0cb2e6fbcc8c2d66d67c845120c7f6b,0bfd845cc0320fd13bec1643f3bcbae3 \
                64809234      d0cb2e6fbcc8c2d66d67c845120c7f6b,1b2c3187aff093ae84f7d3028840c20b \
#+end_src

***** Reconcile script

#+begin_src python :tangle hpcc/scripts/reconcile_hdf5s.py
  from wepy.hdf5 import WepyHDF5

  def combine_orch_wepy_hdf5s(
          new_orch,
          new_hdf5_path,
          hdf5_paths,
          run_ids=None
  ):



      if run_ids is None:
          run_ids = new_orch.run_hashes()

      else:
          # check that all of the given run_ids are valid runs in the orch
          orch_run_ids = new_orch.run_hashes()

          for run_id in run_ids:

              if not run_id in orch_run_ids:

                  print("The orch run_ids:")
                  print("\n".join([f"{start}, {end}" for start, end in orch_run_ids]))

                  import ipdb; ipdb.set_trace()

                  raise ValueError(
                      f"run {run_id} not in the orchestrator run_ids"
                  )

      # we assume that the run we are interested in is the only run in
      # the WepyHDF5 file so it is index 0
      singleton_run_idx = 0


      # check that all of the runs have enough frames as are specified
      # in orch
      for run_id, wepy_h5_path in zip(run_ids, hdf5_paths):

          orch_run_num_cycles = new_orch.run_last_cycle_idx(*run_id)

          # add one since that was the last cycle index and not the number of cycles
          orch_run_num_cycles += 1

          # get the number of cycles that are in the data for the run in
          # the HDF5 to compare to the number in the orchestrator run
          # record
          wepy_h5 = WepyHDF5(wepy_h5_path, mode='r')
          with wepy_h5:
              h5_run_num_cycles = wepy_h5.num_run_cycles(singleton_run_idx)


          # sanity check for if the number of cycles in the
          # orchestrator is greater than the HDF5
          if orch_run_num_cycles > h5_run_num_cycles:

              # DEBUG
              import ipdb; ipdb.set_trace()

              print(f"Orch Run Cycles: {orch_run_num_cycles}")
              print(f"H5 Run Cycles: {h5_run_num_cycles}")

              # raise ValueError("Number of cycles in orch run is more than HDF5."\
              #                  "This implies missing data, and cannot continue.")

          elif orch_run_num_cycles < h5_run_num_cycles:

              # this would be "anomalous" but not necessarily erroneous
              # so we just issue a warning
              print(f"Orch Run Cycles: {orch_run_num_cycles}")
              print(f"H5 Run Cycles: {h5_run_num_cycles}")

              print(f"WARNING: for {run_id} number of cycles according to Orch checkpoints is less than the number of cycles in the HDF5 dataset, please check that this is intentional. (Likely due to a failed job and restarting from an intermediate checkpoint).")




      print("Combining these HDF5 files:")
      print('\n'.join([str(p) for p in hdf5_paths]))

      # now that we have the paths (or lack of paths) for all
      # the runs we need to start linking them all
      # together.

      # first we need a master linker HDF5 to do this with

      # so load a template WepyHDF5
      template_wepy_h5_path = hdf5_paths[0]
      template_wepy_h5 = WepyHDF5(template_wepy_h5_path, mode='r')

      # clone it
      with template_wepy_h5:
          master_wepy_h5 = template_wepy_h5.clone(new_hdf5_path, mode='x')

      print("Into a single master hdf5 file: {}".format(new_hdf5_path))

      # then link all the files to it
      run_mapping = {}
      for run_id, wepy_h5_path in zip(run_ids, hdf5_paths):

          # in the case where continuations were done from
          # checkpoints then the runs data will potentially (and
          # most likely) contain extra cycles since checkpoints are
          # typically produced on some interval of cycles. So, in
          # order for us to actually piece together contigs we need
          # to take care of this.

          # There are two ways to deal with this which can both be
          # done at the same time. The first is to keep the "nubs",
          # which are the small leftover pieces after the checkpoint
          # that ended up getting continued, and make a new run from
          # the last checkpoint to the end of the nub, in both the
          # WepyHDF5 and the orchestrator run collections.

          # The second is to generate a WepyHDF5 run that
          # corresponds to the run in the checkpoint orchestrator.

          # To avoid complexity (for now) we opt to simply dispose
          # of the nubs and assume that not much will be lost from
          # this. For the typical use case of making multiple
          # independent and linear contigs this is also the simplest
          # mode, since the addition of multiple nubs will introduce
          # an extra spanning contig in the contig tree.

          # furthermore the nubs provide a source of problems if
          # rnus were abruptly stopped and data is not written some
          # of the frames can be corrupted. SO until we know how to
          # stop this (probably SWMR mode will help) this is also a
          # reason not to deal with nubs.

          # TODO: add option to keep nubs in HDF5, and deal with in
          # orch (you won't be able to have an end snapshot...).

          # to do this we simply check whether or not the number of
          # cycles for the run_id are less than the number of cycles
          # in the corresponding WepyHDF5 run dataset.
          orch_run_last_cycle_idx = new_orch.run_last_cycle_idx(*run_id)

          orch_run_num_cycles = orch_run_last_cycle_idx + 1

          # get the number of cycles that are in the data for the run in
          # the HDF5 to compare to the number in the orchestrator run
          # record
          wepy_h5 = WepyHDF5(wepy_h5_path, mode='r')
          with wepy_h5:
              h5_run_num_cycles = wepy_h5.num_run_cycles(singleton_run_idx)


          # copy the run (with the slice)
          with master_wepy_h5:

              # TODO: this was the old way of combining where we would
              # just link, however due to the above discussion this is
              # not tenable now. In the future there might be some more
              # complex options taking linking into account but for now
              # we just don't use it and all runs will be copied by this
              # operation

              # # we just link the whole file then sort out the
              # # continuations later since we aren't necessarily doing
              # # this in a logical order
              # new_run_idxs = master_wepy_h5.link_file_runs(wepy_h5_path)

              # extract the runs from the file (there should only be
              # one). This means copy the run, but if we only want a
              # truncation of it we will use the run slice to only get
              # part of it

              # so first we generate the run slices for this file using
              # the number of cycles recorded in the orchestrator
              run_slices = {singleton_run_idx : (0, orch_run_last_cycle_idx)}

              print(f"Extracting Run: {run_id}")
              print(f"Frames 0 to {orch_run_last_cycle_idx} for a total of {orch_run_num_cycles} out of total number in H5 {h5_run_num_cycles}")

              # then perform the extraction, which will open the other
              # file on its own
              new_run_idxs = master_wepy_h5.extract_file_runs(wepy_h5_path,
                                                              run_slices=run_slices)

              # map the hash id to the new run idx created. There should
              # only be one run in an HDF5 if we are following the
              # orchestration workflow.
              assert len(new_run_idxs) < 2, \
                  "Cannot be more than 1 run per HDF5 file in orchestration workflow"

              run_mapping[run_id] = new_run_idxs[0]

              print("Set as run: {}".format(new_run_idxs[0]))

      print("Done extracting runs, setting continuations")

      with master_wepy_h5:

          # now that they are all linked we need to add the snapshot
          # hashes identifying the runs as metadata. This is so we can
          # map the simple run indices in the HDF5 back to the
          # orchestrator defined runs. This will be saved as metadata on
          # the run. Also:

          # We need to set the continuations correctly betwen the runs
          # in different files, so for each run we find the run it
          # continues in the orchestrator
          for run_id, run_idx in run_mapping.items():

              # set the run snapshot hash metadata except for if we have
              # already done it
              try:
                  master_wepy_h5.set_run_start_snapshot_hash(run_idx, run_id[0])
              except AttributeError:
                  # it was already set so just move on
                  pass
              try:
                  master_wepy_h5.set_run_end_snapshot_hash(run_idx, run_id[1])
              except AttributeError:
                  # it was already set so just move on
                  pass

              # find the run_id that this one continues
              continued_run_id = new_orch.run_continues(*run_id)

              # if a None is returned then there was no continuation
              if continued_run_id is None:
                  # so we go to the next run_id and don't log any
                  # continuation
                  continue

              # get the run_idx in the HDF5 that corresponds to this run
              continued_run_idx = run_mapping[continued_run_id]

              print("Run {} continued by {}".format(continued_run_id, run_idx))

              # add the continuation
              master_wepy_h5.add_continuation(run_idx, continued_run_idx)


  if __name__ == "__main__":

      import sys
      import os
      import os.path as osp
      import shutil as sh
      from pathlib import Path


      from wepy.orchestration.orchestrator import Orchestrator

      # PARSE ARGS

      lig_id = sys.argv[1]

      num_runs = len(sys.argv[2:]) // 2

      print(num_runs)

      job_spec_args = sys.argv[2:]

      job_ids = []
      job_run_ids = []
      for run_idx in range(num_runs):

          job_id = job_spec_args[run_idx*2]
          run_id = job_spec_args[run_idx*2 + 1]

          run_id = tuple(run_id.split(","))

          print(f"Job ID: {job_id}")
          print(f"run id: {run_id}")

          job_ids.append(job_id)
          job_run_ids.append(run_id)


      # SHELL ENV

      tree_dir = Path(osp.expandvars("$TREE"))


      # PATHS

      projects_dir = tree_dir / "lab/projects"

      project_dir = projects_dir / "seh.pathway_hopping"

      data_dir = project_dir /"data"
      results_dir = data_dir / "results"

      # directory to put the reconciled results in
      lig_results_dir = results_dir / f"{lig_id}"

      results_file_path = lig_results_dir / "all_results.wepy.h5"

      sim_dir = project_dir / "hpcc/simulations"
      lig_sim_dir = sim_dir / f"{lig_id}_simulations"

      master_orch_path = lig_sim_dir / f"orchs/master_sEH_lig-{lig_id}.orch.sqlite"

      # CLEAN AND PREPARE DIRS

      os.makedirs(lig_results_dir, exist_ok=True)

      # remove old results if still there
      if results_file_path.exists():
          os.remove(results_file_path)

      ## GET CHECKPOINT AND HDF5 Paths

      job_checkpoints = []
      job_hdf5s = []
      for job_id in job_ids:

          job_dir = lig_sim_dir / "jobs" / f"{job_id}"
          results_dir = list(job_dir.glob(f"lig-{lig_id}_contig-*_production"))[0]

          print(job_id)
          print(results_dir)

          checkpoint_path = results_dir / "checkpoint.orch.sqlite"
          h5_path = list(results_dir.glob(f"*.wepy.h5"))[0]

          print(checkpoint_path.parts[-1])
          print(h5_path.parts[-1])

          job_checkpoints.append(checkpoint_path)
          job_hdf5s.append(h5_path)

      # OPEN ORCH
      orch = Orchestrator(str(master_orch_path), mode='r')

      combine_orch_wepy_hdf5s(
          orch,
          results_file_path,
          job_hdf5s,
          run_ids=job_run_ids
  )
#+end_src



**** COMMENT batch script: with HDF5 reconciliation

#+NAME: reconcile-runs
#+BEGIN_SRC bash :shebang #!/bin/bash -l :tangle hpcc/scripts/reconcile_runs_hdf5.sh
  # inputs are the jobids/jobdir-names of the directories you want to
  # grab results from


  # get the ligand id and set up paths for this ligand
  lig_id="$1"
  projects_dir="$TREE/lab/projects"
  #projects_dir="/mnt/home/lotzsamu/projects"
  project_dir="${projects_dir}/seh.pathway_hopping"

  data_dir="${project_dir}/data"
  results_dir="${data_dir}/results"

  # directory to put the reconciled results in
  lig_results_dir="${results_dir}/${lig_id}"

  # make sure we have the bash funcs
  source $project_dir/hpcc/scripts/bash_funcs.sh

  sim_dir="${project_dir}/hpcc/simulations"
  lig_sim_dir="${sim_dir}/${lig_id}_simulations"


  # lig_results_dir="${lig_sim_dir}/results"


  mkdir -p ${lig_results_dir}

  # get the orchestrator paths from the job ids
  #checkpoint_paths=
  gather-checkpoint-paths "$@"


  echo ""
  echo "orchestrators being reconciled"
  for orch in ${checkpoint_paths[@]}; do
      echo "$orch"
  done

  # do the reconciliation
  # conda activate seh_pathway_hopping
  echo "Reconciling"

  # clean this up in advance

  results_file_path="${lig_results_dir}/all_results.wepy.h5"

  echo "removing old HDF5: ${results_file_path}"

  rm "${results_file_path}"


  master_orch_path="${lig_sim_dir}/orchs/master_sEH_lig-${lig_id}.orch.sqlite"
  echo "Reconciling snapshots to: ${master_orch_path}"

  wepy reconcile orch --hdf5 "$results_file_path" \
       "$master_orch_path" \
       ${checkpoint_paths[*]}

  tmp_path="${lig_results_dir}/tmp_all_results_compressed.wepy.h5"

  echo "Repacking and compressing the reconciled dataset"

  # once we are reconciled compress and replace

  h5repack -f GZIP=2 $results_file_path $tmp_path

  rm $results_file_path

  mv $tmp_path $results_file_path

#+END_SRC


*** COMMENT Show spans


#+begin_src bash
python ./hpcc/scripts/show_spans.py \
              ./hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite
#+end_src

**** script

#+begin_src python :tangle hpcc/scripts/show_spans.py
  import click

  from wepy.orchestration.orchestrator import Orchestrator

  COMMON_ROOT = '6a33f4a4746dc7a1e83296a727c1f8b5'

  # we manually specify the spans since we do patching on the starting
  # snapshots which screws it all up
  SPANS = {
      0 : [
          (COMMON_ROOT, '6d2c47827d6c6bfcc7974fda080f4378'),
          ('e5cdd6a932d7ef99f42af4d0fc0b13c3', '0fe3b4450344a149fdf6778b013dcd79'),
      ],

      1 : [
          (COMMON_ROOT, 'e160f24a6155bf7664b2e6a41b8b2da7'),
      ],

      2 : [
          (COMMON_ROOT, 'af45df1ec10ad6286f3c7ca37401ddef'),
          ('a727fe1b95ef3a91aa103a30f3c734c9', 'f9d613627fdd07fd714afb6a025084e8'),
      ],

      3 : [
          (COMMON_ROOT, '8d219d1788b9c8d3c5f8f649755f1221'),
          ('64c6723e262b80a4a99b4f69a2ebabb9', '61cb809ee5a2fcc8ac2ca96203999770'),
      ],

      4 : [
          (COMMON_ROOT, '1bf3e875f58e8ea7996efb2a3dbce966'),
          ('7f24b6fe68739af9536e1eebf14d870f', '6328a9234a028649ddf94bc40499e083'),
      ],

      5 : [
          ('0b7c48749264490eadac5af79987ca00', 'd6c98acf0b786c8dd94cf957469d0dcf'),
      ],
  }


  def report_spans(orch, spans):
      run_records = orch.get_run_records()
      run_lengths = {(start, end) : n_cycles for start, end, _, n_cycles in run_records}
      span_lengths = {span_id : 0 for span_id in spans.keys()}
      for span_id, span_segs in spans.items():
          for run_id in span_segs:
              span_lengths[span_id] += run_lengths[run_id]
      return span_lengths


  @click.command()
  @click.argument('master_orch', type=click.Path(exists=True))
  def cli(master_orch):

      # open the orch
      orch = Orchestrator(master_orch, mode='r')

      span_lengths = report_spans(orch, SPANS)

      for span, length in span_lengths.items():
          click.echo(f"span {span}: {length}")


  if __name__ == "__main__":

      cli()

#+end_src


*** TODO Backup Simulations


#+BEGIN_SRC bash
inv refugue.rsync --source=icer/scratch --target=volta --force --safe
#+END_SRC

Good simulations need to be backed up. They will first go to the
synology NAS the group has as this will be the long term home for them
anyhow.

During my analysis I should just back up everything so I will just
mirror it directly from HPCC.

Upon completion of the project I will clean it up perhaps.

#+begin_src bash
backup-simulations-to-volta
#+end_src

#+begin_src bash
  rsync -ravvhhiz -n \
        --exclude=".git/" \
        --exclude="_env/" \
        --exclude="_conda_envs" \
          $SCRATCH/tree/ \
          salotz@volta.bch.msu.edu:~/tree
#+end_src

**** Full project backup

From HPCC to volta:

#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
  backup-simulations-to-volta () {
      # rsync dereferencing the symlinks
      rsync -rvLptgoD \
            --exclude=".git/" \
            /mnt/home/lotzsamu/projects/seh.pathway_hopping/ \
            salotz@volta.bch.msu.edu:/volume1/lotzsamu/sEH/seh.pathway_hopping/sims/

  }
#+END_SRC

Move to scratch:

#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
  backup-simulations-to-scratch () {
      # rsync dereferencing the symlinks
      rsync -rvLptgoD \
            --exclude=".git/" \
            /mnt/home/lotzsamu/projects/seh.pathway_hopping/ \
            /mnt/gs18/scratch/users/lotzsamu/seh.pathway_hopping/sims

  }
#+END_SRC



From volta to the locally attached allegheny:

#+BEGIN_SRC bash :tangle hpcc/scripts/backup_icer_scratch_to_volta.sh
  rsync -ravvhhiz -n \
        $SCRATCH/tree/ \
        salotz@volta.bch.msu.edu:~/tree
#+END_SRC


**** Moving individual simulation runs

If we only want individual simulations we do it a little differently.

We want to keep the relative structure of the runs for the
all_results.wepy.h5 file.

We also keep a list of the simulations we want to keep synced for each
ligand. For this we just make a copy of the all_results linker HDF5
then move that to storage.

Make the file and resolve all the external links from the linker file.

Set the ligand ID
#+BEGIN_SRC bash
LIG_ID=3
#+END_SRC


First we want to make a copy of the linker file that has all of the
data inside of it:

#+NAME: compile_hdf5
#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
compile-hdf5() {

    simulations_dir="/mnt/home/lotzsamu/projects/seh.pathway_hopping/simulations"

    lig_id="$1"
    lig_result_dir="${simulations_dir}/${lig_id}_simulations/results"

    # make a temporary directory for this compiled file to go
    tmp_result_dir="$SCRATCH/seh.pathway_hopping/results/${lig_id}"
    mkdir -p "$tmp_result_dir"

    resolved_h5_path="${tmp_result_dir}/all_results_lig-${lig_id}.wepy.h5"
    if ! ( rm ${resolved_h5_path} ); then echo "No existing compiled file"; 
    else echo "removed existing compiled file"; 
    fi

    conda activate seh_pathway_hopping
    echo "copying file"
    wepy copy-h5 "${lig_result_dir}/all_results.wepy.h5" "$resolved_h5_path"
}
#+END_SRC


Sync to volta.

#+NAME: backup_hdf5_to_volta
#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
backup-hdf5-to-volta() {

    lig_id="$1"

    volta_results="salotz@volta.bch.msu.edu:/volume1/lotzsamu/sEH/seh.pathway_hopping/results/${lig_id}"

    tmp_result_dir="$SCRATCH/seh.pathway_hopping/results/${lig_id}"
    resolved_h5_path=${tmp_result_dir}/all_results_lig-${lig_id}.wepy.h5
    rsync -rav ${resolved_h5_path} ${volta_results}/

}
#+END_SRC

Sync to superior.

#+NAME: backup_hdf5_to_superior
#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
backup-to-superior() {

    lig_id="$1"

    superior_results="salotz@superior.bch.msu.edu:/home/salotz/tree/lab/projects/seh.pathway_hopping/results/${lig_id}"

    tmp_result_dir="$SCRATCH/seh.pathway_hopping/results/${lig_id}"
    resolved_h5_path=${tmp_result_dir}/all_results_lig-${lig_id}.wepy.h5
    rsync -rav ${resolved_h5_path} ${superior_results}/

}
#+END_SRC


Sync from HPCC to local

[[file:analysis.org::pull_results][pull_results]]


*** TODO Tree link results

HDF5 results are stored in the
lab/projects/seh.pathway_hopping/results tree subdir.

We want to link them to the "mount point" in the analysis project:

#+BEGIN_SRC bash
ln -s -r ~/tree/lab/projects/seh.pathway_hopping/results/3/all_results_compressed.wepy.h5 \
         ~/tree/lab/projects/seh.pathway_hopping/data/results/3/all_results_lig-3.wepy.h5
#+END_SRC



*** COMMENT old

**** COMMENT Rescuing work dirs from scratch exec dirs

 If you run simulations that don't complete and the exec dir is
 somewhere else in the file system they will need to be moved back to
 main storage, symlinked to mount points, and then have the
 orchestrators reporter paths patched.

 #+BEGIN_SRC bash
   srun -N 1 -c 8 --mem=60G --time=48:00:00 --pty /bin/bash
 #+END_SRC

 For example to copy some jobs to the DicksonLab research directory we
 give the path to the container directory for simulations. Underneath
 it there will be a directory each for the different ligands
 '${lig_id}_simulations/jobs/' where these jobs will be copied to:


 Two possible locations:

 Dickson lab filesystem:

 #+BEGIN_SRC bash
 copy-scratch-runs 3 /mnt/research/DicksonLab/lotzsamu/projects/seh.pathway_hopping/simulations\
                   13327704 13326969 13326970 13326971 13326972 13326973
 #+END_SRC

 Home filesystem:

 #+BEGIN_SRC bash
 copy-scratch-runs 3 /mnt/research/DicksonLab/lotzsamu/data/seh.pathway_hopping/simulations\
                   13327704 13326969 13326970 13326971 13326972 13326973
 #+END_SRC


***** batch script

 #+NAME: copy-scratch-runs
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   copy-scratch-runs () {
       lig_id="$1"

       sim_dir="$2"

       # this is the hardcoded path to the jobs we would like to copy
       # somewhere else i.e. rescue
       execjobs_dir="/mnt/gs18/scratch/users/lotzsamu/seh.pathway_hopping/jobs"

       # to parse out all the remainnig args as a list, we first get all
       # of the args in a list
       nargs=$#

       # then we calculate how many of them are the variadic terms, by
       # subtracting the non-variadic terms
       let "njobargs = $nargs - 2"

       # then slice the variadic terms out
       job_ids=("${@:3:$njobargs}")


       research_projects_dir="/mnt/research/DicksonLab/lotzsamu/projects"
       research_project_dir="${research_projects_dir}/seh.pathway_hopping"

       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"

       scripts_dir="${project_dir}/scripts"

       # depending on whether you want to move them to home or research
       # change this

       lig_sim_dir="${sim_dir}/${lig_id}_simulations"


       jobs_dir="${lig_sim_dir}/jobs"


       for job_id in ${job_ids[@]}; do

           echo "Copying for job: ${job_id}"

           # rename everything to just have the jobid
           job_dir=(${jobs_dir}/${job_id})
           output_dir="${job_dir}/output"

           # make it if it isn't there
           mkdir -p $output_dir

           exec_dir=(${execjobs_dir}/*${job_id}*)

           # copy the output files
           cp -rf ${exec_dir}/* ${output_dir}/

       done

   }
 #+END_SRC


**** COMMENT Symlink from dir

 To symlink jobs to the "mount" point use this function e.g.:

 FOr research:
 #+BEGIN_SRC bash
 symlink-lig-jobs 3 /mnt/research/DicksonLab/lotzsamu/projects/seh.pathway_hopping/simulations
 #+END_SRC

 From the datadir in my home:

 #+BEGIN_SRC bash
 symlink-lig-jobs 20 /mnt/home/lotzsamu/data/seh.pathway_hopping/simulations
 #+END_SRC

***** script
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh

   symlink-lig-jobs () {

       lig_id="$1"
       target_sims_dir="$2"

       sims_dir="/mnt/home/lotzsamu/projects/seh.pathway_hopping/simulations"

       target_jobs_dir=${target_sims_dir}/${lig_id}_simulations/jobs
       mount_jobs_dir=${sims_dir}/${lig_id}_simulations/jobs
       for jobdir in ${target_jobs_dir}/*; do
           jobid=$(basename $jobdir)
           echo "symlinking to ${jobdir}"
           ln -s -f -T $jobdir ${mount_jobs_dir}/$jobid
       done
   }
 #+END_SRC




**** COMMENT Cleaning Orches


 TODO: do I need this anymore??


 The old orches have this issue where they store a bunch of runs in
 them. We need to only have one per those.

 #+begin_src bash
 clean-orchs 3 \
     13326969 13326970 13326971 13326972 13326973 13327704 2503029 3075070 3722763 5940557 5940559 5940766 5940966 5940967 5940968 \
     22160883 22073108 22073109 22073110 22073111 22073112
 #+end_src

***** batch script

 #+begin_src bash :tangle hpcc/scripts/bash_funcs.sh
   clean-orchs () {

         # run the command to gather orches. They will be put in the
         # orch_paths, output_dirs, and sim_results_dirnames arrays
         gather-orch-paths "$@"

         lig_id=$1

         # get just the job ids as an array
         nargs=$#
         let "njobargs = $nargs - 1"
         job_ids=("${@:2:$njobargs}")

         projects_dir="/mnt/home/lotzsamu/projects"
         project_dir="${projects_dir}/seh.pathway_hopping"
         scripts_dir="${project_dir}/scripts"

         sims_dir="${project_dir}/simulations"
         lig_sim_dir="${sims_dir}/${lig_id}_simulations"

         conda activate seh_pathway_hopping

         # iterate over the indices of the job ids
         for i in ${!job_ids[@]}; do

             # get the values for this job
             sim_orch_path="${orch_paths[i]}"

             python ${scripts_dir}/clean_orch.py "${job_ids[i]}" "$sim_orch_path"

         done


   }
 #+end_src


***** python script

 #+begin_src python :tangle hpcc/scripts/clean_orch.py


   # jobid -> end_hash
   CLEAN_RUNS = {
       '2503029'  : 'de390f00dac09f52f97d82ca5b8789fa',
       '3075070'  : '2a2888da6c4733be0328b22eb4b7af26',
       '3722763'  : 'd1317b41436d5d9c48b79c1b500d759f',
       '5940559'  : '055e58cbe616fc271fb25d3daa78b471',
       '13327704' : '0cf1c1dbbdb8a8e91f0bfc5c090a0b74',
       '5940557'  : 'bb621a3fa5b36b7954a3d21745f203aa',
       '13326969' : '8e97b2d28e2afd49db6b2e1c18d48608',
       '5940766'  : '12f792ca1ec034cac0de7d9ff6517c91',
       '13326970' : '1dde5be5baf0faef2233201b68832245',
       '5940966'  : '9b027c1488717712905b00fe3e8cb7a4',
       '13326971' : '1ff9228c6da5cd22713a9d8ec42351ad',
       '5940967'  : 'eaab5d019e061a229dd07e387a242ffc',
       '13326972' : '5756b809fe2f60269f46eff3407d76db',
       '5940968'  : 'ff4b43ac2bb8c4277fbc0cb3b08496c4',
       '13326973' : '18dcfb9749be00a67e22f85105ee378d',
       '22160883' : '7153db16782171f2534656644c98ffea',
       '22073108' : 'ec513e3d59a5b0d921bbe93a9b17857e',
       '22073109' : 'f5835aba9352e5d8c4bd73e22c0b74f5',
       '22073110' : '844000198b1d1c40a6a2a23bf726887b',
       '22073111' : '09706bed29a4b433302be2566848d861',
       '22073112' : 'afbc501c4a89356fc68164cb15fc3ea8',
   }

   def clean_runs(job_id, orch):

       # get the corresponding start hash
       clean_run = [(start_hash, end_hash) for start_hash, end_hash in orch.run_hashes()
                          if end_hash == CLEAN_RUNS[job_id]][0]

       print("purging all things except: {}".format(clean_run))

       clean_config_hash = orch.run_configuration_hash(*clean_run)

       # run_id is the run to keep, get the snapshots that are not this
       bad_snaphashes = [snaphash for snaphash in orch.snapshot_hashes
                         if snaphash not in clean_run]

       #DEBUG
       print("Deleting these snapshots:")
       print(bad_snaphashes)

       # get the runs associated with these snapshots
       dirty_run_ids = [(start_hash, end_hash) for start_hash, end_hash in orch.run_hashes()
                        if (start_hash, end_hash) != clean_run]

       print("Deleting these run ids")
       print(dirty_run_ids)

       # delete the snapshots
       for snapshot_hash in bad_snaphashes:

           if snapshot_hash in orch.snapshot_kv:
               del orch.snapshot_kv[snapshot_hash]

       # then remove those run records of these runs
       print("deleting these configuration")
       for dirty_run_id in dirty_run_ids:

           config_hash = orch.run_configuration_hash(*dirty_run_id)
           print(config_hash)

           # delete the configuration if it is not the clean one
           if (config_hash in orch.configuration_kv) and \
              (config_hash != clean_config_hash):
               del orch.configuration_kv[config_hash]

           # drop the run record
           orch._delete_run_record(*dirty_run_id)

       return orch

   if __name__ == "__main__":

       import sys
       from wepy.orchestration.orchestrator import Orchestrator

       job_id = str(sys.argv[1])
       orch_path = str(sys.argv[2])

       orch = Orchestrator(orch_path,
                           mode='r+', append_only=False)

       print("cleaning orch: {}".format(orch_path))

       orch = clean_runs(job_id, orch)

       print("remaining run records")

       for rec in orch.get_run_records():
           print(rec)

       orch.close()
 #+end_src


**** COMMENT Localizing Orchestrators

 Should patch all of the orchestrators before doing the relocalization
 as it will be faster.

 Because we don't run in containers or use UUIDs for orchestrator
 objects we also have to change the paths of all the reporters so that
 if we want to reference the built outputs of those reporters (i.e. the
 data from the simulation) we have a path we can follow to get
 them. This is used in reconciliation to automatically combine all of
 the HDF5 files in the proper way.

 To localize with the batch script:

 E.g.:
 #+begin_src bash
 localize-orchs 3 22160883 22073108 22073109 22073110 22073111 22073112
 #+end_src

***** batch script for the mount point

 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   localize-orchs () {

       # run the command to gather orches. They will be put in the
       # orch_paths, output_dirs, and sim_results_dirnames arrays
       gather-orch-paths "$@"

       lig_id=$1

       # get just the job ids as an array
       nargs=$#
       let "njobargs = $nargs - 1"
       job_ids=("${@:2:$njobargs}")

       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       scripts_dir="${project_dir}/scripts"

       sims_dir="${project_dir}/simulations"
       lig_sim_dir="${sims_dir}/${lig_id}_simulations"

       conda activate seh_pathway_hopping

       # iterate over the indices of the job ids
       for i in ${!job_ids[@]}; do

           # get the values for this job
           sim_orch_path="${orch_paths[i]}"
           output_dir="${output_dirs[i]}"
           sim_results_dirname="${sim_results_dirnames[i]}"

           # when the cli is used to do runs it takes an argument for a
           # work dir which is used to put all the results of the
           # reporters and dynamically generates the paths to them and
           # reparametrizes all them using a framework in wepy. If that
           # relative structure is preserved then all we need to do is
           # change the work_dir portion of the path in each orchestrator
           # and leave the base filename alone. So we just need to set
           # the new_workdir_path as the path to the outputs dir plus the
           # sim_result dirname
           new_workdir_path="${output_dir}/${sim_results_dirname}"

           echo "setting the configurations for ${sim_orch_path} to ${new_workdir_path}"
           python ${scripts_dir}/localize_reporter_paths.py ${sim_orch_path} ${new_workdir_path}

       done

   }
 #+END_SRC




***** python script

 #+BEGIN_SRC python :tangle hpcc/scripts/localize_reporter_paths.py
   from wepy.reporter.reporter import FileReporter
   from wepy.orchestration.orchestrator import Orchestrator

   def localize_old_orch_reporter_paths(orch, new_workdir_path):

       for run_id in orch.runs:

           # get the configuration used for this run
           run_config = orch._run_configurations[run_id]

           run_config = patch_configuration(run_config, new_workdir_path)

           orch._run_configurations[run_id] = run_config

       return orch

   def patch_configuration(run_config, new_workdir_path):

       # from that configuration find the FileReporters
       for reporter_idx, reporter in enumerate(run_config._reporters):

           if issubclass(type(reporter), FileReporter):

               # then reset the path for each of the file paths it
               # has
               for i, file_path in enumerate(reporter._file_paths):
                   new_file_path = osp.join(new_workdir_path, osp.basename(file_path))
                   reporter._file_paths[i] = new_file_path

               run_config._reporters[reporter_idx] = reporter

       return run_config



   def localize_orch_reporter_paths(orch, new_workdir_path):

       for start_hash, end_hash, config_hash, cycle_idx in orch.get_run_records():

           # get the configuration for this run
           config = orch.get_configuration(config_hash)

           patched_config = patch_configuration(config, new_workdir_path)

           patch_config_hash = orch.add_configuration(patched_config)

           # make a new record with the new configuration
           new_rec = (start_hash, end_hash, patch_config_hash, cycle_idx)

           # update the record with the new value
           orch._update_run_record(*new_rec)

       return orch



   if __name__ == "__main__":

       import sys
       import os
       import os.path as osp
       import pickle

       sim_orch_path = str(sys.argv[1])
       new_workdir_path = str(sys.argv[2])

       # normalize the paths if they were given as relative
       sim_orch_path = osp.abspath(osp.expanduser(sim_orch_path))
       new_workdir_path = osp.abspath(osp.expanduser(new_workdir_path))

       # connect to the orchestrator
       orch = Orchestrator(sim_orch_path,
                              mode='r+',
                              append_only=False)

       # localize the orchestrator and update it in place. This will not
       # remove any configurations but will add new ones and then update
       # the run record for it.
       print("Orch: {}".format(sim_orch_path))
       print("Setting new workdir path:\n{}".format(new_workdir_path))

       orch = localize_orch_reporter_paths(orch, new_workdir_path)

       orch.close()
 #+END_SRC

**** COMMENT Patching HDF5s

Some of the old HDF5s don't have the init walkers. We will add them
just so they are compatible in the future.

**** batch script

#+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh

  patch-run-hdf5s () {

      # run the command to gather orches. They will be put in the
      # orch_paths array
      gather-orch-paths "$@"

      lig_id="$1"
      projects_dir="/mnt/home/lotzsamu/projects"
      project_dir="${projects_dir}/seh.pathway_hopping"

      scripts_dir="${project_dir}/scripts"

      sim_dir="${project_dir}/simulations"
      lig_sim_dir="${sim_dir}/${lig_id}_simulations"

      echo ""
      echo "orchestrators being queried for HDF5s"
      for orch in ${orch_paths[@]}; do
          echo "$orch"
      done

      # do the reconciliation
      conda activate seh_pathway_hopping
      echo "Patching HDF5s"
      for orch in ${orch_paths[@]}; do
          python "${scripts_dir}/patch_wepy_hdf5.py" "$orch"
      done


  }
#+END_SRC

**** script

#+BEGIN_SRC python :tangle hpcc/scripts/patch_wepy_hdf5.py
  from wepy.hdf5 import WepyHDF5

  class InitWalkersError(Exception):
      pass

  def patch_wepy_h5(wepy_h5, run_idx, init_walkers):

      with wepy_h5:

          # check if the init walkers are already set
          try:
              num_walkers = wepy_h5.num_init_walkers(run_idx)
          except KeyError:
              # no initial walkers so we good
              pass
          else:
              raise InitWalkersError("this hdf5 has init walkers already")

          run_grp = wepy_h5._h5['{}/{}'.format('runs', run_idx)]

          # set the initial walkers group
          init_walkers_grp = run_grp.create_group('init_walkers')

          wepy_h5._add_init_walkers(init_walkers_grp, init_walkers)


  if __name__ == "__main__":

      import sys

      from wepy.orchestration.orchestrator import Orchestrator
      from wepy.reporter.hdf5 import WepyHDF5Reporter

      orch_path = sys.argv[1]

      # the job id for simulation we want to patch continues another
      # one. We want to take the initial walkers from the orchestrator
      # snapshot for it

      orch = Orchestrator(orch_path, mode='r')

      assert len(orch.run_hashes()) == 1, "Only works if there is only one run in the orch"

      run_idx = 0

      run_id = orch.run_hashes()[run_idx]
      start_snaphash = run_id[0]

      snapshot = orch.get_snapshot(start_snaphash)

      init_walkers = snapshot.walkers

      # get the hdf5 from the reporter paths

      run_config = orch.run_configuration(*run_id)

      # from that configuration find the WepyHDF5Reporters
      wepy_h5_path = None
      for reporter in run_config.reporters:

          if isinstance(reporter, WepyHDF5Reporter):

              # and save the path for that run
              wepy_h5_path = reporter.file_path
              break

      assert wepy_h5_path is not None

      wepy_h5 = WepyHDF5(wepy_h5_path, mode='r+')

      print("patching the file: {}".format(wepy_h5_path))

      try:
          patch_wepy_h5(wepy_h5, run_idx, init_walkers)
      except InitWalkersError:
          print("not patching, already has initial walkers")
#+END_SRC



** Simulation Analysis


*** Preparing the Legacy Data

This should do everything needed assuming you have the other project
available:

#+begin_src sh
python -m seh_pathway_hopping.execution.make_legacy_results
#+end_src

*** Patching in the Continuations

Sometimes the continuations are not set correctly for the HDF5. This
should be fixed but it might not be.

# TODO


*** Writing Warping Tables

You'll probably want like a CSV table you can pop open in a
spreadsheet editor to look at the weights etc. so we write those here.

**** Span Warping Tables

This will generate tables for each span that had an exit point.

#+begin_src sh
python -m seh_pathway_hopping.execution.gen_spans_warp_table $LIG_ID
#+end_src

**** Gexp Warping Tables

This is for making a unified table of the exit points across the
entire gexp.

#+begin_src sh
python -m seh_pathway_hopping.execution.gen_gexp_warp_table $LIG_ID
#+end_src


*** Write Out Topology Files for Visualization

When you go to visualize you'll want premade topology files (e.g. PDB
files) for loading into the visualizers.

So we make them:

#+begin_src sh
python -m seh_pathway_hopping.execution.gen_ref_tops $LIG_ID
#+end_src

*** Warping Lineages Visualization

First step is just to get all the data from the simulation loaded up
without doing any heavy computations so we can just look at it and see
what it has.


Steps:

1. This will save each warp lineage to a single DCD.

#+begin_src bash
python -m seh_pathway_hopping.execution.gen_warp_lineages_multi $LIG_ID
#+end_src


*** Final Walkers Lineages Visualization


First step is just to get all the data from the simulation loaded up
without doing any heavy computations so we can just look at it and see
what it has.

Get the full lineages from the final walkers, just to visualize the
that the simulations are clean.

Steps:

1. This will save each warp lineage to a single DCD.

#+begin_src bash
python -m seh_pathway_hopping.execution.gen_final_lineages_multi $LIG_ID
#+end_src

*** Highest Progress Walkers Visualization

**** The walkers themselves

top-n is the number of walkers that you are going to save in the traj.

#+begin_src sh
python -m seh_pathway_hopping.execution.gen_high-progress_walkers --top-n=20 $LIG_ID
#+end_src

**** Lineages of them

#+begin_src sh
  python -m seh_pathway_hopping.execution.gen_high-progress_walker_lineages \
         --top-n=5 $LIG_ID
#+end_src


*** Contig Stats

First you will need to export the simulation management tables so that
we can read them and get the user defined span ids i.e. contig_ids
where 0-2 is span 0, segment 2.

To do this go to the table and run the emacs command:
~org-table-export~.

#+begin_src bash
python -m seh_pathway_hopping.execution.gen_contig_stats_table "$LIG_ID"
#+end_src

*** Weights & Rates Plots

Aggregate Probabilities

- [ ] show
#+begin_src bash
python -m seh_pathway_hopping.execution.show_agg_prob_plots "$LIG_ID"
#+end_src

- [ ] save
#+begin_src bash
python -m seh_pathway_hopping.execution.save_agg_prob_plots "$LIG_ID"
#+end_src

Rates:

- [ ] show
#+begin_src bash
python -m seh_pathway_hopping.execution.show_rate_plots "$LIG_ID"
#+end_src

- [ ] save
#+begin_src bash
python -m seh_pathway_hopping.execution.save_rate_plots "$LIG_ID"
#+end_src

Residence Times:

- [ ] show
#+begin_src bash
python -m seh_pathway_hopping.execution.show_rt_plots "$LIG_ID"
#+end_src

- [ ] save
#+begin_src bash
python -m seh_pathway_hopping.execution.save_rt_plots "$LIG_ID"
#+end_src



*** Calculating Observables: Basic

These are observables that are independent of other analyses, like
RMSD. They can be calculated directly from the simulation data.

For derived observables (like PCA projections of TSE point clouds) see
the workflow.

They all make use of the distributed version of the compute observable
since this is necessary for our large datasets.

**** Test: Compute Box Volumes


The other and simpler option is to just use a multiprocessing Pool
with the ~imap~ function to compute them.

I've found this to work well for the not as big stuff:

#+begin_src bash
  python -m seh_pathway_hopping.execution.box_volume_observable_pool -n 3 "gexp=3"
#+end_src


You can set the number of cores and choice of a mapper see ~--help~.

***** On Dask Cluster

Some of the other computations can be quite expensive and a little
tricky to set up so use this to make sure everything is working
properly.

This will run it locally and start up a local dask cluster for gexp 3:

#+begin_src bash
python -m seh_pathway_hopping.execution.box_volume_observable ':local' 'gexp=3'
#+end_src

This will produce a dashboard to see the progress the address of which
should be printed to the terminal.

If you want to run on a cluster you'll need to have one online (see
[[*Schedulers][Schedulers]] for advice on how to do this) then use the address of it
when you run the execution script:

#+begin_src bash
  cluster_addr="10.3.8.48:42805"
  python -m seh_pathway_hopping.execution.box_volume_observable "$cluster_addr" "gexp=3"
#+end_src


**** Ligand RMSD

This can be run with the pool mostly:

#+begin_src bash
  python -m seh_pathway_hopping.execution.lig_rmsd_observable_pool -n 3 "gexp=3"
#+end_src

**** BS-Lig Pair Distances

This is fine to run with the ~mp.Pool~:

#+begin_src bash
  python -m seh_pathway_hopping.execution.lig_bs_atom_pair_dist_observable_pool -n 3 "gexp=3"
#+end_src


**** Ligand SASA

For the sasas you can specify the number of sphere points used in the
calculation which can greatly impact the speed of computations. You
should make sure it doesn't impact the correctness though.

If not specified the default of the mdtraj library will be used,
should be 960 which tends to be slow.

We have found 100 points to be much faster and similar in accuracy.

#+begin_src bash
  python -m seh_pathway_hopping.execution.lig_sasa_observable -n 4 "gexp=3" "n_sphere_points=100"
#+end_src


*** Free Energy Profiles: Basic

**** Ligand RMSD

***** Spans

- [ ] show
#+begin_src bash
python -m seh_pathway_hopping.execution.fe_profile_spans_show '10' 'lig_rmsd'
#+end_src

- [ ] save
#+begin_src bash
python -m seh_pathway_hopping.execution.fe_profile_spans_save '10' 'lig_rmsd'
#+end_src


***** Spans Convergence

- [ ] show
#+begin_src bash
python -m seh_pathway_hopping.execution.fe_profile_spans_convergence_show '10' 'lig_rmsd'
#+end_src

- [ ] save
#+begin_src bash
python -m seh_pathway_hopping.execution.fe_profile_spans_convergence_save '10' 'lig_rmsd'
#+end_src

***** Agg. All Compare

Compare all the different GEXPs for the observable.

#+begin_src bash
python -m seh_pathway_hopping.execution.fe_profile_all_gexps_agg --show --save 'lig_rmsd'
#+end_src


**** TODO Ligand SASA


*** Conformation State Networks

**** Visually Inspect Chosen Atomic Features

The feature type for clustering is the all to all atom distances
between the binding site of the receptor and some atoms on the ligand
that are homologous between all of the query ligands.

Before you cluster you probably want to inspect them to make sure they
look correct.

In an Ipython session you can run this:
#+begin_src python

  from seh_pathway_hopping._tasks import *

  # show them for the main rep, this is what gets used in the features
  for gexp in ('3', '10', '17', '18', '20'):

      print("----------------------------------------")
      print(f"Indices for gexp: {gexp}")

      print("Protein Idxs:")
      print(get_lig_bs_bs_atom_idxs(gexp, rep_key="main_rep"))

      # this is for visualizing
      print("Ligand Idxs:")
      print(get_lig_bs_lig_atom_idxs(gexp, rep_key="main_rep"))

      print("For visualization in the correct_rep:")
      vmd_query_lig_bs_atom_pairs(gexp, rep_key="correct_rep")
#+end_src

You should see output like this, which gives you the VMD query to run
on the correct_rep as well as the absolute indices that are used for
the actual calculation on the 'main_rep':

#+begin_example
  ----------------------------------------
  Indices for gexp: 3
  Protein Idxs:
  [3634 1590 2332 2903 2066]
  Ligand Idxs:
  [43 40 29  0 11 13]
  For visualization in the correct_rep:
  Protein selection indices:  index 3719 1658 2400 2971 2134
  Ligand selection indices:  index 43 40 29 0 11 13
  ----------------------------------------
  Indices for gexp: 10
  Protein Idxs:
  [3634 1590 2332 2903 2066]
  Ligand Idxs:
  [10  5  0 24 35 37]
  For visualization in the correct_rep:
  Protein selection indices:  index 3719 1658 2400 2971 2134
  Ligand selection indices:  index 10 5 0 24 35 37
  ----------------------------------------
  Indices for gexp: 17
  Protein Idxs:
  [3634 1590 2332 2903 2066]
  Ligand Idxs:
  [ 5 10 13 15 22 24]
  For visualization in the correct_rep:
  Protein selection indices:  index 3719 1658 2400 2971 2134
  Ligand selection indices:  index 5 10 13 15 22 24
  ----------------------------------------
  Indices for gexp: 18
  Protein Idxs:
  [3634 1590 2332 2903 2066]
  Ligand Idxs:
  [14 11  0 19 36 37]
  For visualization in the correct_rep:
  Protein selection indices:  index 3719 1658 2400 2971 2134
  Ligand selection indices:  index 14 11 0 19 36 37
  ----------------------------------------
  Indices for gexp: 20
  Protein Idxs:
  [3634 1590 2332 2903 2066]
  Ligand Idxs:
  [46 37 32  0 11 13]
  For visualization in the correct_rep:
  Protein selection indices:  index 3719 1658 2400 2971 2134
  Ligand selection indices:  index 46 37 32 0 11 13
#+end_example


**** Ligand-BS Atom Pairs

Calculate the ligand pair observables for a gexp:

#+begin_src sh
  python -m seh_pathway_hopping.execution.lig_bs_atom_pair_dist_observable_pool "gexp=3"
#+end_src


**** Clustering

If you know what parameters you want to use for clustering add them to
the ~CLUSTER_SPECS~ constant with a "classifier id" and then run the
clustering here and choose which observable to use:

#+begin_src sh
  python -m seh_pathway_hopping.execution.do_gexp_classification \
         "gexp=3" \
         "observable=bs_lig_pair_dists" \
         "clf_id=0"
#+end_src


**** TODO Hyperparameter Optimization

If you don't know exactly what parameters you want to use, do this
step first and then add the specs to the clustering specs.



**** Make and Save a Basic CSN

After you have a clustering you will want to create a network model
and compute some basic data on it for visualization.

The CSN model is a separate model spec from the
classifier/clusterer. There are specs for this which is a similar
structure. These reference a classifier, but also have a lag time and
a specific observable they are using.

You can have different CSNs (just another name for an MSM) by using
the 'tag' option. This is for saving things like different data in
them. Each of these has a number of layouts.

Layout are used in saving the gexf file which is the layout. A full
MSN object can hold different layouts, but a gexf file can only have
one. So choose which layout to use. 'main' is the default one.

#+begin_src sh
  python -m seh_pathway_hopping.execution.save_basic_msm \
         'gexp=3' \
         'csn_id=0' \
         'layout_id=main'
#+end_src

Once this is done you can load the graph model from the gexf file into
gephi for visualization.

You should save a ~.gephi~ file as well but this will not be used in
any analysis.


**** Calculate Stats for Node Observables

This is to calculate the statistics for each node across the network
for the microstates.

#+begin_src sh
  python -m seh_pathway_hopping.execution.calc_msn_obs_node_stats \
         'gexp=3' \
         'csn_id=0' \
         'layout_id=main' \
         'obs_name=lig_rmsd'
#+end_src

Replacing 'lig_rmsd' with whatever the name of the observable is in
the HDF5 dataset.

**** Save Cluster Centers

Write out a DCD for visualization of the cluster centers:

#+begin_src sh
  python -m seh_pathway_hopping.execution.write_cluster_center_dcd \
         'gexp=3' \
         'msm_id=0'
#+end_src

**** Making and saving a Visualization Layout

The first thing you should do is make a layout using force atlas (or
whatever really) in gephi and then save this out to the gexf file.



Once you do that, run this script to reload the gexf file and update
the other files.

#+begin_src sh
    python -m seh_pathway_hopping.execution.update_from_gexf \
           'gexp=3' \
           'csn_id=0' \
           'csn_id=main' \
           'layout_id=0'
#+end_src

Use this throughout when needed.

Load the networks in and take a look at them. This is a good first
step, but you'll want to do more.

**** COMMENT Finding the Native State

This will find the cluster/node that has the starting structure
(i.e. "native state") and make a group for that on the network.

NOTE: this should be done now in the basic MSN creation script so you
probably don't need to run this.

#+begin_src sh
  python -m seh_pathway_hopping.execution.find_native_state \
         'gexp=3' \
         'msm_id=0' \
         'csn_id=main' \
         'layout_id=main'
#+end_src


**** Markov State Modelling

For each CSN we typically also want to apply a process of Markov State
Modelling to them. This is necessary for other analyses like
calculating committor probabilities.

This will transform the CSN by adding trimming groups, but it will
also save the MSM model itself to an msm model file.

Trimming is needed for when certain nodes have no flow back into the
main component of the network. This will compute these nodes according
to the 'msm_id' MSM spec and add them as groups.

#+begin_src sh
  python -m seh_pathway_hopping.execution.msn_markov_model \
         'gexp=3' \
         'msm_id=0' \
         'csn_id=main' \
         'layout_id=main'
#+end_src


*** Compute Committor Probabilities & TS Prediction

**** Choose Basins

First you need to choose the basins.

So run the basin finder and visualize to see if that makes sense:

To run for a specific basin model:

#+begin_src sh
  python -m seh_pathway_hopping.execution.choose_committor_basins \
         'gexp=3' \
         'basin_id=0' \
         'csn_id=main' \
         'layout_id=main'
#+end_src

To run all of them (and save all the data) leave out the basin id:

#+begin_src sh
  python -m seh_pathway_hopping.execution.choose_committor_basins \
         'gexp=3' \
         'csn_id=main' \
         'layout_id=main'
#+end_src

I suggest running them by names and formatting:

#+begin_src sh
  gexp='3'
  csn_id='0'
  layout_id='main'

  cutoff='1'

  for bound in '0.1' '0.2' '0.3'; do
      for unbound in '0.7' '0.6' '0.5'; do

          basin_id="msm-cutoff-${cutoff}_basin-bound-${bound}-unbound-${unbound}-cutoff"

          python -m seh_pathway_hopping.execution.choose_committor_basins \
                 'gexp=${gexp}' \
                 "basin_id=${basin_id}" \
                 'csn_id=${csn_id}' \
                 'layout_id=${layout_id}' || break
      done
  done
#+end_src


Once you do that you can evaluate them with plots and stuff:

#+begin_src sh
  python -m seh_pathway_hopping.execution.evaluate_basins \
         'gexp=3' \
         'csn_id=main'
#+end_src


**** Transition State Prediction

Run this script to compute committor probabilities for a basin model:

The basin model is a model relative to the MSM model and is a unique
choice of basin nodes for calculating committor probabilities on.

This will compute the committor probabilities, add them as properties
of the network.

Further choose a transition state model and save this to the
network. The TS model is really just a range around the 0.5 committor
probability that you want to consider.

#+begin_src sh
  python -m seh_pathway_hopping.execution.predict_ts \
         'gexp=3' \
         'csn_id=0' \
         'layout_id=main' \
         'ts_id=0'
#+end_src


*** Comparative Analysis of TSs


To do a cross analysis of the Transition States we need to do PCA over
the transition states of either some of the simulation data or all of
it.

So here we run a PCA analysis doing a subsampling of the available
simulations and then plot the scores of the models as a boxplot.

All you need to do is choose the ts_id that is saved for each of the
gexps you want to do the model selection for.


Once you have made the TS PCA model from all of the simulations you
can then apply this to the individual datasets for analysis and
visualization. The following will be done:

- calculate the projection observable over all of the microstates as a
  dataset observable
- add network node attributes for projection stats
- Generate 3D structures as PDB/DCD of the ligand centers of mass
  (COMs) with projections as b-factors for 3D visualization
- Calculate Free energy profiles across the TS PCA projections and
  make 1D and 2D plots of this.

For all of these you will need the id of the tspca model.

**** PCA Model Selection & Creation

This will take all combinations of the gexps available and make models
for each of them.

e.g.:

#+begin_src sh
  trim='1'
  bound='0.3'
  unbound='0.6'
  python -m seh_pathway_hopping.execution.ts_pca_model_selection \
         'csn_id=0' \
         "ts_id=basin-msm-cutoff-${trim}_basin-bound-${bound}-unbound-${unbound}-cutoff_committor-msmb"
#+end_src


**** Calculate Projection Observable

#+begin_src sh
  python -m seh_pathway_hopping.execution.calc_tspca_projection_obs \
         'gexp=3' \
         'tspca_id=0'
#+end_src

**** MSN Projection Observable Stats

This will output things to the network data, so you need to specify a
layout as well.

#+begin_src sh
  python -m seh_pathway_hopping.execution.calc_tspca_projection_obs_msn_stats \
         'gexp=3' \
         'tspca_id=0' \
         'csn_id=0' \
         'layout_id=main'
#+end_src


**** COM Trajs

#+begin_src sh
  python -m seh_pathway_hopping.execution.make_tspca_com_trajs \
         'gexp=3' \
         'tspca_id=0' \
         'csn_id=0'
#+end_src


**** Free Energy Profiles

#+begin_src sh
  python -m seh_pathway_hopping.execution.make_tspca_fe_profiles \
         'gexp=3' \
         'tspca_id=0' \
         'csn_id=0'
#+end_src

** Visualization Supporting Data

*** Gephi color bar codes


Pulsar (free energies):

24048F
5C029E
951995
CF4B72
E67553
F4A939
F2ED26


UTB scheme:

386CB0

- T : f0027fff
- U : 386cb0ff
- B : 7fc97fff
- N : cfcfcfff


*** PCA COM Clouds

Convert the Tachyon tga files to pngs with Image Magick

#+BEGIN_SRC bash
convert center_coms_pc0_lig-17-top.tga center_coms_pc0_lig-17-top.png
convert center_coms_pc0_lig-3-top.tga center_coms_pc0_lig-3-top.png
convert center_coms_pc1_lig-17-rside.tga center_coms_pc1_lig-17-rside.png
convert center_coms_pc1_lig-3-rside.tga center_coms_pc1_lig-3-rside.png
convert center_coms_pc0_lig-17-front.tga center_coms_pc0_lig-17-front.png
convert center_coms_pc0_lig-3-front.tga center_coms_pc0_lig-3-front.png
#+END_SRC


*** Pathway microstate selections

- lig3
  - path 1 : 725
  - path 2 : 132
- TPPU
  - path 1 : 400
  - path 2 : 1934


**** Generate PDBs
Generate PDBs of these structures for visualization.

#+BEGIN_SRC python
  from seh_pathway_hopping._tasks import *

  import numpy as np

  import mdtraj as mdj

  from wepy.util.util import traj_box_vectors_to_lengths_angles
  from wepy.util.mdtraj import json_to_mdtraj_topology, traj_fields_to_mdtraj

  from geomm.superimpose import superimpose
  from geomm.grouping import group_pair
  from geomm.centering import center_around
  from geomm.centroid import centroid


  lig3_path1_node = 281 #215 # 725
  lig3_path2_node = 132

  tppu_path1_node = 400
  tppu_path2_node = 1934


  MODEL = 'kcenters_canberra_bs_lig_pair_dists'
  LAG_TIME = 2
  BOUND_CUTOFF = 0.07

  net = lig_state_network_ts(3, MODEL, LAG_TIME, BOUND_CUTOFF)
  lig3_h5 = get_gexp_wepy_h5(3)

  lig3_top = lig_selection_tops(3)['main_rep']

  lig3_path1_frame = net.get_node_attribute(lig3_path1_node, 'center_idx')
  lig3_path2_frame = net.get_node_attribute(lig3_path2_node, 'center_idx')

  with lig3_h5:
      lig3_path1_fields = lig3_h5.get_trace_fields([lig3_path1_frame], ['positions', 'box_vectors'])
      lig3_path2_fields = lig3_h5.get_trace_fields([lig3_path2_frame], ['positions', 'box_vectors'])

  lig3_path1_fields['positions'] = recenter_superimpose_traj(lig3_path1_fields, 3, 'main_rep')[0]
  lig3_path2_fields['positions'] = recenter_superimpose_traj(lig3_path2_fields, 3, 'main_rep')[0]

  lig3_path1_traj = traj_fields_to_mdtraj(lig3_path1_fields, lig3_top)
  lig3_path2_traj = traj_fields_to_mdtraj(lig3_path2_fields, lig3_top)

  lig3_path1_traj.save_pdb(osp.join(media_path(),
                           'path_microstates/path1_frame-{}_lig-3.pdb'.format(lig3_path1_node)))
  lig3_path2_traj.save_pdb(osp.join(media_path(),
                           'path_microstates/path2_frame-{}_lig-3.pdb'.format(lig3_path2_node)))

  ### TPPU

  tppu_h5 = get_legacy_h5()

  leg_clust_assgs = get_legacy_cluster_canonical_traj_assignments()
  leg_cluster_idxs = get_legacy_ts_cluster_idxs()

  assert tppu_path1_node in leg_cluster_idxs
  assert tppu_path2_node in leg_cluster_idxs

  tppu_path1_frame = leg_clust_assgs[tppu_path1_node][0]
  tppu_path2_frame = leg_clust_assgs[tppu_path2_node][0]


  # reference frame
  ref_traj = mdj.load_pdb(osp.join(data_path(), 'top/{}/real_rep_center_ref.pdb'.format(17)))
  centered_ref_positions = ref_traj.xyz[0]

  tppu_sel_idxs = lig_selection_idxs(17)
  tppu_bs_idxs = tppu_sel_idxs['real_rep/binding_site']
  tppu_lig_idxs = tppu_sel_idxs['real_rep/ligand']
  tppu_hom_idxs = tppu_sel_idxs['real_rep/ligand/homology']

  tppu_sel_tops = lig_selection_tops(17)
  tppu_real_rep_top = json_to_mdtraj_topology(tppu_sel_tops['real_rep'])


  with tppu_h5:
      tppu_path1_fields = tppu_h5.get_trace_fields([tppu_path1_frame], ['positions', 'box_vectors'])
      tppu_path2_fields = tppu_h5.get_trace_fields([tppu_path2_frame], ['positions', 'box_vectors'])

  # get the necessary data for recentering of the frames
  tppu_path1_box_lengths, _ = traj_box_vectors_to_lengths_angles(tppu_path1_fields['box_vectors'])
  tppu_path2_box_lengths, _ = traj_box_vectors_to_lengths_angles(tppu_path2_fields['box_vectors'])

  ## regroup, center, and superimpose the frames

  # group the pair of ligand and binding site together in the same image
  tppu_path1_grouped_positions = [group_pair(positions, tppu_path1_box_lengths[idx],
                                      tppu_bs_idxs, tppu_lig_idxs)
                for idx, positions in enumerate(tppu_path1_fields['positions'])]
  tppu_path2_grouped_positions = [group_pair(positions, tppu_path2_box_lengths[idx],
                                      tppu_bs_idxs, tppu_lig_idxs)
                for idx, positions in enumerate(tppu_path2_fields['positions'])]


  # center all the positions around the binding site
  tppu_path1_centered_positions = [center_around(positions, tppu_bs_idxs)
                        for idx, positions in enumerate(tppu_path1_grouped_positions)]
  tppu_path2_centered_positions = [center_around(positions, tppu_bs_idxs)
                        for idx, positions in enumerate(tppu_path2_grouped_positions)]

  # then superimpose the binding sites
  tppu_path1_sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=tppu_bs_idxs)[0]
                                       for pos in tppu_path1_centered_positions])
  tppu_path2_sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=tppu_bs_idxs)[0]
                                       for pos in tppu_path2_centered_positions])

  # write these out
  tppu_path1_traj = mdj.Trajectory(tppu_path1_sup_positions, topology=tppu_real_rep_top)
  tppu_path2_traj = mdj.Trajectory(tppu_path2_sup_positions, topology=tppu_real_rep_top)

  tppu_path1_traj.save_pdb(osp.join(media_path(),
                           'path_microstates/path1_frame-{}_tppu.pdb'.format(tppu_path1_node)))
  tppu_path2_traj.save_pdb(osp.join(media_path(),
                           'path_microstates/path2_frame-{}_tppu.pdb'.format(tppu_path2_node)))
#+END_SRC

**** process renderings

#+BEGIN_SRC bash
convert path2_rep_tppu.tga path2_rep_tppu.png
convert path1_rep_tppu.tga path1_rep_tppu.png
convert path2_rep_lig-3.tga path2_rep_lig-3.png
convert path1_rep_lig-3.tga path1_rep_lig-3.png
#+END_SRC





* Analysis

Refactoring so I can get caching and pipelining etc. so it is easier
to migrate to HPCC for computing stuff and running specific workflows.


Analysis stuff just enough for doing stuff for ACS, the other section
was for all ligands this is just for the one that I have results for.


** Tasks

*** Initialization

Domain specific stuff related to working with the project:
**** Header
#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  """Generated file from the analysis.org file. Do not edit directly."""
#+END_SRC


**** Imports

***** Standard

What comes with the analytics project:

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  # standard library
  import os
  import os.path as osp
  import shutil as sh
  import pickle
  from pathlib import Path
  from copy import copy, deepcopy
  import itertools as it
  import gc
  import time

  # de facto standard library
  import numpy as np
  import pandas as pd
  import sqlalchemy as sqla
  import matplotlib.pyplot as plt

  # extra non-domain specific
  import joblib


#+END_SRC

***** Project

Add your additions in here:

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  # 3rd party domain specific
  import mdtraj as mdj
  import simtk.unit as tkunit

  from tabulate import tabulate

  # auxiliary supporting repos
  import geomm
  import wepy


  # some utilities
  from wepy.util.mdtraj import traj_fields_to_mdtraj


  # project specific auxiliary
  import seh_prep

  # truly ad hoc

#+END_SRC


**** Configuration

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  TREE_PATH = Path(osp.expandvars("$TREE"))

  PROJECT_PATH = TREE_PATH / "lab/projects/seh.pathway_hopping"
#+end_src

**** Paths

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  ## Paths

  # for localizing paths to very commonly used resources and resrouces
  # which may change schema. The directory structure for the rest is the
  # schema, so just use osp.join(project_path(), 'subpath/to/resource')
  # for the rest so a lot of work is reduced in specifying all of them

  def data_path():
      return Path(osp.join(PROJECT_PATH, 'data'))

  def hpcc_path():
      return Path(osp.join(PROJECT_PATH, 'hpcc'))

  def db_path():
      return Path(osp.join(PROJECT_PATH, 'db'))

  def media_path():
      return Path(osp.join(PROJECT_PATH, 'media'))

  def scratch_path():
      return Path(osp.join(PROJECT_PATH, 'scratch'))

  def scripts_path():
      return Path(osp.join(PROJECT_PATH, 'scripts'))

  def src_path():
      return Path(osp.join(PROJECT_PATH, 'src'))

  def tmp_path():
      return Path(osp.join(PROJECT_PATH, 'tmp'))

  def troubleshoot_path():
      return Path(osp.join(PROJECT_PATH, 'troubleshoot'))


  # specific things
  def sqlite_path():
      return Path(osp.join(PROJECT_PATH, 'db/db.sqlite'))

  def joblib_cache_path():
      return Path(osp.join(PROJECT_PATH, 'cache/joblib'))


  # other projects paths

  def projects_path():
      return TREE_PATH / "lab/projects"


  # inhibitors
  def inhibitors_proj_path():

      return projects_path() / "seh.inhibitors"
#+END_SRC


**** Setup

Set up caching of the tasks.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  ## Setup

  # create the sqlite database

  # set up the joblib cache
  jlmem = joblib.Memory(joblib_cache_path())



  # set this when you want to do some recursion stuff with contigtrees
  def set_recursion_limit():
      recursion_limit = 5000
      import sys; sys.setrecursionlimit(recursion_limit)
      print("Setting recursion limit to {}".format(recursion_limit))

  # set the recursion depth since it is always needing to be increased
  set_recursion_limit()
#+END_SRC


**** Data: Read & Write

These functions infer the type of file you want to write based on the
file extension.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def load_obj(filepath):

      import os.path as osp
      import pickle

      import joblib

      fname = osp.basename(filepath)

      # use the file extension for how to load it
      if fname.endswith('jl.pkl'):
          # it is a joblib object so use joblib to load it
          with open(filepath, 'rb') as rf:
              obj = joblib.load(rf)

      elif fname.endswith('pkl'):
          # it is a pickle object so use joblib to load it
          with open(filepath, 'rb') as rf:
              obj = pickle.load(rf)


      return obj


  def save_obj(obj_path, obj, overwrite=False, ext='jl.pkl'):

      import os
      import os.path as osp
      import pickle
      import joblib

      if ext == 'jl.pkl':
          pickler_dump = joblib.dump
      elif ext == 'pkl':
          pickler_dump = pickle.dump
      else:
          raise ValueError("Must choose an extension for format selection")

      # if we are not overwriting check if it exists
      if not overwrite:
          if osp.exists(obj_path):
              raise OSError("File exists ({}), not overwriting".format(obj_path))

      # otherwise make sure the directory exists
      os.makedirs(osp.dirname(obj_path), exist_ok=True)

      # it is a joblib object so use joblib to load it
      with open(obj_path, 'wb') as wf:
          pickler_dump(obj, wf)


  def load_table(filepath):

      import os.path as osp

      import pandas as pd

      fname = osp.basename(filepath)

      # use the file extension for how to load it
      if fname.endswith('csv'):

          df = pd.read_csv(filepath, index_col=0)

      elif fname.endswith('pkl'):

          df = pd.read_pickle(filepath)

      else:
          raise ValueError("extension not supported")



      return df

  def save_table(table_path, df, overwrite=False, ext='csv'):

      import os
      import os.path as osp
      import pickle

      import pandas as pd

      # if we are not overwriting check if it exists
      if not overwrite:
          if osp.exists(table_path):
              raise OSError("File exists ({}), not overwriting".format(table_path))

      # otherwise make sure the directory exists for this observable
      os.makedirs(osp.dirname(table_path), exist_ok=True)

      if ext == 'csv':

          df.to_csv(table_path)

      elif ext == 'pkl':

          df.to_pickle(table_path)

      else:
          raise ValueError("extension not supported")



#+end_src


*** Parameters

The parameters and constants for the pipeline.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  ## Parameters

  # most constants we should take from seh_prep so we don't have
  # duplication and potential error
  import seh_prep.parameters as seh_params

  # TODO: separate the ligand ids from the gexps, since now we have two
  # gexps for the same ligand (TPPU/17)
  GEXPS = ('3', '10', '17', '18', '20', 'TPPU-legacy',)

  LEGACY_GEXPS = ('TPPU-legacy',)

  # TODO: currently the LIG_IDS is used in a lot of places that the
  # GEXPS should be used. These need to be refactored
  LIG_IDS = ('3', '10', '17', '18', '20')

  # maps the gexps to the ligand they use, since this is reused
  GEXP_LIG_IDS = (
      ('3', '3'),
      ('10', '10'),
      ('17', '17'),
      ('18', '18'),
      ('20', '20'),
      ('TPPU-legacy', '17'),
  )


  # number of individual steps per cycle in simulations
  NUM_CYCLE_STEPS = 20000

  CYCLE_TIME = NUM_CYCLE_STEPS * seh_params.STEP_TIME

  GEXP_NUM_WALKERS = {
      '3' : 48,
      '10' : 48,
      '17' : 48,
      '18' : 48,
      '20' : 48,
      'TPPU-legacy' : 48,

  }


#+END_SRC


**** Experimental Constants

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  # load the inhibitor data from the csv table

  def inhibitors_df():
      import os.path as osp
      import pandas as pd

      return pd.read_csv(inhibitors_proj_path() / 'inhibitors.csv')

  # the key for the indexes we use in this analysis
  INDEX_KEY = 'inhibitor_index'

  # TODO: refactor the MFPT to HALFTIME

  # its not the raw rate since it is ln(2) / koff

  # TODO: refactor the RATE to KOFF

  # these will help with the ambiguity and confusion between the units

  # keys for different values. We have a mapping since in the table
  # there is a bunch of complexity in the experimental conditions etc.
  HALFTIME_KEY = 'half-time_human'
  KOFF_KEY = 'koff'

  HALFTIME_ERROR_KEY = 'half-time_error_human'
  KOFF_ERROR_KEY = 'koff_error'


  EXPERIMENTAL_HALFTIME_UNIT = tkunit.minute
  EXPERIMENTAL_HALFTIME_FACTOR = 1

  EXPERIMENTAL_KOFF_UNIT = (1/tkunit.second).unit
  EXPERIMENTAL_KOFF_FACTOR = 1e-4

  # e.g.:
  # koff_q = koff_value * EXPERIMENTAL_KOFF_FACTOR * EXPERIMENTAL_KOFF_UNIT


  def convert_rate_to_halftime(rate_q, time_base=tkunit.minute):

      return (np.log(2) / rate_q).value_in_unit(time_base)

  def convert_halftime_to_rate(halftime_q):

      # lol its the same

      return np.log(2) / halftime_q

  def convert_halftime_to_rt(halftime_q):

      return 1 / (np.log(2) / halftime_q)

  def convert_rt_to_halftime(rt_q):

      # lol its the same

      return np.log(2) / (1 / halftime_q)


  def experimental_halftimes():

      inh_df = inhibitors_df()
      values = [(
          lig_id,
          inh_df[inh_df[INDEX_KEY] == int(lig_id)][HALFTIME_KEY].values[0],
      )
                for lig_id in LIG_IDS]

      # convert to the proper units/factors
      quantities = [(
          lig_id,
          val * EXPERIMENTAL_HALFTIME_FACTOR * EXPERIMENTAL_HALFTIME_UNIT,
      )
                    for lig_id, val in values]

      return tuple(quantities)

  def experimental_koffs():

      inh_df = inhibitors_df()
      values = [(
          lig_id,
          inh_df[inh_df[INDEX_KEY] == int(lig_id)][KOFF_KEY].values[0],
      )
                for lig_id in LIG_IDS]

      # convert to the proper units/factors
      quantities = [(
          lig_id,
          val * EXPERIMENTAL_KOFF_FACTOR * EXPERIMENTAL_KOFF_UNIT,
      )
                    for lig_id, val in values]


      return tuple(quantities)

  def experimental_halftime_errors():


      inh_df = inhibitors_df()
      values = [(
          lig_id,
          inh_df[inh_df[INDEX_KEY] == int(lig_id)][HALFTIME_ERROR_KEY].values[0],
      )
                for lig_id in LIG_IDS]

      # convert to the proper units/factors
      quantities = [(
          lig_id,
          val *  EXPERIMENTAL_HALFTIME_FACTOR * EXPERIMENTAL_HALFTIME_UNIT,
      )
                    for lig_id, val in values]


      return tuple(quantities)



  def experimental_koff_errors():



      inh_df = inhibitors_df()
      values = [(
          lig_id,
          inh_df[inh_df[INDEX_KEY] == int(lig_id)][KOFF_ERROR_KEY].values[0],
      )
                for lig_id in LIG_IDS]

      # convert to the proper units/factors
      quantities = [(
          lig_id,
          val * EXPERIMENTAL_KOFF_FACTOR * EXPERIMENTAL_KOFF_UNIT,
      )
                    for lig_id, val in values]


      return tuple(quantities)
#+END_SRC


**** Figures

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  ## Details on how to save figures and media

  # all the extensions of figures to save from matplotlib
  FIG_EXTENSIONS = (
      'pdf',
  )

  ## Categorical color scheme

  CATEGORY_COLORS = (

      ('blue', (
          ('base', "#C6DBEF"),
          ('pair-light', "#1F77B4"),
      )),

      ('orange', (
          ('base', "#FF7F0E"),
          ('pair-light', "#FFBB78"),
      )),

      ('green', (
          ('base', "#2CA02C"),
          ('pair-light', "#98DF8A"),
      )),

      ('red', (
          ('base', "#D62728"),
          ('pair-light', "#FF9896"),
      )),

      ('purple', (
          ('base', "#9467BD"),
          ('pair-light', "#C5B0D5"),
      )),

      ('brown', (
          ('base', "#8C564B"),
          ('pair-light', "#C49C94"),
      )),

      ('pink', (
          ('base', "#E377C2"),
          ('pair-light', "#F7B6D2"),
      )),

      ('grey', (
          ('base', "#7F7F7F"),
          ('pair-light', "#C7C7C7"),
      )),

      ('yellow', (
          ('base', "#BCBD22"),
          ('pair-light', "#DBDB8D"),
      )),

      ('cyan', (
          ('base', "#17BECF"),
          ('pair-light', "#9EDAE5"),
      )),

  )





  # Each GEXP gets a color assigned to it to be used for all things if
  # possible

  GEXP_COLORS = (
        ('3', 'blue'),
        ('10', 'orange'),
        ('17', 'green'),
        ('18', 'red'),
        ('20', 'purple'),
        ('TPPU-legacy', 'pink'),
  )

  REPLICATE_COLORS = (

      (0, (
          ('base', "#1B9E77"),
      )),

      (1, (
          ('base', "#D95F02"),
      )),

      (2, (
          ('base', "#7570B3"),
      )),

      (3, (
          ('base', "#E7298A"),
      )),

      (4, (
          ('base', "#66A61E"),
      )),

      (5, (
          ('base', "#E6AB02"),
      )),

      (6, (
          ('base', "#A6761D"),
      )),

      # for aggregates of replicates
      ('agg', (
          ('base', "#666666"),
          ('light', "#CCCCCC")
      )),
  )

  # plot colors

  # plots with different lines for different runs with averages with
  # errors need to have consistent colors.







#+end_src


*** Data

**** Legacy Data

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def legacy_project_path():

      return projects_path() / 'seh.tppu_unbinding'

  ### PDB topology stuff

  def get_legacy_real_rep_src_top():
      """Load the two source files necessary to get a complete system. Note
      that the box will not be perfect but it will do for our purposes
      since the ligand at this point is not through the boundaries

      """

      src_pdb_path = legacy_project_path() / 'nowater.pdb'
      src_box_vectors_path = legacy_project_path() / 'image_frame.pdb'

      src_traj = mdj.load_pdb(str(src_pdb_path))
      src_bv_traj = mdj.load_pdb(str(src_box_vectors_path))

      # combine the box vectors with the src traj
      src_traj.unitcell_vectors = src_bv_traj.unitcell_vectors

      return src_traj

  def make_real_rep_top():
      """Using the reference file make a processed version compatible with
      our modern naming schemes.

      """
      import json

      from wepy.util.mdtraj import (
          mdtraj_to_json_topology,
          json_to_mdtraj_topology,
      )

      src_traj = get_legacy_real_rep_src_top()

      # convert to json_top
      src_json_top_str = mdtraj_to_json_topology(src_traj.top)

      src_json_top = json.loads(src_json_top_str)


      # make the new json top starting with some top level stuff
      real_rep_json_top = {
          'bonds' : src_json_top['bonds'],
          # there is only one chain so hardcode
          'chains' : [
              {
                  'index' : src_json_top['chains'][0]['index'],
                  'residues' : [],
              },
          ],
      }

      # process this to rename stuff
      for residue in src_json_top['chains'][0]['residues']:

          # initialize with the same
          new_residue = residue

          # match it against some cases and alter if necessary

          if residue['segmentID'] == 'SML':

              new_residue['segmentID'] = 'HETA'

          if residue['name'] == '2RV':

              new_residue['name'] = 'UNL'

          if (
                  residue['segmentID'] == 'HETA' and
                  residue['name'] == 'SOD'
          ):

              new_residue['segmentID'] = 'SOD'

          # add the new residue to the chain
          real_rep_json_top['chains'][0]['residues'].append(new_residue)

      # make the real rep traj with this topology

      real_rep_mdj_top = json_to_mdtraj_topology(json.dumps(real_rep_json_top))

      real_rep_traj = mdj.Trajectory(
          src_traj.xyz,
          topology=real_rep_mdj_top,
          unitcell_lengths=src_traj.unitcell_lengths,
          unitcell_angles=src_traj.unitcell_angles,
      )

      return real_rep_traj, real_rep_json_top

  def save_real_rep_top():

      import json
      import os
      os.makedirs(
          data_path() / 'legacy/top',
          exist_ok=True
      )

      # make the top from the original source
      real_rep_traj, real_rep_json_top = make_real_rep_top()

      # where to write the mdtraj normalized top
      target_pdb_path = data_path() / 'legacy/top/real_rep.pdb'
      target_json_path = data_path() / 'legacy/top/real_rep.top.json'

      real_rep_traj.save_pdb(str(target_pdb_path))

      with open(target_json_path, 'w') as wf:

          wf.write(json.dumps(real_rep_json_top))

      return real_rep_traj, real_rep_json_top

  def get_real_rep_tops():
      """Get the 'real_rep' for the legacy TPPU system as both mdtraj Traj
      from the PDB and the JSON top.

      This doesn't remake it, just loads from a generated file.
      """

      target_pdb_path = data_path() / 'legacy/top/real_rep.pdb'
      target_json_path = data_path() / 'legacy/top/real_rep.top.json'

      real_rep_traj = mdj.load_pdb(str(target_pdb_path))

      with open(target_json_path, 'r') as rf:

          real_rep_json_top = rf.read()


      return real_rep_traj, real_rep_json_top


  ### Writing out centered and transformed stuff for the reference topology

  def real_rep_selection_idxs_tops(main_sel_idxs, main_sel_tops):
      """Get the special 'real_rep' selection idxs for the legacy top

      Warning
      -------

      Does not give the binding site stuff selection see: real_rep_bs_selection_idxs_tops

  """

      from seh_prep.modules import (
          ligand_idxs,
          protein_idxs,
      )

      from seh_prep.modules import (
          binding_site_idxs,
      )
      from seh_prep.parameters import BINDING_SITE_CUTOFF

      sel_idxs = {}
      sel_tops = {}


      # the initial state
      init_state = get_ref_state('TPPU-legacy')

      # get some things from this
      init_box_vectors = init_state['box_vectors'] # * init_state.box_vectors_unit

      # get the init state for another known gexp
      tmp_state = get_ref_state('17')

      # use this for the units
      positions_unit = tmp_state.positions_unit
      box_vectors_unit = tmp_state.box_vectors_unit

      # load the uncentered reference for bootstrapping purposes
      real_rep_centered_path = data_path() / 'legacy/top/real_rep.pdb'

      real_ref_traj, real_rep_json_top = get_real_rep_tops()

      sel_tops['real_rep'] = real_rep_json_top

      real_rep_positions = real_ref_traj.xyz[0]

      # get the ligand, protein, and binding site indices for this
      real_ligand_idxs = ligand_idxs(real_rep_json_top)
      real_protein_idxs = protein_idxs(real_rep_json_top)


      sel_idxs['real_rep/ligand'] = real_ligand_idxs
      sel_idxs['real_rep/protein'] = real_protein_idxs

      sel_idxs['real_rep/ligand/homology'] = \
          sel_idxs['real_rep/ligand'][
              main_sel_idxs['ligand/homology']
          ]


      # binding site stuff
      real_bs_idxs = binding_site_idxs(
          real_rep_json_top,
          real_rep_positions * positions_unit,
          init_box_vectors * box_vectors_unit,
          BINDING_SITE_CUTOFF,
      )

      sel_idxs['real_rep/binding_site'] = real_bs_idxs


      # get them relative to the all_atoms selections, which is all of
      # them, so we just set them the same
      sel_idxs['all_atoms/real_rep'] = range(real_ref_traj.n_atoms)
      sel_idxs['all_atoms/real_rep/ligand'] = sel_idxs['real_rep/ligand']
      sel_idxs['all_atoms/real_rep/ligand'] = sel_idxs['real_rep/ligand']


      return sel_idxs, sel_tops

  # NOTE: don't need it

  # def make_centered_real_rep_top():

  #     from geomm.grouping import group_pair
  #     from geomm.centering import center_around

  #     traj, _ = get_real_rep_tops()

  #     sel_idxs, _ = real_rep_selection_idxs_tops()

  #     protein_idxs = sel_idxs['real_rep/protein']
  #     lig_idxs = sel_idxs['real_rep/ligand']

  #     # group
  #     grouped_positions = group_pair(
  #         traj.xyz[0],
  #         traj.unitcell_lengths,
  #         protein_idxs,
  #         lig_idxs,
  #     )

  #     # center
  #     centered_positions = center_around(
  #         grouped_positions,
  #         protein_idxs,
  #     )


  #     centered_traj = traj_fields_to_mdtraj(centered_traj_fields)

  #     return centered_traj

  # def save_centered_real_rep_top():
  #     """Save the centered version of the topology for the legaxy
  #     real_rep."""

  #     target_path = data_path() / 'legacy/top/real_rep_center_ref.pdb'

  #     centered_traj = make_centered_real_rep_top()

  #     centered_traj.save_pdb(str(target_path))

  #     return centered_traj





  ### legacy HDF5 stuff
  def legacy_new_template_path():

      return data_path() / 'legacy/linker_template.wepy.h5'

  def legacy_results_file_paths():

      f = osp.join(legacy_project_path(), 'all_trajs.wepy.h5')
      f_ext = osp.join(legacy_project_path(), 'all_trajs_ext.wepy.h5')

      return [f, f_ext]


  def legacy_linker_file_path():

      return osp.join(data_path(), 'legacy/17.wepy.h5')

  def make_legacy_linker_header_template():

      import h5py

      # use ligand 17 as the template
      wepy_h5 = get_gexp_wepy_h5('17')

      # get the reference topologies
      real_rep_traj, real_rep_json = get_real_rep_tops()

      n_atoms = real_rep_traj.n_atoms

      with wepy_h5:
          # we will make the base one the templat
          legacy_template_path = legacy_new_template_path()

          hdf = h5py.File(legacy_template_path, mode='w')

          with hdf:

              # topology
              hdf.create_dataset(
                  'topology',
                  data=real_rep_json,
              )

              # units, the old simulations units are the same as the new ones
              _ = hdf.create_group('units/units')
              for unit_key in list(wepy_h5.h5['units/units']):

                  hdf['units/units'].create_dataset(
                      unit_key,
                      data=wepy_h5.h5[f'units/units/{unit_key}'][()]
                  )

              # settings
              hdf.create_group('_settings')

              hdf.create_group('_settings/alt_reps_idxs')

              hdf.create_dataset('_settings/continuations',
                                 shape=(0,2),
                                 dtype=wepy_h5.h5['_settings/continuations'].dtype,
                                 maxshape=wepy_h5.h5['_settings/continuations'].maxshape)

              hdf.create_group('_settings/field_feature_dtypes')
              hdf.create_group('_settings/field_feature_shapes')

              # TODO: field_feature shapes and types actually set, don't think these
              # are really used outside of data generation

              hdf.create_dataset("_settings/n_atoms", data=n_atoms)

              n_dims = 3
              hdf.create_dataset("_settings/n_dims", data=n_dims)

              # record fields
              hdf.create_group('_settings/record_fields')
              # for rec_field_key in list(wepy_h5.h5['_settings/record_fields']):

              #     p = f'_settings/record_fields/{rec_field_key}'
              #     hdf.create_dataset(
              #         p,
              #         data=wepy_h5.h5[p],
              #     )

              # sparse fields
              hdf.create_dataset(
                  '_settings/sparse_fields',
                  data=[],
              )

              main_rep_idxs = range(n_atoms)
              hdf.create_dataset('_settings/main_rep_idxs', data=main_rep_idxs)

      return hdf


  def make_legacy_results_linker_h5():

      from wepy.hdf5 import WepyHDF5

      # first make sure the linker header template exists
      _ = make_legacy_linker_header_template()

      template_path = legacy_new_template_path()
      component_paths = legacy_results_file_paths()
      linker_path = legacy_linker_file_path()

      template_wepy = WepyHDF5(template_path, mode='r')

      with template_wepy:
          linker_wepy_h5 = template_wepy.clone(linker_path, mode='w')

      with linker_wepy_h5:

          for component_path in component_paths:
              with WepyHDF5(component_path, mode='r') as other_wepy:
                  run_idxs = other_wepy.run_idxs

              for run_idx in run_idxs:
                  new_run_idx = linker_wepy_h5.link_run(component_path, run_idx)

  def get_legacy_h5(
          mode='r',
  ):

      from wepy.hdf5 import WepyHDF5

      path = legacy_linker_file_path()

      # don't open in swmr mode if we have write intent
      if mode in ('r+', ):
          wepy_h5 = WepyHDF5(
              path,
              mode=mode,
          )

      # if its just read use swmr mode
      elif mode in ('r',):
          wepy_h5 = WepyHDF5(
              path,
              mode=mode,
              swmr_mode=True,
          )
      else:
          raise ValueError(f"Invalid mode {mode}")


      return wepy_h5

  ### Data specific to old WExplore
  def get_hist_df():

      import pandas as pd

      path = osp.join(legacy_project_path(), 'hist.csv')

      return pd.read_csv(path, index_col=0)

  @jlmem.cache
  def legacy_walker_traj_map():
      """For a multi-run hist file returns (run_idx, walker_idx)->[(traj_idx, frame_idx),... ]

      Maps the (run_idx, traj_idx) for the HDF5 to the (legacy_traj_idx,
      frame_idx) of the raw set of trajectories.

      """

      from collections import defaultdict

      hist_df = get_hist_df()

      first_frame_map = defaultdict(list)
      second_frame_map = defaultdict(list)

      for run_idx, run_df in hist_df.groupby('run_number'):
          for walker_idx, walker_df in run_df.groupby('walker_idx'):
              for row_idx, row in walker_df.iterrows():

                  first_frame_map[(run_idx, walker_idx)].append(
                      ( int(row['traj_idx']), int(row['first_frame']) )
                  )

                  second_frame_map[(run_idx, walker_idx)].append(
                      ( int(row['traj_idx']), int(row['second_frame']) )
                  )

      return first_frame_map, second_frame_map

  @jlmem.cache
  def legacy_second_frame_map():
      """(leg_traj_idx, leg_frame_idx) -> (run_idx, traj_idx, frame_idx) """

      _, wt_map = legacy_walker_traj_map()

      frame_map = {}
      for rec, leg_trace in wt_map.items():

          run_idx, traj_idx = rec

          for frame_idx, leg_rec in enumerate(leg_trace):

              leg_traj_idx, leg_frame_idx = leg_rec

              frame_rec = (run_idx, traj_idx, frame_idx)

              frame_map[(leg_traj_idx, leg_frame_idx)] = frame_rec

      return frame_map

  @jlmem.cache
  def legacy_first_frame_map():
      """(leg_traj_idx, leg_frame_idx) -> (run_idx, traj_idx, frame_idx) """

      wt_map, _ = legacy_walker_traj_map()

      frame_map = {}
      for rec, leg_trace in wt_map.items():

          run_idx, traj_idx = rec

          for frame_idx, leg_rec in enumerate(leg_trace):

              leg_traj_idx, leg_frame_idx = leg_rec

              frame_rec = (run_idx, traj_idx, frame_idx)

              frame_map[(leg_traj_idx, leg_frame_idx)] = frame_rec

      return frame_map


  def legacy_frame_map():

      return legacy_second_frame_map()

  def get_legacy_cluster_legacy_traj_assignments():

      import pickle

      assg_path = osp.join(legacy_project_path(), 'cluster_frames_map.pkl')
      with open(assg_path, 'rb') as rf:
          traj_assgs = pickle.load(rf)

      return traj_assgs

  def legacy_trace_update(legacy_trace):
      """convert trace of legacy idxs (legacy_traj_idx, legacy_frame_idx) to
      a trace of (run_idx, traj_idx, frame_idx).

      This will not ignore any data at all. If a frame cannot be found a
      None will be used instead.

      """

      ff_map = legacy_first_frame_map()
      sf_map = legacy_second_frame_map()

      new_trace = []
      for leg_rec in legacy_trace:
          if leg_rec in ff_map:
              new_rec = ff_map[leg_rec]
              new_trace.append(new_rec)

          elif leg_rec in sf_map:
              new_rec = sf_map[leg_rec]
              new_trace.append(new_rec)

          else:
              new_trace.append(None)

      return new_trace


  def legacy_trace_canonical_update(legacy_trace):
      """convert trace of legacy idxs (legacy_traj_idx, legacy_frame_idx) to
      a trace of (run_idx, traj_idx, frame_idx).

      This ignores any data that is not a 'second_frame' since the
      firs_frame is redundant. ALso ignores frames that can't be found.

      Puts Nones in their place so you can do indexing if you need.

      Use 'legacy_trace_canonical_filtered_update' to just get the valid
      ones.

      """

      sf_map = legacy_second_frame_map()

      new_trace = []
      for leg_rec in legacy_trace:
          if leg_rec in sf_map:
              new_rec = sf_map[leg_rec]
              new_trace.append(new_rec)

          else:
              new_trace.append(None)

      return new_trace


  def legacy_trace_canonical_filtered_update(legacy_trace):

      sf_map = legacy_second_frame_map()

      new_trace = []
      for leg_rec in legacy_trace:
          if leg_rec in sf_map:
              new_rec = sf_map[leg_rec]
              new_trace.append(new_rec)

      return new_trace

  ### Things related to the old clustering

  @jlmem.cache
  def get_legacy_cluster_canonical_traj_assignments():

      leg_assgs = get_legacy_cluster_legacy_traj_assignments()

      # convert these to the canonical trace filtereing out the junk
      canon_assgs = {}
      for clust_id, leg_trace in leg_assgs.items():

          print("Working on cluster {}".format(clust_id))

          canon_assgs[clust_id] = legacy_trace_canonical_filtered_update(leg_trace)

      return canon_assgs


  def get_legacy_ts_cluster_idxs():

      path = osp.join(legacy_project_path(), 'TSE_idxs.dat')

      with open(path, 'r') as rf:

          cluster_idxs = []
          for line in rf.readlines():
              cluster_idxs.append(int(line.strip()))

      return cluster_idxs
#+END_SRC

**** Orchestrators

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def get_gexp_sim_dir(gexp):

      return hpcc_path() / f'simulations/{gexp}_simulations'

  def get_gexp_orch_path(gexp):

      return get_gexp_sim_dir(gexp) / f'orchs/master_sEH_lig-{gexp}.orch.sqlite'

  def get_gexp_master_orch(gexp):

      from wepy.orchestration.orchestrator import Orchestrator

      orch_path = get_gexp_orch_path(gexp)

      orch = Orchestrator(str(orch_path), mode='r')

      return orch
#+end_src

**** WepyHDF5s

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def all_gexp_wepy_h5s():
      d = {}
      for gexp in GEXPS:
          d[gexp] = get_gexp_wepy_h5(gexp)
      return d

  def gexp_wepy_h5_path(gexp):

      return data_path() / f"results/{gexp}/all_results.wepy.h5"

  def get_gexp_wepy_h5(
          gexp,
          mode='r',
  ):

      # short circuit on special case for legacy
      if gexp in LEGACY_GEXPS:
          return get_legacy_h5(mode=mode)

      from wepy.hdf5 import WepyHDF5

      path = gexp_wepy_h5_path(gexp)

      # don't open in swmr mode if we have write intent
      if mode in ('r+', ):
          wepy_h5 = WepyHDF5(
              path,
              mode=mode,
          )

      # if its just read use swmr mode
      elif mode in ('r',):
          wepy_h5 = WepyHDF5(
              path,
              mode=mode,
              swmr_mode=True,
          )
      else:
          raise ValueError(f"Invalid mode {mode}")


      return wepy_h5


  def delete_wepy_h5_observable(
          gexp,
          obs_name,
  ):

      wepy_h5 = get_gexp_wepy_h5(
          gexp,
          mode='r+',
      )

      with wepy_h5:

          for traj in wepy_h5.iter_trajs():

              try:
                  del traj[f'observables/{obs_name}']
              except KeyError:
                  print("already deleted")
#+END_SRC


**** Groups and Job Data

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  # the parameters used for each test group
  from wepy.resampling.resamplers.resampler import NoResampler
  from wepy.resampling.resamplers.revo import REVOResampler
  from wepy.resampling.resamplers.wexplore import WExploreResampler

  from wepy.resampling.decisions.decision import NoDecision
  from wepy.resampling.decisions.clone_merge import MultiCloneMergeDecision


  GEXP_METADATA = {
      '3' : {
          'resampler' : WExploreResampler,
          'decision' : MultiCloneMergeDecision,
          'num_walkers' : 48,
      },
      '10' : {
          'resampler' : WExploreResampler,
          'decision' : MultiCloneMergeDecision,
          'num_walkers' : 48,
      },
      '17' : {
          'resampler' : WExploreResampler,
          'decision' : MultiCloneMergeDecision,
          'num_walkers' : 48,
      },
      '18' : {
          'resampler' : WExploreResampler,
          'decision' : MultiCloneMergeDecision,
          'num_walkers' : 48,
      },
      '20' : {
          'resampler' : WExploreResampler,
          'decision' : MultiCloneMergeDecision,
          'num_walkers' : 48,
      },
      'TPPU-legacy' : {
          'num_walkers' : 48,
      },

  }

  # we use the job management tables for getting the span IDs and
  # segments so we can reference them in a sane way

  # Datatypes for the ones we care about parsing
  JOBS_TABLE_COLUMNS = (
      ('job_ID', 'string'),
      ('contig_ID', 'string'),
      ('start_hash', 'string'),
      ('patched_start_hash', 'string'),
      ('end_hash', 'string'),
  )

  def get_gexp_jobs_df(gexp):


      jobs_path = data_path() / f'sim_management/lig_{gexp}.csv'

      print("WARNING: Make sure this is a recent table export")
      print(f"reading: {jobs_path}")

      jobs_df = pd.read_csv(jobs_path,
                            dtype=dict(JOBS_TABLE_COLUMNS)
      )

      # add in the gexp as a column
      jobs_df['gexp'] = gexp

      # parse the contig_ID column and get the span_id and segment_idx
      span_id_col = []
      segment_idx_col = []
      for contig_id in jobs_df['contig_ID']:

          span_id, segment_idx = [int(e) for e in contig_id.split('-')]

          span_id_col.append(span_id)
          segment_idx_col.append(segment_idx)

      jobs_df['span_id'] = span_id_col
      jobs_df['segment_idx'] = segment_idx_col

      return jobs_df

  def get_master_jobs_df():

      dfs = []

      for gexp in GEXPS:
          dfs.append(get_gexp_jobs_df(gexp))

      return pd.concat(dfs)

  def get_gexp_jobs():

      gexp_jobs = {}
      for gexp in GEXPS:

          jobs_df = get_gexp_jobs_df(gexp)

          gexp_jobs[gexp] = list(grp_df['job_ID'].values)

      return gexp_jobs


  def get_gexp_paths():
      """Get a dictionary of all of the paths for the jobs dirs for each
      experimental group."""

      raise NotImplementedError

      # gexp_jobs = get_gexp_jobs()

      # # TODO:
      # sims_dir = data_path() / 'sims/wepy/jobs'

      # gexp_paths = defaultdict(list)
      # for grp_name, jobids in gexp_jobs.items():

      #     for job_id in jobids:
      #         path = sims_dir / str(job_id)
      #         gexp_paths[grp_name].append(path)

      # return gexp_paths


  def get_gexp_jobs_to_runs_map(gexp):
      """Get the runs in the HDF5 for the gexp that are mapped to by the
      jobids."""

      wepy_h5 = get_gexp_wepy_h5(gexp)

      jobs_df = get_gexp_jobs_df(gexp)

      jobs_runs_d = {}
      with wepy_h5:

          for run_idx in wepy_h5.run_idxs:
              run_grp = wepy_h5.run(run_idx)

              # this is an attribute as well
              run_idx = run_grp.attrs['run_idx']

              # get the start and end hashes from the HDF5 runs and use
              # this to get the job id for this run idx. Just use the
              # end hash for this

              end_hash = run_grp.attrs['end_snapshot_hash']

              # get the row from the dataframe for this
              job_row = jobs_df[jobs_df['end_hash'] == end_hash]

              jobid = job_row['job_ID'].values[0]

              jobs_runs_d[jobid] = run_idx

      return jobs_runs_d


  def get_gexp_span_ids(gexp):

      gexp_jobs_df = get_gexp_jobs_df(gexp)

      span_ids = sorted(list(set(gexp_jobs_df['span_id'].values)))

      return span_ids

  def get_gexp_span_ids_run_idxs(gexp):
      """Get a mapping of the span_ids (names given to them) to the run idxs
      (in order of the segments in the span via 'segment_idxs').

      The span_ids and segment_idxs come from the jobs_df table.

      """

      gexp_jobs_df = get_gexp_jobs_df(gexp)

      # get the mapping of jobids -> h5 run idxs
      jobs_runs_d = get_gexp_jobs_to_runs_map(gexp)

      # then go through each span and get the run idxs for it
      spans_runs = {}
      for span_id, span_df in gexp_jobs_df.groupby('span_id'):

          # make records for each segment so that we can sort them
          span_segs = []
          for idx, row in span_df.iterrows():

              seg_rec = (row['segment_idx'], row['job_ID'])
              span_segs.append(seg_rec)

          # sort them, dereference the run_idx
          span_run_idxs = [jobs_runs_d[jobid]
                           for seg_idx, jobid in sorted(span_segs)]

          spans_runs[span_id] = span_run_idxs

      return spans_runs


#+end_src

**** Contig Trees

Get handles and contig trees to all the files.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py


  def gexps_contigtree():
      d = {}
      for gexp in GEXPS:
          d[gexp] = get_contigtree(gexp)

      return d


  # the base contigtree is the one that can be cached not the one with
  # the HDF5 file in it, so we cache this, you can use the other
  # function to get the contigtree with the HDF5, but the openening of
  # file handles is annoying and this might be the best way to do this
  # anyhow
  @jlmem.cache
  def get_base_contigtree(gexp,
                          runs=Ellipsis,
  ):

      # uncache

      from wepy.analysis.contig_tree import BaseContigTree

      from wepy.resampling.decisions.clone_merge import MultiCloneMergeDecision
      from wepy.boundary_conditions.receptor import UnbindingBC

      wepy_h5 = get_gexp_wepy_h5(gexp)

      set_recursion_limit()
      base_contigtree = BaseContigTree(
          wepy_h5,
          runs=runs,
          decision_class=MultiCloneMergeDecision,
          boundary_condition_class=UnbindingBC)

      return base_contigtree

  def get_contigtree(gexp,
                     runs=Ellipsis):

      from wepy.analysis.contig_tree import ContigTree

      wepy_h5 = get_gexp_wepy_h5(gexp)
      base_contigtree = get_base_contigtree(gexp,
                                            runs=runs
      )

      return ContigTree(wepy_h5,
                        base_contigtree=base_contigtree,
      )

  @jlmem.cache
  def get_gexp_span_ids_span_idxs_map(gexp):
      """Get a mapping of the span_ids to the span_idxs in the
      contigtree."""

      contigtree = get_base_contigtree(gexp)

      # get the first run idx -> span_id map
      run_idxs_to_span_ids = {run_idxs[0] : span_id
                              for span_id, run_idxs
                              in get_gexp_span_ids_run_idxs(gexp).items()}

      span_ids_to_span_idxs_d = {}
      for span_idx, trace in contigtree.span_traces.items():
          run_idx = trace[0][0]

          span_id = run_idxs_to_span_ids[run_idx]

          span_ids_to_span_idxs_d[span_id] = span_idx

      return span_ids_to_span_idxs_d


  def get_gexp_span_contig(gexp, span_id):

      # get the span idx in the contigtree from the span_id in the jobs
      # table
      span_ids_to_span_idxs_d = get_gexp_span_ids_span_idxs_map(gexp)

      span_idx = span_ids_to_span_idxs_d[span_id]

      # get the contig for this span
      contigtree = get_contigtree(gexp)
      contig = contigtree.span_contig(span_idx)

      return contig
#+END_SRC


**** Reference States

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def ref_states():

      ref_states = {}

      for gexp in GEXPS:
          ref_states[gexp] = get_ref_state(gexp)

      return ref_states


  def get_ref_state(gexp):

      from wepy.walker import WalkerState

      if gexp in LEGACY_GEXPS:

          # get the PDB mdtraj traj
          ref_traj, _ = get_real_rep_tops()

          # convert to a state object
          kwargs = {
              'positions' : ref_traj.xyz[0],
              'box_vectors' : ref_traj.unitcell_vectors[0],
          }

          init_state = WalkerState(**kwargs)

      else:

          init_state_path = data_path() / \
                f"md_systems/{gexp}/sEH_lig-{gexp}_equilibrated.state.pkl"

          with open(init_state_path, 'rb') as rf:
              init_state = pickle.load(rf)

      return init_state


  def get_ref_state_traj_fields(gexp, rep_key='main_rep'):

      import numpy as np

      selection_idxs = lig_selection_idxs(gexp)
      tops = lig_selection_tops(gexp)
      ref_state = get_ref_state(gexp)

      ref_state_traj_fields = {}

      if rep_key is None or rep_key == 'all_atoms':
          positions = ref_state['positions']
      else:
          positions = ref_state['positions'][
                           selection_idxs['all_atoms/{}'.format(rep_key)]]

      ref_state_traj_fields['positions'] = np.array([positions])
      ref_state_traj_fields['box_vectors'] = np.array([ref_state['box_vectors']])

      return ref_state_traj_fields

  def get_centered_ref_state_traj_fields(gexp, rep_key='main_rep'):
      """Get the reference coordinates for this gexp and rep.

      This is typically used for grouping, aligning, and recentering
      other structures to a common reference point.

      """

      import numpy as np
      from wepy.util.util import box_vectors_to_lengths_angles

      from geomm.grouping import group_pair
      from geomm.centering import center_around

      # get the selection idxs
      sel_idxs = lig_selection_idxs(gexp)

      # get the required selections for the grouping
      lig_idxs = sel_idxs['{}/ligand'.format(rep_key)]
      bs_idxs = sel_idxs['{}/binding_site'.format(rep_key)]

      # get the native state as a traj field from the all atoms
      # reference state
      ref_fields = get_ref_state_traj_fields(gexp, rep_key=rep_key)

      # then group and center the reference state
      ref_box_lengths, _ = box_vectors_to_lengths_angles(ref_fields['box_vectors'][0])

      grouped_ref_positions = group_pair(ref_fields['positions'][0], ref_box_lengths,
                                         bs_idxs, lig_idxs)

      centered_ref_positions = center_around(grouped_ref_positions, bs_idxs)

      ref_fields['positions'] = centered_ref_positions

      return ref_fields



  def ligs_ref_selection_trajs():

      d = {}
      for gexp in GEXPS:
          d[gexp] = lig_ref_selection_trajs(gexp)

      return d

  @jlmem.cache
  def lig_ref_selection_trajs(gexp):

      import numpy as np

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      selection_idxs = lig_selection_idxs(gexp)
      tops = lig_selection_tops(gexp)
      ref_state = get_ref_state(gexp)

      ref_state_fields = {key : np.array([value]) for key, value in ref_state.dict().items()}

      # make fields for the different representations

      # DEBUG: we have issues with the main rep here so we just don't do this one
      if gexp != 17:
          ref_state_fields['main_rep'] = ref_state['positions'][selection_idxs['all_atoms/main_rep']]
      ref_state_fields['image'] = ref_state['positions'][selection_idxs['all_atoms/image']]
      ref_state_fields['correct_rep'] = ref_state['positions'][selection_idxs['all_atoms/correct_rep']]
      ref_state_fields['ligand'] = ref_state['positions'][selection_idxs['all_atoms/ligand']]

      # for all atoms
      all_atoms_traj = traj_fields_to_mdtraj(ref_state_fields, tops['all_atoms'],
                                             rep_key='positions')

      # main rep
      # DEBUG
      if gexp != 17:
          main_rep_traj = traj_fields_to_mdtraj(ref_state_fields, tops['main_rep'],
                                                 rep_key='main_rep')

      # image
      image_traj = traj_fields_to_mdtraj(ref_state_fields, tops['image'],
                                             rep_key='image')

      # correct rep
      correct_rep_traj = traj_fields_to_mdtraj(ref_state_fields, tops['correct_rep'],
                                             rep_key='correct_rep')

      # ligand
      ligand_traj = traj_fields_to_mdtraj(ref_state_fields, tops['ligand'],
                                             rep_key='ligand')

      ref_selections = {}
      ref_selections['all_atoms'] = all_atoms_traj
      if gexp != 17:
          ref_selections['main_rep'] = main_rep_traj
      ref_selections['image'] = image_traj
      ref_selections['correct_rep'] = correct_rep_traj
      ref_selections['ligand'] = ligand_traj

      return ref_selections



  def lig_centered_ref_selection_trajs(gexp):

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      # DEBUG
      if gexp == 17:
          rep_keys = ('all_atoms', 'correct_rep',)
      else:
          rep_keys = ('all_atoms', 'main_rep', 'correct_rep',)

      tops = lig_selection_tops(gexp)

      ref_selections = {}
      for rep_key in rep_keys:
          top = tops[rep_key]
          ref_state_fields = get_centered_ref_state_traj_fields(gexp, rep_key=rep_key)
          ref_selections[rep_key] = traj_fields_to_mdtraj(ref_state_fields, top,
                                                          rep_key='positions')

      return ref_selections
#+END_SRC

**** Topologies

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_topologies():
      tops = {}
      for gexp in GEXPS:

          tops[gexp] = get_topology(gexp)

      return tops


  def get_topology(gexp):


      if gexp in LEGACY_GEXPS:

          _, json_top = get_real_rep_tops()

      else:
          top_path = data_path() / f"md_systems/{gexp}/sEH_lig-{gexp}_system.top.json"

          with open(top_path, 'rb') as rf:
              json_top = rf.read()

      return json_top
#+END_SRC


**** Number of walkers

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  # was previously named get_lig_num_walkers
  def get_gexp_span_num_walkers(gexp, span_id):

      # all are the same
      return GEXP_NUM_WALKERS[gexp]
#+end_src


*** Generating Files

**** Write Reference Topology PDBs
Write out topology PDBs for all the representations of each
ligand. These should be from the initial walkers so we can also use
them as reference states.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def write_ligs_ref_selection_pdbs():

      for lig_id in LIG_IDS:
          write_lig_ref_selection_pdbs(lig_id)

  def write_lig_ref_selection_pdbs(lig_id):

      import os
      import os.path as osp

      ref_selections = lig_ref_selection_trajs(lig_id)

      lig_top_dir = osp.join(data_path(), 'top/{}'.format(lig_id))

      # make sure the directory exists
      os.makedirs(lig_top_dir,
                  exist_ok=True)

      for sel_name, sel_ref_traj in ref_selections.items():

          pdb_path = osp.join(lig_top_dir, "{}_ref.pdb".format(sel_name))
          sel_ref_traj.save_pdb(pdb_path)

  def write_lig_centered_ref_selection_pdbs(lig_id):

      import os
      import os.path as osp

      ref_selections = lig_centered_ref_selection_trajs(lig_id)

      lig_top_dir = osp.join(data_path(), 'top/{}'.format(lig_id))

      # make sure the directory exists
      os.makedirs(lig_top_dir,
                  exist_ok=True)

      for sel_name, sel_ref_traj in ref_selections.items():

          pdb_path = osp.join(lig_top_dir, "{}_center_ref.pdb".format(sel_name))
          sel_ref_traj.save_pdb(pdb_path)
#+END_SRC


**** Warp Table

***** By Span

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_span_warp_table(gexp, span_idx, overwrite=True):

      table_path = data_path() / f'warp-table/spans/{gexp}_{span_idx}__warps.csv'

      warps_df = get_span_warps_df(gexp, span_idx)

      # only write if there is data
      if len(warps_df) > 0:
          save_table(table_path, warps_df, overwrite=overwrite)

#+end_src



***** By GEXP

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_gexp_warp_table(gexp, overwrite=True):

      table_path = data_path() / f'warp-table/gexps/{gexp}_warps.csv'

      warps_df = get_gexp_warps_df(gexp)

      # only write if there is data
      if len(warps_df) > 0:
          save_table(table_path, warps_df, overwrite=overwrite)

#+end_src



**** Span Stats

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_span_stats_table(gexp, overwrite=True):

      table_path = data_path() / f'span-stats/{gexp}_span-stats.csv'

      spans_df = get_span_stats_df(gexp)

      save_table(table_path, spans_df, overwrite=overwrite)

#+end_src


*** Accessing Data

**** Correct Rep

There was an old issue with some histidines getting left out due to
bad queries in mdtraj.

So I made it that new simulations would save a "missing" alt rep which
splits up the main reps and the missing data.

This ended up becoming obsolete because I ended up throwing out that
data that didn't have the missing data.

Anyhow, this wasn't fixed in those simulations, so we need to patch it
together in the end for visualizing stuff.

There isn't a helper function in WepyHDF5 for this (and wouldn't make
sense to) so I have to provide it here.


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def traj_fields_to_correct_rep(
          traj_fields,
          gexp,
  ):

      # lookup the ligand
      lig_id = dict(GEXP_LIG_IDS)[gexp]

      return traj_fields_to_correct_rep_lig(
          traj_fields,
          lig_id
      )

  # based on ligand
  def traj_fields_to_correct_rep_lig(
          traj_fields,
          lig_id,
  ):
      """Given a traj_fields struct with both the 'positions'
      (i.e. 'main_rep') and the 'alt_reps/missing' fields returns a
      traj_fields struct where 'positions' is the 'correct_rep'.

      Will just copy all the other fields over.

      """

      # there is no correct rep data for ligand 3 so skip it
      if lig_id == '3':
          raise ValueError("Cannot make correct rep for ligand 3.")

      assert 'alt_reps/missing' in traj_fields, \
          "The field 'alt_reps/missing' must be in the traj_fields"

      assert 'positions' in traj_fields, \
          "The field 'positions' must be in the traj_fields"

      # get the number of frames, we know positions will be here
      n_frames = traj_fields['positions'].shape[0]

      # get the selection idxs for this ligand
      sel_idxs = lig_selection_idxs(lig_id)

      correct_idxs = sel_idxs['all_atoms/correct_rep']

      # the indices of the missing atoms in the correct rep
      correct_missing_idxs = sel_idxs['correct_rep/missing']
      correct_main_rep_idxs = sel_idxs['correct_rep/main_rep']

      correct_rep_positions = np.zeros((n_frames, len(correct_idxs), 3))

      correct_rep_positions[:, correct_main_rep_idxs, :] = traj_fields['positions'][...]
      correct_rep_positions[:, correct_missing_idxs, :] = traj_fields['alt_reps/missing'][...]

      # make the new struct, but leave out the old stuff
      correct_traj_fields = {
          key : value for key, value in traj_fields.items()
          if key not in ('positions', 'alt_reps/missing')
      }

      # now set the correct one in
      correct_traj_fields['positions'] = correct_rep_positions

      return correct_traj_fields
#+END_SRC

**** Misc. Models

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_model(
          model_name,
          gexp,
          ext="jl.pkl",
  ):

      model_path = data_path() / f'models/{model_name}/{gexp}.{ext}'

      return load_obj(model_path)
#+END_SRC


**** Observables

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def ls_h5_observables(gexp):
      """List the observables that are in the HDF5 file."""

      wepy_h5 = get_gexp_wepy_h5(gexp)
      with wepy_h5:
          obs_names = wepy_h5.observable_field_names

      print('\n'.join(obs_names))

      return obs_names


  def ls_dir_observables(gexp):
      """List the observables that are in the file system as a pickle
      file."""

      import os
      import os.path as osp

      observables_dir = osp.join(data_path(), 'observables')

      obs_names = []
      for f in os.listdir(observables_dir):
          obs_names.append(f.split('.')[0])

      print('\n'.join(obs_names))

      return obs_names

  def get_observable_h5(
          observable_name,
          gexp,
  ):

      wepy_h5 = get_gexp_wepy_h5(gexp)

      observable_key = f'observables/{observable_name}'

      with wepy_h5:

          # the reshaped run data initialized for the runs and trajs
          data = [[[] for traj_idx in range(wepy_h5.num_run_trajs(run_idx))]
                  for run_idx in range(wepy_h5.num_runs)]

          for traj_id, traj_fields in wepy_h5.iter_trajs_fields(
                  [observable_key],
                  idxs=True,
          ):

              run_idx = traj_id[0]
              traj_idx = traj_id[1]

              # add the observable to the data, we destructure since
              # traj_fields can have more than one field
              data[run_idx][traj_idx] = traj_fields[observable_key]

      return data

  def get_observable_fs(
          observable_name,
          gexp,
  ):

      obs_dir = data_path() / f'observables/{observable_name}'

      # check the folder for the file you want to load depending on the
      # ligand ID, we will use the extension to figure out how to load
      # it
      matches = []
      for f in os.listdir(obs_dir):
          if f.startswith(f'gexp-{gexp}'):
              matches.append(f)

      if len(matches) > 1:
          raise ValueError("Multiple matches")
      elif len(matches) < 1:
          raise ValueError("No match")
      else:
          fname = matches[0]

          obs_path = obs_dir / fname

          return load_obj(obs_path)

  def get_observable(
          observable_name,
          gexp,
          source="h5",
  ):
      """Get an observable in a standard format. Specify 'h5' or 'fs' to get
      it from the HDF5 file or the filesystem."""

      if source == 'h5':
          return get_observable_h5(
              observable_name,
              gexp,
          )

      elif source == 'fs':
          return get_observable_fs(
              observable_name,
              gexp,
          )


#+END_SRC

**** Clustering Models

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def ls_clustering_models():
      """List the observables that are in the file system as a pickle
      file."""

      observables_dir = data_path() / 'clustering/models'

      obs_names = []
      for f in os.listdir(observables_dir):
          obs_names.append(f)

      print('\n'.join(obs_names))

      return obs_names



  def get_clustering_model(
          clf_id,
          gexp,
          ext='jl.pkl',
  ):

      model_path = data_path() / f'clustering/models/clfid-{clf_id}/gexp-{gexp}.{ext}'

      return load_obj(model_path)


  def get_clustering_traintest(
          clf_id,
          gexp,
          ext='jl.pkl',
  ):

      traintest_dir = data_path() / f'clustering/train_test/clfid-{clf_id}'
      traintest_path = traintest_dir / f'gexp-{gexp}.{ext}'

      # WARN: for some reason its always saving this as a tuple, so we
      # just strip it off
      return load_obj(traintest_path)[0]
#+END_SRC

**** CSN

***** Networks

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def ls_networks():
      """List the observables that are in the file system as a pickle
      file."""

      import os
      import os.path as osp

      observables_dir = osp.join(data_path(), 'csn/network')

      obs_names = []
      for model in os.listdir(observables_dir):
          model_names = []
          for lag_time in os.listdir(osp.join(observables_dir, model)):
              model_spec = "{}/{}".format(model, lag_time)
              obs_names.append(model_spec)

      print('\n'.join(obs_names))

      return obs_names



  def get_msn(
          csn_id,
          gexp,
          tag=None,
          ext='jl.pkl'
  ):

      directory = data_path() / f'csn/network/csnid-{csn_id}'
      path = directory / f'gexp-{gexp}_tag-{tag}.{ext}'

      # SNIPPET: I don't think I need this
      # # check the folder for the file you want to load depending on the
      # # ligand ID, we will use the extension to figure out how to load
      # # it
      # matches = []
      # for f in os.listdir(model_dir):
      #     if f.startswith(str(lig_id)):
      #         matches.append(f)

      # if len(matches) > 1:
      #     raise ValueError("Multiple matches")
      # elif len(matches) < 1:
      #     raise ValueError("No match")
      # else:
      #     fname = matches[0]

      # model_path = osp.join(model_dir, fname)

      return load_obj(path)

  def get_h5_msn(
          csn_id,
          gexp,
          tag=None,
          ext='jl.pkl'
  ):
      """Get a full MSN backed by the HDF5 so you have access to microstate data."""

      from wepy.analysis.network import MacroStateNetwork

      # get the base MSN
      base_msn = get_msn(
          csn_id,
          gexp,
          tag=tag,
          ext=ext,
      )

      # get the HDF5
      contigtree = get_contigtree(gexp)

      msn = MacroStateNetwork(
          contigtree,
          base_network=base_msn,
      )

      return msn
#+END_SRC

***** GEXF

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def load_gephi_gexf(path):
      """Load and clean a gexf file generated by gephi."""

      import io

      from networkx.readwrite.gexf import GEXFReader


      # load the gexf and then clean it, and use a temporary buffer for
      # it
      with open(path) as rf:
          gexf_buf = io.StringIO(clean_gephi_gexf(rf.read()))

      return GEXFReader(node_type=int)(gexf_buf)


  def load_gephi_graph(
          csn_id,
          gexp,
          tag=None,
          layout_id='main',
  ):
      """Load the networkx graph from the standard gephi generated gexf
      file."""

      directory = data_path() / f'csn/gexf/csnid-{csn_id}'
      model_path = directory / f'gexp-{gexp}_tag-{tag}_layout-{layout_id}.gexf'

      # NOTE: this was how I did it before, thats alright
      # fname = '{}_{}.gephi.gexf'.format(lig_id, name_pattern)

      if not osp.exists(model_path):
          raise OSError("Gexf does not exist")

      return load_gephi_gexf(model_path)

  # SNIPPET: this was doing too much so removing it
  # def get_gephi_network(
  #         msm_id,
  #         gexp,
  #         csn_id='main',
  #         layout_id='main',
  # ):
  #     """Load the gephi (CSN) network and combine with the network we
  #     produce, so that the data is combined (incorporating changes from
  #     gephi editing)

  #     """

  #     graph = load_gephi_graph(
  #         msm_id,
  #         gexp,
  #         csn_id=csn_id,
  #         layout_id=layout_id,
  #     )

  #     net = get_msn(
  #         msm_id,
  #         gexp,
  #         csn_id=csn_id,
  #     )

  #     # merge them
  #     return update_network_from_gephi(
  #         net,
  #         graph,
  #         layout_name='main',
  #     )

  def update_network_layout_from_gephi(
          net,
          graph,
          layout_name=None,
  ):

      node_values = {node : graph.nodes[node]['viz']
                     for node in graph.nodes}

      net.set_nodes_layout(layout_name, node_values)

      return net


  def update_network_from_gephi(
          net,
          graph,
          fields=None,
          layout_name=None,
  ):
      """This takes a gexf networkx object and updates the net/msn with
      the special data from this file.

      This is for if you want to add data from the gexf to the network.

      If fields is specified import only those fields.

      WARNING
      -------

      This will overwrite all data. If you just want the layout use the
      other function.

      """

      ignore_fields = ('label',)

      if fields is None:
          node = list(graph.nodes.keys())[0]
          fields = graph.nodes[node].keys()
          del node


      # assume all nodes have the same fields
      for field in fields:

          # don't update columns which are the same name, we assume they are
          # immutable and if there is new data it has a new name
          if (field in net.graph.nodes[node] or
              field in ignore_fields or
              (field == 'viz' and layout_name is None)
          ):
              continue

          # get all values dict for this attribute
          node_values = {node : graph.nodes[node][field]
                         for node in graph.nodes}


          # we treat the layout data differently, and save them in
          # layouts structure
          if field == 'viz':

              net.set_nodes_layout(layout_name, node_values)

          elif field.startswith('_groups'):

              # if it is a node grouping we also need to get the indices for
              # which the group was true
              group_nodes = [node_id for node_id, node_value in node_values.items()
                             if node_value is True]

              group_name = '/'.join(field.split('/')[1:])

              net.set_node_group(group_name, group_nodes)

          elif field.startswith('_observables'):

              observable_name = '/'.join(field.split('/')[1:])

              net.set_nodes_observable(observable_name, node_values)

          else:
              net.set_nodes_attribute(field, node_values)

      return net
#+END_SRC

***** Nodes and Edges tables

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def get_network_nodes(model_name, lig_id, lag_time):

      import os
      import os.path as osp

      model_dir = osp.join(data_path(),
                           'csn/nodes/{}/{}'.format(model_name, lag_time))

      # check the folder for the file you want to load depending on the
      # ligand ID, we will use the extension to figure out how to load
      # it
      matches = []
      for f in os.listdir(model_dir):
          if f.startswith(str(lig_id)):
              matches.append(f)

      if len(matches) > 1:
          raise ValueError("Multiple matches")
      elif len(matches) < 1:
          raise ValueError("No match")
      else:
          fname = matches[0]

      model_path = osp.join(model_dir, fname)

      return load_table(model_path)



  def get_network_edges(model_name, lig_id, lag_time):

      import os
      import os.path as osp

      model_dir = osp.join(data_path(),
                           'csn/edges/{}/{}'.format(model_name, lag_time))

      # check the folder for the file you want to load depending on the
      # ligand ID, we will use the extension to figure out how to load
      # it
      matches = []
      for f in os.listdir(model_dir):
          if f.startswith(str(lig_id)):
              matches.append(f)

      if len(matches) > 1:
          raise ValueError("Multiple matches")
      elif len(matches) < 1:
          raise ValueError("No match")
      else:
          fname = matches[0]

      model_path = osp.join(model_dir, fname)

      return load_table(model_path)
#+END_SRC

**** MSM

There is the MSM and the mapping for it, we just load both at once as they are tuples:

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def load_msm(
          msm_id,
          gexp,
          ext="jl.pkl",
  ):

      model_dir = data_path() / f'msm/models/msmid-{msm_id}'
      path = model_dir / f'gexp-{gexp}.{ext}'

      pyemma_msm, trimming_mapping = load_obj(path)

      return pyemma_msm, trimming_mapping
#+END_SRC

**** TS PCA

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def load_tspca_model(
          tspca_id,
          ext="jl.pkl",
  ):

      model_dir = data_path() / f'ts_pca/models'
      path = model_dir / f'tspca-id-{tspca_id}.{ext}'

      pca_model, model_score, mode_scores, test_size, gexps = load_obj(path)

      return pca_model, model_score, mode_scores, test_size, gexps
#+END_SRC


*** Saving Data

**** Misc. Models

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def save_model(
          model_name,
          gexp,
          model,
          overwrite=True,
          ext="jl.pkl",
  ):

      model_dir = data_path() / f'models/{model_name}'
      model_path = model_dir / '{gexp}.{ext}'

      os.makedirs(model_dir, exist_ok=True)

      save_obj(
          model_path,
          model,
          overwrite=overwrite,
          ext=ext,
      )

      return model_path
#+END_SRC


**** Observables

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_obs_path(
          observable_name,
          gexp,
          ext="jl.pkl",
  ):

      all_obs_dir = data_path() / 'observables'

      obs_dir = all_obs_dir / observable_name

      obs_path = obs_dir / f'gexp-{gexp}.{ext}'

      return obs_path

  def save_observable(
          observable_name,
          gexp,
          obs_data,
          overwrite=True,
          ext="jl.pkl",
  ):

      obs_path = get_obs_path(observable_name,
                              gexp,
                              ext=ext)

      save_obj(obs_path,
               obs_data,
               overwrite=overwrite,
               ext=ext)

      return obs_path


  def attach_observable(
          observable_name,
          gexp,
  ):
      """Save one of the saved observables from the filesystem to the
      corresponding WepyHDF5.
      """

      with get_gexp_wepy_h5(gexp, mode='r+') as wepy_h5:

          # get the observable (only from the filesystem), and add it to
          # the wepyHDF5

          wepy_h5.add_observable(
              observable_name,
              get_observable(
                  observable_name,
                  gexp,
                  source='fs',
              )
          )

  def save_external_observable(observable_name,
                               gexp,
                               overwrite=True,
                               ext="jl.pkl",
  ):

      """Save one of the observables from the HDF5 to the filesystem."""

      with get_gexp_wepy_h5(gexp) as wepy_h5:

          save_observable(
              observable_name,
              gexp,
              get_observable(
                  observable_name,
                  gexp,
                  source='h5',
              ),
              overwrite=overwrite,
              ext=ext,
          )

#+END_SRC



**** Clustering 

***** Models 

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_clustering_model(
          clf_id,
          gexp,
          model,
          overwrite=True,
          ext="jl.pkl",
  ):

      import os
      import os.path as osp

      model_dir = data_path() / f'clustering/models/clfid-{clf_id}'
      model_path = model_dir / f'gexp-{gexp}.{ext}'

      os.makedirs(model_dir, exist_ok=True)

      save_obj(
          model_path,
          model,
          overwrite=overwrite,
          ext=ext,
      )

      return model_path

  def save_clustering_traintest(
          clf_id,
          gexp,
          train_idxs,
          test_idxs,
          overwrite=True,
          ext='jl.pkl',
  ):

      traintest_dir = data_path() / f'clustering/train_test/clfid-{clf_id}'
      traintest_path = traintest_dir / f'gexp-{gexp}.{ext}'

      os.makedirs(traintest_dir, exist_ok=True)

      train_test_d = {
          'training_idxs' : train_idxs,
          'testing_idxs' : test_idxs,
      },


      save_obj(
          traintest_path,
          train_test_d,
          overwrite=overwrite,
          ext=ext,
      )

      return traintest_path

#+END_SRC

***** Cluster Centers trajectories

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def save_cluster_centers(
          msm_id,
          gexp,
          traj,
          overwrite=True,
          ext='dcd',
  ):

      traj_dir = data_path() / f'clustering/cluster_center_trajs/msm-{msm_id}'
      traj_path = traj_dir / f'gexp-{gexp}.{ext}'

      os.makedirs(traj_dir, exist_ok=True)

      traj.save(str(traj_path), force_overwrite=overwrite)

      return traj_path

#+END_SRC



**** Warping Lineages

This will set up the directory and do naming of the warping lineage
trajectory files.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def save_span_warp_lineages_dcds(gexp, span_id):
      """Save the warps as individual trajectory files named by the index of
      the warp event.

      WARNING: probably you will want to use `save_gexp_warp_lineages`
      so you can match that to the the table of warping.

      """

      lineages_dir_path = data_path() / f'warp-lineages_by-span/gexp-{gexp}/span-{span_id}_warp-lineages'

      lineages_dir_path.mkdir(exist_ok=True, parents=True)

      contig = get_gexp_span_contig(gexp, span_id)

      lineage_trajs_gen = get_warp_lineage_trajs(gexp, span_id)

      total_frames = 0
      for warp_idx, lineage_traj in enumerate(lineage_trajs_gen):

          n_frames = len(lineage_traj)
          total_frames += n_frames

          print(f"warp: {warp_idx} has {n_frames} frames")

          path = lineages_dir_path / f'warp-{warp_idx}.dcd'

          print("saving")
          lineage_traj.save_dcd(str(path))

      print(f"total frames: {total_frames}")


  def save_gexp_warp_lineages_dcds(gexp):
      """Save the warps as individual trajectory files named by the index of
      the warp event.

      WARNING: probably you will want to use `save_gexp_warp_lineages`
      so you can match that to the the table of warping.

      """

      lineages_dir_path = data_path() / f'warp-lineages_by-gexp/gexp-{gexp}'

      # /span-{span_id}_warp-lineages
      filename_tmpl = "{warp_idx}_span-{span_id}_{span_warp_idx}"

      lineages_dir_path.mkdir(exist_ok=True, parents=True)

      # collect all of the generators for each span
      lineages_gens = []
      for span_id in get_gexp_span_ids(gexp):

          contig = get_gexp_span_contig(gexp, span_id)

          lineage_trajs_gen = get_warp_lineage_trajs(gexp, span_id)

          lineages_gens.append(lineage_trajs_gen)

      # as we go along increment the warp index to keep track
      warp_idx = 0
      for span_id, lineage_gen in enumerate(lineages_gens):
          for span_warp_idx, lineage_traj in enumerate(lineage_gen):

              n_frames = len(lineage_traj)

              print(f"warp: {warp_idx} span: {span_id}:{span_warp_idx} has frames: {n_frames}")

              filename_root = filename_tmpl.format(
                  warp_idx=warp_idx,
                  span_id=span_id,
                  span_warp_idx=span_warp_idx
              )

              path = lineages_dir_path / f'{filename_root}.dcd'

              print("saving")
              lineage_traj.save_dcd(str(path))

              # increment the warp index
              warp_idx += 1



  # UGLY,SNIPPET: this did them all in one file which just doesn't work
  # unfortunately since its just too big

  # def save_warp_lineages_dcd(gexp, span_id):
  #     """Save all the warps to the same DCD concatenated together"""

  #     lineages_path = data_path() / f'warp-lineages/gexp-{gexp}/span-{span_id}_warp-lineages.dcd'

  #     lineages_path.parent.mkdir(exist_ok=True, parents=True)

  #     lineages_trace = it.chain(*get_warp_lineage_traces(gexp, span_id))

  #     print(f"total frames: {len(lineage_trace)}")

  #     contig = get_gexp_span_contig(gexp, span_id)

  #     print("Loading")
  #     with contig:
  #         traj = contig.wepy_h5.traj_fields_to_mdtraj(
  #             contig.wepy_h5.get_trace_fields(lineages_trace,
  #                                             ['positions', 'box_vectors']
  #             ))

  #     print("saving DCD")
  #     traj.save_dcd(str(lineages_path))

#+end_src


**** Final Walker Lineages

This will set up the directory and do naming of the warping lineage
trajectory files.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_gexp_final_lineages_dcds(gexp):
      """Save the final walkers as individual trajectory files.

      """

      lineages_dir_path = data_path() / f'final-walker-lineages_by-gexp/gexp-{gexp}'

      # /span-{span_id}_warp-lineages
      filename_tmpl = "span-{span_id}_{span_final_idx}"

      lineages_dir_path.mkdir(exist_ok=True, parents=True)

      # collect all of the generators for each span
      lineages_gens = []
      for span_id in get_gexp_span_ids(gexp):

          contig = get_gexp_span_contig(gexp, span_id)

          lineage_trajs_gen = get_final_lineage_trajs(gexp, span_id)

          lineages_gens.append(lineage_trajs_gen)

      for span_id, lineage_gen in enumerate(lineages_gens):
          for span_final_idx, lineage_traj in enumerate(lineage_gen):

              n_frames = len(lineage_traj)

              print(f"span: {span_id}:{span_final_idx} has frames: {n_frames}")

              filename_root = filename_tmpl.format(
                  span_id=span_id,
                  span_final_idx=span_final_idx
              )

              path = lineages_dir_path / f'{filename_root}.dcd'

              print("saving")
              lineage_traj.save_dcd(str(path))



#+end_src

**** High Progress Walkers


***** Just the Walkers

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_gexp_high_progress_walkers(
          gexp,
          top_n,
          progress_key='min_distances',
    ):
      """Save the highest progress walkers to trajectory files for:

      - each span
      - across each gexp

      This will get the highest 'top_n' from each span. The full gexp
      one will be all of those.

      """

      from wepy.util.util import concat_traj_fields
      from wepy.util.mdtraj import traj_fields_to_mdtraj

      dir_path = data_path() / f'high-progress_walkers/gexp-{gexp}'

      dir_path.mkdir(exist_ok=True, parents=True)

      suffix = f"top-{top_n}_prog-{progress_key}"

      span_filename_tmpl = f"span-{{span_id}}_{suffix}"

      gexp_filename = f"all_{suffix}"

      span_traj_fields = {}
      for span_id in get_gexp_span_ids(gexp):

          traj_fields = get_high_progress_traj_fields(
                      gexp,
                      span_id,
                      top_n,
                      progress_key,
                  )

          ## then save it to the mapping for writing them all out later
          span_traj_fields[span_id] = traj_fields

          # make a traj for this one to save individually

          with contig:
              span_traj = contig.wepy_h5.traj_fields_to_mdtraj(traj_fields)

          del traj_fields
          gc.collect()

          ## write the span traj
          span_filename = span_filename_tmpl.format(
              span_id=span_id,
          )
          span_path = dir_path / f"{span_filename}.dcd"

          print(f"saving for span {span_id}")

          span_traj.save_dcd(str(span_path))


          del span_traj
          gc.collect()

      contigtree = get_contigtree(gexp)

      with contigtree:
          # write out all of them
          all_traj = contigtree.wepy_h5.traj_fields_to_mdtraj(
              concat_traj_fields(
                  [span_traj_fields[span_id]
                   for span_id in  get_gexp_span_ids(gexp)]
              ))

      del span_traj_fields
      gc.collect()

      print(f"saving all traj")

      all_path = dir_path / f"{gexp_filename}.dcd"

      all_traj.save_dcd(str(all_path))
#+end_src


***** Lineages of the Walkers

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_gexp_high_progress_walkers_lineages_dcds(
          gexp,
          top_n,
          progress_key='min_distances',
  ):
      """Save lineages of the high progress walkers to dcds

      """

      lineages_dir_path = data_path() / f'high-progress_lineages_by-gexp/gexp-{gexp}'

      filename_tmpl = "span-{span_id}_{span_lineage_idx}"

      lineages_dir_path.mkdir(exist_ok=True, parents=True)

      # collect all of the generators for each span
      lineages_gens = []
      for span_id in get_gexp_span_ids(gexp):


          contig = get_gexp_span_contig(gexp, span_id)

          lineage_trajs_gen = get_high_progress_lineages_trajs(
              gexp,
              span_id,
              top_n,
              progress_key)

          lineages_gens.append(lineage_trajs_gen)

      for span_id, lineage_gen in enumerate(lineages_gens):
          for span_lineage_idx, lineage_traj in enumerate(lineage_gen):

              n_frames = len(lineage_traj)

              print(f"span: {span_id}:{span_lineage_idx} has frames: {n_frames}")

              filename_root = filename_tmpl.format(
                  span_id=span_id,
                  span_lineage_idx=span_lineage_idx
              )

              path = lineages_dir_path / f'{filename_root}.dcd'

              print("saving")
              lineage_traj.save_dcd(str(path))



#+end_src


**** Plots


#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_fig(
          group,
          name_root,
          fig,
          tags=None,
  ):
      """Save a figure of any kind.

      Useful for when all gexps are present or some other snowflake plot.

      See Also
      --------

      save_gexp_fig : helper to do automatic and consistent name
                      formatting for gexp specific plots

      """

      # if there are tags render to string, and add to the root name

      if tags is None:
          name_tagged_root = name_root
      else:
          tag_str = "_".join(
              [f'{key}-{val}' for key, val in tags.items()])

          name_tagged_root = f"{name_root}__{tag_str}"


      # make the path
      fig_stem = media_path() / group / name_tagged_root

      # ensure the directory exists for the group
      fig_stem.parent.mkdir(exist_ok=True, parents=True)

      # save a copy for each extension
      for ext in FIG_EXTENSIONS:

          fig_path = f"{fig_stem}.{ext}"

          print(f"PLOTTING: {fig_path}")

          fig.savefig(fig_path)

      return fig_stem


  def save_gexp_fig(gexp,
                    group,
                    fig,
                    tags=None):
      """Save a figure to media group

      group is a grouping of figures that are the same kind. For
      instance if you have plots of rates, you might call this 'rates'
      or 'fe/spans'.

      Each gexp will get it's own figure in this group in the different
      formats etc.

      If there are any tags, these will be added to the end of the
      filename stem as '{key}-{value}' separated by '_' characters.

      """

      name_root = f"gexp-{gexp}"

      return save_fig(
          group,
          name_root,
          fig,
          tags=tags,
      )
#+end_src

**** CSNs

***** TODO Gephi

Gephi projects.


***** Gexf

Original gexf files written from CSN. Used to initialize a graph with
layout in gephi. Should use the node and edge tables to update the
table and save as the gephi project file, until there is need for
other output.


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_csn_gexf(
              csn_id,
              gexp,
              msn,
              tag=None,
              layout_id='main',
              overwrite=True,
  ):

      directory = data_path() / f'csn/gexf/csnid-{csn_id}'
      path = directory / f'gexp-{gexp}_tag-{tag}_layout-{layout_id}.gexf'

      os.makedirs(directory, exist_ok=True)

      # don't short circuit to the None layout, this is confusing
      if layout_id is None or layout_id == "None":
          msn.write_gexf(path, layout=None)

      else:
          msn.write_gexf(path, layout=layout_id)

      return path


#+END_SRC

***** Networks

The pickles of the BaseMacroStateNetworks.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_csn(
              csn_id,
              gexp,
              msn,
              tag=None,
              overwrite=True,
              ext='jl.pkl'):

        directory = data_path() / f'csn/network/csnid-{csn_id}'
        path = directory / f'gexp-{gexp}_tag-{tag}.{ext}'

        os.makedirs(directory, exist_ok=True)

        save_obj(
              path,
              msn,
              overwrite=overwrite,
              ext=ext
        )

        return path


#+END_SRC

***** Nodes and Edges tables

CSV of the nodes that can be read by gephi to add new attributes to a
layout.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_csn_nodes(
              csn_id,
              gexp,
              df,
              tag=None,
              overwrite=True,
              ext='csv',
  ):

        directory = data_path() / f'csn/nodes/csnid-{csn_id}'
        path = directory / f'gexp-{gexp}_tag-{tag}.{ext}'

        os.makedirs(directory, exist_ok=True)

        save_table(
              path,
              df,
              overwrite=overwrite,
              ext=ext,
        )

        return path

  def save_csn_edges(
              csn_id,
              gexp,
              df,
              tag=None,
              overwrite=True,
              ext='csv',
  ):

        directory = data_path() / f'csn/edges/csnid-{csn_id}'
        path = directory / f'gexp-{gexp}_tag-{tag}.{ext}'

        os.makedirs(directory, exist_ok=True)

        save_table(
              path,
              df,
              overwrite=overwrite,
              ext=ext,
        )

        return path

  def save_csn_tables(
              csn_id,
              gexp,
              msn,
              tag=None,
              overwrite=True,
              ext='csv',
  ):

        nodes_df = msn.nodes_to_dataframe()
        edges_df = msn.edges_to_dataframe()

        save_csn_nodes(
              csn_id,
              gexp,
              nodes_df,
              tag=tag,
              overwrite=overwrite,
              ext=ext,
        )

        save_csn_edges(
              csn_id,
              gexp,
              edges_df,
              tag=tag,
              overwrite=overwrite,
              ext=ext,
        )
#+END_SRC



***** Save all

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def save_all_csn_stuff(
              csn_id,
              gexp,
              msn,
              tag=None,
              layout_id=Ellipsis,
              overwrite=True,
  ):
      """Save all output files for the relevant MSM.

      There is an additional 'csn_id' which can be used to save
      different CSNs with different attached data, so you don't have
      to overwrite other ones.

      For the 'tag' the layout_id if the value is 'None' then it will
      be saved to a default 'nil' file.

      If the values are Ellipsis then all of the available IDs will be
      saved.

      """


      # save the MSN as a pickle object, this will have all the data
      save_csn(
          csn_id,
          gexp,
          msn,
          tag=tag,
      )

      # save the data from the network as both an edge and a node
      # table CSVs
      save_csn_tables(
          csn_id,
          gexp,
          msn,
          tag=tag,
      )

      # save the gexf XML file with a layout for visualization
      save_csn_gexf(
          csn_id,
          gexp,
          msn,
          tag=tag,
          layout_id=layout_id,
      )
#+END_SRC

***** CSN state and group trajs

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def save_csn_group_traj(model_name, lig_id,
                          net, group_name,
                          downsample=None,
                          overwrite=True, ext='dcd'):

      import os
      import os.path as osp

      # transform the name to a file name
      grp_filename = group_name.replace('/', '-')

      traj_dir = osp.join(data_path(), 'csn/group_trajs/{}'.format(model_name))
      traj_path = osp.join(traj_dir, '{}_{}.{}'.format(lig_id, grp_filename, ext))

      os.makedirs(traj_dir, exist_ok=True)

      # make the traj, downsampling if requested
      traj = None
      for node_id in net.node_groups[group_name]:
          print("working on node {}".format(node_id))
          if traj is None:
              traj = get_cluster_micro_traj(model_name, lig_id, node_id,
                                            downsample=downsample)
          else:
              traj += get_cluster_micro_traj(model_name, lig_id, node_id,
                                             downsample=downsample)

          print("{} frames".format(traj.n_frames))

      print("saving traj")
      traj.save(traj_path, force_overwrite=overwrite)

      return traj_path

#+END_SRC
**** MSM

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_msm(
          msm_id,
          gexp,
          msm,
          trimming_mapping,
          overwrite=True,
          ext="jl.pkl",
  ):

      import os

      model_dir = data_path() / f'msm/models/msmid-{msm_id}'
      model_path = model_dir / f'gexp-{gexp}.{ext}'

      os.makedirs(model_dir, exist_ok=True)


      # save the MSM and the mapping together
      obj = (msm, trimming_mapping)

      save_obj(
          model_path,
          obj,
          overwrite=overwrite,
          ext=ext,
      )

      return model_path
#+END_SRC


**** TS PCA


***** TS PCA Models and Scores

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_tspca_model(
          tspca_id,
          pca_model,
          model_score,
          mode_scores,
          test_size,
          gexps,
          overwrite=True,
          ext="jl.pkl",
  ):

      model_dir = data_path() / f'ts_pca/models'
      model_path = model_dir / f'tspca-id-{tspca_id}.{ext}'

      os.makedirs(model_dir, exist_ok=True)

      # save the MSM and the mapping together
      obj = (pca_model, model_score, mode_scores, test_size, gexps,)

      save_obj(
          model_path,
          obj,
          overwrite=overwrite,
          ext=ext,
      )

      return model_path


  def save_tspca_score_table(
          score_df,
  ):

      table_path = data_path() / f'ts_pca/model_scores/scores.csv'

      save_table(
          table_path,
          score_df,
          overwrite=True,
      )

#+END_SRC

***** 3D Structures of COMs

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def save_com_trajs(
          ts_id,
          gexp,
          com_traj,
          pc_projections,
          overwrite=True,
  ):

      dir_path = data_path() / f'ts_pca/com_trajs/tsid-{ts_id}'

      os.makedirs(dir_path, exist_ok=True)

      # write these trajectories out. Make one for each PC and put these
      # as the bfactors so we can color them in a visualizer

      # ensure the directory
      os.makedirs(
          f'data/ts_pca/gexp-{gexp}/ts-{ts_id}',
          exist_ok=True,
      )

      for pc_idx, pc_proj in enumerate(pc_projections):

          fname = f'coms_pc-{pc_idx}.pdb'

          fpath = dir_path / fname

          com_traj.save_pdb(
              str(fpath),
              bfactors=pc_proj
          )


      return dir_path
#+END_SRC


*** Jigs

**** Single atom topology

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def single_atom_json_rec(atom_idx):
      atom_rec = {'index' : atom_idx,
                  'name' : 'H',
                  'element' : 'He'}

      return atom_rec

  def n_atom_top(n_atoms):

      import json

      n_atom_top_d = {'bonds' : [],
                           'chains' : [
                               {'index' : 0,
                                'residues' : [
                                    {'index' : 0,
                                     'name' : 'DUM',
                                     'resSeq' : 0,
                                     'segmentID' : 'DUMA',
                                     'atoms' :
                                     [single_atom_json_rec(i) for i in range(n_atoms)]}
                                ]}
                           ]}
      n_atom_top = json.dumps(n_atom_top_d)

      return n_atom_top

  def n_atom_mdj_top(n_atoms):

      from wepy.util.mdtraj import json_to_mdtraj_topology

      return json_to_mdtraj_topology(n_atom_top(n_atoms))


  def save_n_atom_top_pdb(n_atoms):

        import os
        import os.path as osp

        import numpy as np

        import mdtraj as mdj

        top_dir = osp.join(data_path(), 'top/util')

        # make sure the directory exists
        os.makedirs(top_dir,
                    exist_ok=True)

        pdb_path = osp.join(top_dir, "{}_atom_ref.pdb".format(n_atoms))

        ref_traj = mdj.Trajectory(np.zeros((1,1,3)),
                                  n_atom_mdj_top(n_atoms))

        ref_traj.save_pdb(pdb_path)
#+END_SRC

*** Common Functions

**** Unit Conversion

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def convert_rate_to_rt(rate_q, time_base=tkunit.minute):

      return (np.float64(1.0) / rate_q.value_in_unit(rate_q.unit)) * (1/rate_q.unit).in_units_of(time_base)


  def convert_rt_to_rate(rt_q, time_base=tkunit.second):

      time_unit = (1/time_base).unit

      rate = (1 / rt_q).in_units_of(time_unit)

      return rate

#+end_src

**** Plotting

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  ### Plotting Functions

  def bin_centers_from_edges(bin_edges):
      return np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                              for i in range(bin_edges.shape[0] - 1)])


  def move_axes(ax, fig, subplot_spec=111):
      """Move an Axes object from a figure to a new pyplot managed Figure in
      the specified subplot."""

      # get a reference to the old figure context so we can release it
      old_fig = ax.figure

      # remove the Axes from it's original Figure context
      ax.remove()

      # set the pointer from the Axes to the new figure
      ax.figure = fig

      # add the Axes to the registry of axes for the figure
      fig.axes.append(ax)
      # twice, I don't know why...
      fig.add_axes(ax)

      # then to actually show the Axes in the new figure we have to make
      # a subplot with the positions etc for the Axes to go, so make a
      # subplot which will have a dummy Axes
      dummy_ax = fig.add_subplot(subplot_spec)

      # then copy the relevant data from the dummy to the ax
      ax.set_position(dummy_ax.get_position())

      # then remove the dummy
      dummy_ax.remove()

      # close the figure the original axis was bound to
      plt.close(old_fig)


#+END_SRC


**** Recenter and Superimpose Binding Site

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py


  def recenter_superimpose_traj(traj_fields, lig_id, rep_key):
      """Recenter the coordinates based on the ligand id and the
      representation key. Fields must only be 'positions' and
      'box_vectors'.

      """

      import numpy as np
      from wepy.util.util import traj_box_vectors_to_lengths_angles

      from geomm.superimpose import superimpose
      from geomm.grouping import group_pair
      from geomm.centering import center_around

      sel_idxs = lig_selection_idxs(lig_id)

      lig_idxs = sel_idxs['{}/ligand'.format(rep_key)]
      prot_idxs = sel_idxs['{}/protein'.format(rep_key)]
      bs_idxs = sel_idxs['{}/binding_site'.format(rep_key)]

      centered_traj_fields = get_centered_ref_state_traj_fields(lig_id, rep_key=rep_key)

      centered_ref_positions = centered_traj_fields['positions']

      box_lengths, _ = traj_box_vectors_to_lengths_angles(traj_fields['box_vectors'])

      ## regroup, center, and superimpose the frames

      # group the pair of ligand and binding site together in the same image
      grouped_positions = [group_pair(positions, box_lengths[idx],
                                          bs_idxs, lig_idxs)
                    for idx, positions in enumerate(traj_fields['positions'])]

      # center all the positions around the binding site
      centered_positions = [center_around(positions, bs_idxs)
                            for idx, positions in enumerate(grouped_positions)]

      # then superimpose the binding sites
      sup_positions = [superimpose(centered_ref_positions, pos, idxs=bs_idxs)[0]
                       for pos in centered_positions]

      return np.array(sup_positions), centered_ref_positions
#+END_SRC


**** GEXF

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def clean_gephi_gexf(gexf):

      from xmltodict import parse, unparse

      # parse the bad gexf file
      bad_gexf = parse(gexf)

      # the good tag we take things from
      good_gexf_tag = """
      <gexf version="1.2"
            xmlns:viz="http://www.gexf.net/1.2draft/viz"
            xmlns="http://www.gexf.net/1.2draft"
            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
            xsi:schemaLocation="http://www.w3.org/2001/XMLSchema-instance">
      </gexf>
      """

      # parse the good tag and get the values
      tag_attrs = {}
      for key, value in parse(good_gexf_tag)['gexf'].items():

          if not key.startswith('@'):
              continue
          else:
              tag_attrs[key] = value

      # read the bad xml and parse it

      # then replace the tag attributes for this with the good ones
      for key, value in tag_attrs.items():
          bad_gexf['gexf'][key] = value

      # then get the good xml string
      return unparse(bad_gexf)

#+END_SRC

**** Observable Munging

***** Observable Shape

The shape of the observables for each HDF5 results file.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_obs_shape(gexp):

      with get_gexp_wepy_h5(gexp) as wepy_h5:

          runs = []
          for run_idx in range(wepy_h5.num_runs):
              traj_lengths = []
              for traj_idx in range(wepy_h5.num_run_trajs(run_idx)):
                  traj_lengths.append(wepy_h5.num_traj_frames(run_idx, traj_idx))

              runs.append(traj_lengths)

      return runs

  def get_obs_traj_shape(gexp):

      with get_gexp_wepy_h5(gexp) as wepy_h5:

          traj_lengths = []
          for run_idx in range(wepy_h5.num_runs):
              for traj_idx in range(wepy_h5.num_run_trajs(run_idx)):
                  traj_lengths.append(wepy_h5.num_traj_frames(run_idx, traj_idx))

      return traj_lengths
#+END_SRC

***** Reshaping fields by run, traj, and flat features

For doing machine learning things we use a nominally unordered
collection of features. This is just a concatenation of the list of
observable arrays for each trajectory however, and we exploit this to
recover the identity of each label from this. So to reshape back into
an observable list of arrays just use this function.

Currently very confused about observable shapes. There are 3 of them:

- run-traj :: (n_runs, n_trajs, n_frames, *feature_shape)
- traj :: (n_runs x n_trajs, n_frames, *feature_shape)
- features :: (n_runs x n_trajs x n_frames, *feature_shape)  

The (sort of) natural way to get them from the HDF5 is traj form. I
started saving them in run-traj form.

Rename run-traj to just 'run'. I was calling it 'obs' before which is
too vague and I keep forgetting.

- reshape_*
  - [ ] run_to_traj
  - [X] run_to_features
  - [ ] traj_to_run
  - [X] traj_to_features
  - [X] features_to_run
  - [X] features_to_traj


|----------+-----------------+------------------+------------------|
|          | run             | traj             | features         |
|----------+-----------------+------------------+------------------|
| run      | None            | run_to_traj      | run_to_features  |
| traj     | traj_to_run     | None             | traj_to_features |
| features | features_to_run | features_to_traj | None             |
|----------+-----------------+------------------+------------------|


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def are_features_scalar(features):

      return np.isscalar(features[0])

  def scalarize_features(features):

      # if they are already scalars don't do anything
      if are_features_scalar(features):
          return features

      # first check that the features can be scalar
      feature_shape = len(features[0])
      assert features_shape == 1, f"Feature is length {features_shape} cannot be scalarized"

      n_features = len(features)

      return np.reshape(
          np.array(features),
          (n_features,)
      )

  def unscalarize_features(features):

      # if its already not a scalar don't do anything
      if not are_features_scalar(features):
          return features

      n_features = len(features)

      return np.reshape(
          np.array(features),
          (n_features, 1)
      )

  # TODO
  def reshape_run_to_traj(run_obs, gexp):
      raise NotImplemented

  def reshape_run_to_features(run_obs):

      # just a double concatenate

          return np.concatenate(
                   [np.concatenate(
                       [traj for traj in run])
                    for run in run_obs]
          )

  # TODO
  def reshape_traj_to_run(traj_obs, gexp):
      raise NotImplemented

  def reshape_traj_to_features(traj_obs):

      # just a flattening
      return np.concatenate(traj_obs)

  def reshape_features_to_run(features,
                              gexp,
                              run_idxs=None,
  ):

      # this is the "run" shape for all of the runs
      obs_shape = get_obs_shape(gexp)

      # if the run_idxs are given assume the features are for these runs
      # only.

      # otherwise we generate the run_idxs from the whole contigtree
      # structure
      if run_idxs is None:
          run_idxs = list(range(len(obs_shape)))

      start_idx = 0

      obs_runs = []
      for run_idx in run_idxs:

          # for the run_idx get the sub-shape structure for the run
          run = obs_shape[run_idx]

          obs_trajs = []
          for num_traj_frames in run:

              obs_trajs.append(features[start_idx : start_idx + num_traj_frames])

              start_idx += num_traj_frames

          obs_runs.append(obs_trajs)

      return obs_runs

  def reshape_features_to_traj(features, gexp):

      traj_lengths = get_obs_traj_shape(gexp)

      start_idx = 0

      obs_trajs = []
      for num_frames in traj_lengths:

          obs_trajs.append(features[start_idx : start_idx + num_frames])

          start_idx += num_frames

      return obs_trajs
#+END_SRC


***** Misc. Helpers


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  # TODO: move this to the section with other run-> feature conversions
  def feature_idx_to_trace_idx(
          feature_idx,
          gexp,
  ):
      """For a feature major structure get the trace index (run, traj,
      cycle) from the feature index for the splitting/shuffling used in
      that classifier.

      """

      run_feature_idxs = make_feature_idx_observable(gexp)

      count = 0
      for run_idx, run in enumerate(run_feature_idxs):
          for traj_idx, traj in enumerate(run):
              if feature_idx in traj:
                  frame_idx = np.searchsorted(traj, feature_idx)

                  return (run_idx, traj_idx, frame_idx)

  def make_feature_idx_observable(gexp):

      run_obs = []
      start_idx = 0
      for run in get_obs_shape(gexp):

          traj_obs = []
          for num_frames in run:

              traj_obs.append(list(range(start_idx, start_idx + num_frames)))

              start_idx += num_frames

          run_obs.append(traj_obs)

      return run_obs
#+END_SRC

*** Structural Informatics

This is a bunch of special information that relates to the structures
themselves. Indices of things, mappings betwen them. Special
selections thereof etc.

**** Preamble

This is just a few convenience things used to interface with stuff

#+begin_src  python :tangle src/seh_pathway_hopping/_tasks.py
  #  translation of my names to the mdtraj names

  MDJ_COL_NAMES_TRANSLATION = {
      'resname' : 'resName',
      'resid' : 'resSeq',
      'pdb_serial' : 'serial',
      'segid' : 'segmentID',
      'chain' : 'chainID',
  }


  BACKBONE_ATOM_NAMES = (
      'C',
      'CA',
      'N',
      'O',
  )

#+end_src

**** Reference Structures & ROIs

These are the starting point reference structure for the receptor (as
well as the ligand that we have a crystal structure for).

ROIs means Regions of Interest

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  ### Names & Path Templates For Common Structures

  # ROIs means Regions of Interest

  # The reference structures are standard across all ligands and
  # non-specific
  REFERENCE_STRUCTURES = {

      # this is the raw structure from the RCSB crystal
      # structure PDB
      'raw_crystal' : {
          'pdb_path' : "data/seh/rcsb/4od0.pdb",
          'fasta_path' : "data/seh/rcsb/4od0.fasta.txt",
      },

      # this is the structure after some minor things were fixed with
      # PDBFixer, it includes the whole protein and the crystallographic
      # waters for the 'active_domain' ROI. This is used for assembling
      # the final complex for simulation. It is not used for docking.
      'fixed_crystallographic_waters' : {
          'pdb_path' : "data/seh/pdbfixer/4od0_prot_DOI_water.pdbfixer.pdb"
      },


      # this is just the protein structure after being fixed with
      # PDBFixer. This is the target for docking. It includes both
      # domains.
      'fixed_protein' : {
          'pdb_path' : "data/seh/pdbfixer/4od0_protein.pdbfixer.pdb"
      },

      ## This is the reference structure to use for alignments of other
      ## things.

      # this is the same as 'fixed_protein' but it also has the ligand
      'fixed_complex' : {
          'pdb_path' : "data/seh/pdbfixer/4od0_simplified.pdbfixer.pdb"
      },

  }


  # ways to select the different ROIs, if the field is there it can be
  # used to get it uniquely, if its not it you can't. The access methods
  # are ordered such that the first is the default.

  REFERENCE_ROIS = {
      'raw_crystal' : {

          'ligand' : (
              ('resname', '2RV'),
              ('resid', (603,)),
              ('pdb_serial', (4336, 4360)),
          ),
          'protein' : (
              ('chain', 0),
              ('pdb_serial', (1, 4328)),
              ('idx', (0, 4327)),
          ),
          'active_domain' : (
              ('resid', (231, 547,)), # this is inclusive of the second index
          ),
          'crystallographic_waters' : (
              ('resname', 'HOH'),
              ('pdb_serial_range', (4361, 4376)),
          ),
          'phosphate' : (
              ('resname', 'PO4'),
          ),
          'magnesium' : (
              ('resname', 'MG'),
          ),
      },

      'fixed_crystallographic_waters' : {

          'ligand' : None,
          'protein' : (
              ('chain', 0),
              ('pdb_serial', (1, 4333)),
              ('idx', (0, 4327)),
          ),
          'active_domain' : (
              ('resid', (231, 547,)), # this is inclusive of the second index
          ),
          'crystallographic_waters' : (
              ('resname', 'HOH'),
              ('pdb_serial_range', (4334, 4343)),
          ),
          'phosphate' : None,
          'magnesium' : None,
      },

      'fixed_protein' : {

          'ligand' : None,
          'protein' : (
              ('chain', 0),
              ('pdb_serial', (1, 4333)),
              ('idx', (0, 4327)),
          ),
          'active_domain' : (
              ('resid', (231, 547,)), # this is inclusive of the second index
          ),
          'crystallographic_waters' : None,
          'phosphate' : None,
          'magnesium' : None,
      },

      'fixed_complex' : {

          'ligand' : (
              ('resname', '2RV',),
          ),
          'protein' : (
              ('chain', 0),
              ('pdb_serial', (1, 4333)),
              ('idx', (0, 4327)),
          ),
          'active_domain' : (
              ('resid', (231, 547,)), # this is inclusive of the second index
          ),
          'crystallographic_waters' : (
              ('resname', 'HOH'),
          ),
          'phosphate' : (
              ('resname', 'PO4'),
          ),
          'magnesium' : (
              ('resname', 'MG'),
          ),
      },

  }


  ## raw loaders for reference structures
  def load_ref_structure_pdb(ref_id):
      """Load the raw PDB for the given ref_id"""

      pdb_path = REFERENCE_STRUCTURES[ref_id]['pdb_path']
      traj = mdj.load_pdb(pdb_path)

      return traj


  def get_ref_structure_mdj_top(ref_id):
      return load_ref_structure_pdb(ref_id).top


  def get_ref_structure_top_df(ref_id):
      return get_ref_structure_mdj_top(ref_id).to_dataframe()[0]

  def get_ref_structure_json_top(ref_id):

      from wepy.util.mdtraj import mdtraj_to_json_topology

      return mdtraj_to_json_topology(
          load_ref_structure_pdb(ref_id).top
      )



  ## methods to get subsets of a reference structure

  def ref_selection_idxs(
          ref_id,
          roi,
  ):
      """Return the idxs of the ROI in the given reference structure"""


      top_df = get_ref_structure_top_df(ref_id)


      # get the key and value for selecting the ROI, use the first one
      # as a default

      key, val = REFERENCE_ROIS[ref_id][roi][0]

      # convert to the mdtraj key
      mdj_key = MDJ_COL_NAMES_TRANSLATION[key]

      # these are both single selections
      if key in ('resname', 'chain', 'segid',):

          idxs = top_df[top_df[mdj_key] == val].index.values

      # these are ranges
      elif key in ('resid', 'pdb_serial', 'idx',):

          start, stop = val

          idxs = top_df[
              (top_df[mdj_key] >= start) &
               (top_df[mdj_key] <= stop)
          ].index.values

      else:
          raise ValueError("Don't know how to handle the ROI access method")

      return idxs

  def ref_selection_top(
          ref_id,
          roi
  ):

      from wepy.util.json_top import json_top_subset

      # get the indices to use for the subset
      sel_idxs = ref_selection_idxs(
          ref_id,
          roi,
      )


      # get the full JSON topology
      all_json_top = get_ref_structure_json_top(ref_id)

      sel_top = json_top_subset(
          all_json_top,
          sel_idxs,
      )

      return sel_top

  def load_ref_structure_traj(
          ref_id,
          roi=Ellipsis,
  ):

      from wepy.util.mdtraj import json_to_mdtraj_topology

      # load the full raw structure
      traj = load_ref_structure_pdb(ref_id)

      # if it is Ellipsis just return the whole thing
      if roi is Ellipsis:
          return traj

      # get the subset of the topology
      sel_top = ref_selection_top(ref_id, roi)

      # get the selection idxs
      sel_idxs = ref_selection_idxs(ref_id, roi)

      # make an mtraj top out of this
      mdj_sel_top = json_to_mdtraj_topology(sel_top)


      # slice the coordinates
      sel_coords = traj.xyz[:, sel_idxs, :]

      # NOTE: we are ignoring the box here since this only becomes
      # important after we solvate which doesn't effect the reference
      # structures
      sel_traj = mdj.Trajectory(
          sel_coords,
          mdj_sel_top,
      )

      return sel_traj

  def load_ref_structure_traj_fields(
          ref_id,
          roi=Ellipsis,
  ):

      traj = load_ref_structure_traj(
          ref_id,
          roi=roi,
      )

      return {
          'positions' : traj.xyz,
      }
#+end_src

**** Ligand-Complex Structures

These structures are specific to ligands, i.e. they actually have the
different ligands in them.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  # the complex structures are specific to each ligand. They names need
  # string formatting to get a real path.

  COMPLEX_STRUCTURES = {
      # this is the protein with the ligand and crystallographic waters
      # from the 'fixed_crystallographic_waters' model. It includs both domains.
      'docked_complex' : {
          'pdb_path' : "data/docking/assemblies/{lig_id}_4od0_docked.pdb"
      },

      # this is the result of solvating and truncating the
      # 'docked_complex' using the CHARMM-GUI servers. This includes the
      # 'active_domain', ligand, solvent ions, water solvent, as well as
      # being a fully hydrogenated model, unlike the other ones.
      #
      # This is the first structure that uses SEGIDs. It uses different
      # SEGIDs for the crystallographic waters over the solvation ones
      # which is nice.
      'chgui_solvated_complex' : {
          'pdb_path' : "data/charmm-gui_solvated_assemblies/{lig_id}/step2_solvator.pdb"
      },

      # This is a processed version of the 'chgui_solvated_complex'. It
      # doesn't change anything from CHARMM-GUI except some minor things
      # it got wrong for making the forcefields. It also adds complete
      # connectivity records.
      #
      # This is the model that is actually used for simulations and will
      # be minimized and equilibrated. The topology here is made into
      # the standard JSON topology that is used throughout. It also has
      # an detailed CSV file for the atoms that details their
      # relationship to the atom types in the force field etc.
      #
      # The primary changes are simply to normalize the structure so
      # that the indices etc. are more coherent for the actual
      # simulation model, rather than the entire protein itself. It also
      # renames the SOD ions as their own SEGID, rather than SOLV which
      # is reserved for the waters. It also reorders it so all the SOLV
      # waters are at the bottom.
      'fixed_complex' : {
          'pdb_path' : "data/ff_top/{lig_id}/sEH_lig-{lig_id}_system.FIXED.pdb"
      },

      # this is the 'fixed_complex' structure that has been minimized &
      # equilibrated.
      #
      # This also has positions in the state pickle which is used for
      # generating the initial simulation snapshots
      'equilibrated_complex' : {
          'pdb_path' : "data/equilibration/{lig_id}/sEH_lig-{lig_id}_equilibrated.pdb"
      },

  }

  COMPLEX_ROIS = {
      'docked_complex' : {

          'ligand' : (
              ('resname', 'UNL'),
          ),
          'protein' : (
              ('chain', 0),
          ),
          'active_domain' : (
              ('resid', (231, 547,)), # this is inclusive of the second index
          ),
          'crystallographic_waters' : (
              ('resname', 'HOH'),
          ),
      },

      'chgui_solvated_complex' : {

          'ligand' : (
              ('segid', 'HETA'),
          ),
          'protein' : (
              ('segid', 'PROA'),
          ),

          'crystallographic_waters' : (
              ('segid', 'WATA'),
          ),

          'sodium' : (
              ('segid', 'SOD'),
          ),
          'solvent_waters' : (
              ('segid', 'SOLV'),
          ),

      },

      'fixed_complex' : {

          'ligand' : (
              ('segid', 'HETA'),
          ),
          'protein' : (
              ('segid', 'PROA'),
          ),

          'crystallographic_waters' : (
              ('segid', 'WATA'),
          ),

          'sodium' : (
              ('segid', 'SOD'),
          ),
          'solvent_waters' : (
              ('segid', 'SOLV'),
          ),
      },

      'equilibrated_complex' : {

          'ligand' : (
              ('segid', 'HETA'),
          ),
          'protein' : (
              ('segid', 'PROA'),
          ),

          'crystallographic_waters' : (
              ('segid', 'WATA'),
          ),

          'sodium' : (
              ('segid', 'SOD'),
          ),
          'solvent_waters' : (
              ('segid', 'SOLV'),
          ),

      },

  }

  ## raw loaders for reference structures
  def load_complex_structure_pdb(
          complex_id,
          lig_id,
  ):
      """Load the raw PDB for the given complex_id"""

      pdb_path = COMPLEX_STRUCTURES[complex_id]['pdb_path'].format(
          lig_id=lig_id
      )
      traj = mdj.load_pdb(pdb_path)

      return traj


  def get_complex_structure_mdj_top(
          complex_id,
          lig_id,
  ):
      return load_complex_structure_pdb(complex_id, lig_id).top


  def get_complex_structure_top_df(
          complex_id,
          lig_id,
  ):
      return get_complex_structure_mdj_top(complex_id, lig_id).to_dataframe()[0]

  def get_complex_structure_json_top(
          complex_id,
          lig_id,
  ):

      from wepy.util.mdtraj import mdtraj_to_json_topology

      return mdtraj_to_json_topology(
          load_complex_structure_pdb(complex_id, lig_id).top
      )

  ## methods to get subsets of a reference structure

  def complex_selection_idxs(
          complex_id,
          lig_id,
          roi,
  ):
      """Return the idxs of the ROI in the given reference structure"""


      top_df = get_complex_structure_top_df(complex_id, lig_id)


      # get the key and value for selecting the ROI, use the first one
      # as a default

      key, val = COMPLEX_ROIS[complex_id][roi][0]

      # convert to the mdtraj key
      mdj_key = MDJ_COL_NAMES_TRANSLATION[key]

      # these are both single selections
      if key in ('resname', 'chain', 'segid',):

          idxs = top_df[top_df[mdj_key] == val].index.values

      # these are ranges
      elif key in ('resid', 'pdb_serial', 'idx',):

          start, stop = val

          idxs = top_df[
              (top_df[mdj_key] >= start) &
               (top_df[mdj_key] <= stop)
          ].index.values

      else:
          raise ValueError("Don't know how to handle the ROI access method")

      return idxs

  def complex_selection_top(
          complex_id,
          lig_id,
          roi
  ):

      from wepy.util.json_top import json_top_subset

      # get the indices to use for the subset
      sel_idxs = complex_selection_idxs(
          complex_id,
          lig_id,
          roi,
      )


      # get the full JSON topology
      all_json_top = get_complex_structure_json_top(complex_id, lig_id)

      sel_top = json_top_subset(
          all_json_top,
          sel_idxs,
      )

      return sel_top

  def load_complex_structure_traj(
          complex_id,
          lig_id,
          roi=Ellipsis,
  ):

      from wepy.util.mdtraj import json_to_mdtraj_topology

      # load the full raw structure
      traj = load_complex_structure_pdb(complex_id, lig_id)

      # if it is Ellipsis just return the whole thing
      if roi is Ellipsis:
          return traj

      # get the subset of the topology
      sel_top = complex_selection_top(complex_id, lig_id, roi)

      # get the selection idxs
      sel_idxs = complex_selection_idxs(complex_id, lig_id, roi)

      # make an mtraj top out of this
      mdj_sel_top = json_to_mdtraj_topology(sel_top)


      # slice the coordinates
      sel_coords = traj.xyz[:, sel_idxs, :]

      # NOTE: we are ignoring the box here since this only becomes
      # important after we solvate which doesn't effect the complexerence
      # structures
      sel_traj = mdj.Trajectory(
          sel_coords,
          mdj_sel_top,
          unitcell_lengths=traj.unitcell_lengths,
          unitcell_angles=traj.unitcell_angles,
      )

      return sel_traj

  def load_complex_structure_traj_fields(
          complex_id,
          lig_id,
          roi=Ellipsis,
  ):

      traj = load_complex_structure_traj(
          complex_id,
          lig_id,
          roi=roi,
      )

      return {
          'positions' : traj.xyz,
      }

#+end_src



**** COMMENT Alignment to Reference Structures

Leaving this out, not needed.

These are functions and data for aligning the reference structures to
each other, and other things to the reference structures.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  # this is the reference template for everything else to be aligned to
  TEMPLATE_REFERENCE_STRUCTURE = 'fixed_complex'

  def get_ref_template_bs_idxs(prot_roi='protein'):
      """Get the indices of the binding site for the reference template.

      They are returned relative to the protein.
      """

      from seh_prep.parameters import BINDING_SITE_CUTOFF

      cutoff = BINDING_SITE_CUTOFF.value_in_unit(tkunit.nanometer)

      # get the trajs for both the protein and ligand
      prot_traj = load_ref_structure_traj(TEMPLATE_REFERENCE_STRUCTURE, roi=prot_roi)
      lig_traj = load_ref_structure_traj(TEMPLATE_REFERENCE_STRUCTURE, roi='ligand')

      # then stack them into the same trajectory
      complex_traj = lig_traj.stack(prot_traj)

      lig_idxs = list(range(lig_traj.n_atoms))
      prot_idxs = list(range(lig_traj.n_atoms, lig_traj.n_atoms + prot_traj.n_atoms))

      # we just manually compute this since my other implementation
      # relies on boundary conditions

      # selects protein atoms which have less than 8 A from ligand
      # atoms in the crystal structure
      neighbors_idxs = mdj.compute_neighbors(
          complex_traj,
          cutoff,
          lig_idxs,
      )

      # selects protein atoms from neighbors list
      bs_idxs = np.intersect1d(neighbors_idxs, prot_idxs)

      # get the bs_idxs in terms of the protein only
      bs_prot_idxs = bs_idxs - lig_traj.n_atoms

      return bs_prot_idxs

  # NOTE: we don't need to do this because they are all aligned
  # def superimpose_ref_to_ref(ref_id):
  #     pass


  def dehydrogenate_traj(traj):
      """Remove hydrogens from a structure."""

      from wepy.util.mdtraj import (
          json_to_mdtraj_topology,
          mdtraj_to_json_topology,
      )
      from wepy.util.json_top import (
          json_top_atom_df,
          json_top_subset,
      )


      json_top = mdtraj_to_json_topology(traj.top)

      top_atoms_df = json_top_atom_df(json_top)

      # get the atom indices that are not hydrogen elements
      noH_idxs = top_atoms_df[top_atoms_df['element'] != 'H']['index'].values

      # then get a subset of the json_top
      stripped_json_top = json_top_subset(json_top, noH_idxs)

      # convert back to a mdj_top
      stripped_top = json_to_mdtraj_topology(stripped_json_top)

      # slice out the idxs from the old traj and make a new one
      stripped_traj = mdj.Trajectory(
          traj.xyz[:, noH_idxs, :],
          stripped_top,
          unitcell_angles=traj.unitcell_angles,
          unitcell_lengths=traj.unitcell_lengths,
      )

      return stripped_traj

  # TODO: something about this isn't working right, It may be due to
  # reordering between the original structures and those outputted from
  # CHARMM-GUI. Abandoning this.
  def superimpose_complex_to_ref(
          complex_id,
          lig_id,
          roi=Ellipsis,
  ):
      """
      """

      from geomm.superimpose import superimpose
      from geomm.centroid import centroid

      # get the reference traj and positions, only use the active domain
      ref_traj = load_ref_structure_traj(
          TEMPLATE_REFERENCE_STRUCTURE,
          roi='active_domain',
      )

      ref_positions = ref_traj.xyz[0,:,:]

      # get the idxs of atoms to use for the alignment.

      # These are the binding site idxs of the reference template

      # the bs indices are relative to the whole protein in the ref
      # template, so we need to get them in terms of the active_domain
      ref_template_bs_idxs = get_ref_template_bs_idxs(prot_roi='active_domain')

      # get the centroid of the ref templates so we can position the
      # mobile unit properly
      ref_centroid = centroid(ref_positions[ref_template_bs_idxs])


      # load the chosen roi of the complex
      complex_traj = load_complex_structure_traj(
          complex_id,
          lig_id,
          roi=roi,
      )

      # get just the protein of your complex
      complex_prot_traj = load_complex_structure_traj(
          complex_id,
          lig_id,
          roi="protein",
      )

      # the protein of the complexes will need dehydrogenated (other
      # than 'docked_complex')
      if complex_id == 'docked_complex':
          complex_noH_prot_traj = complex_prot_traj
      else:
          complex_noH_prot_traj = dehydrogenate_traj(complex_prot_traj)

      # superimpose the matched structures, and keep the rotation matrix
      # only
      rot_mats = []
      for target_pos in complex_noH_prot_traj.xyz:

          rot_mat = superimpose(
              ref_positions,
              target_pos,
              idxs=ref_template_bs_idxs,
          )[1]

          rot_mats.append(rot_mat)

      # apply the rotation matrices to the whole complex trajectory
      sup_frames = []
      for i, frame_coords in enumerate(complex_traj.xyz):

          rot_coords = np.dot(frame_coords, rot_mats[i])

          # translate the rotated coordinates to the centroid
          sup_coords = rot_coords + ref_centroid

          sup_frames.append(sup_coords)

      # copy the traj and replace the coords with the superimposed coords
      sup_traj = copy(complex_traj)
      sup_traj.xyz = np.array(sup_frames)


      return sup_traj
#+end_src

**** Selection Idxs and Tops

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  # just for reference all the keys we want to fill in
  SELECTION_KEYS = ('all_atoms/ligand',
                    'all_atoms/ligand/homology',
                    'all_atoms/protein',
                    'all_atoms/binding_site',
                    'all_atoms/main_rep',
                    'all_atoms/main_rep/ligand',
                    'all_atoms/main_rep/ligand/homology',
                    'all_atoms/main_rep/protein',
                    'all_atoms/main_rep/binding_site',
                    'all_atoms/correct_rep',
                    'all_atoms/correct_rep/ligand',
                    'all_atoms/correct_rep/ligand/homology',

                    'all_atoms/image',

                    'main_rep/ligand',
                    'main_rep/ligand/homology',
                    'main_rep/protein',
                    'main_rep/binding_site',

                    'ligand/homology',

                    'image/ligand',
                    'image/binding_site',
                    'image/protein',

                    'correct_rep/ligand',
                    'correct_rep/ligand/homology',
                    'correct_rep/protein',
                    'correct_rep/binding_site',
                    'correct_rep/missing',
                    'correct_rep/main_rep',

                    # and for the legacy stuff
                    'real_rep',
                    'real_rep/ligand',
                    'real_rep/ligand/homology',
                    'real_rep/protein',
                    'real_rep/binding_site',

                    )

  TOP_KEYS = ('all_atoms', 'main_rep', 'image', 'correct_rep', 'ligand')


  #@jlmem.cache
  def lig_selection_idxs_tops(gexp):
      """ compute both at once for convenience """

      from wepy.util.json_top import json_top_subset
      from wepy.util.mdtraj import (
          mdtraj_to_json_topology,
          json_to_mdtraj_topology,
          traj_fields_to_mdtraj,
      )

      from seh_prep.modules import (
          ligand_idxs,
          protein_idxs,
          binding_site_idxs,
          old_ligand_idxs,
          old_protein_idxs,
          old_binding_site_idxs,
      )
      from seh_prep.parameters import BINDING_SITE_CUTOFF

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      wepy_h5 = get_gexp_wepy_h5(gexp)

      with wepy_h5:
          main_rep_idxs = wepy_h5.main_rep_idxs


      sel_idxs = {}

      # the initial state
      init_state = get_ref_state(gexp)

      # get some things from this
      init_box_vectors = init_state['box_vectors'] # * init_state.box_vectors_unit

      # for the units if its legacy it won't have the units so we get
      # them from another gexp since they are the same
      if gexp in LEGACY_GEXPS:

          # get the init state for another known gexp
          tmp_state = get_ref_state('17')

          # use this for the units
          positions_unit = tmp_state.positions_unit
          box_vectors_unit = tmp_state.box_vectors_unit


      # otherwise just use the init state
      else:
          positions_unit = init_state.positions_unit
          box_vectors_unit = init_state.box_vectors_unit

      # the full topology
      all_atoms_top = get_topology(gexp)

      all_atoms_positions = init_state['positions'] # * positions_unit

      # get the indices of the ligand, protein and binding site
      all_atoms_ligand_idxs = ligand_idxs(all_atoms_top)
      all_atoms_protein_idxs = protein_idxs(all_atoms_top)
      all_atoms_bs_idxs = binding_site_idxs(all_atoms_top,
                                            all_atoms_positions * positions_unit,
                                            init_box_vectors * box_vectors_unit,
                                            BINDING_SITE_CUTOFF)


      # the all atoms subselections
      sel_idxs['all_atoms/ligand'] = all_atoms_ligand_idxs
      sel_idxs['all_atoms/protein'] = all_atoms_protein_idxs
      sel_idxs['all_atoms/binding_site'] = all_atoms_bs_idxs

      # get the idxs of the ligand homology atoms in terms of the ligand itself
      lig_hom_idxs = np.array(dict(LIGAND_HOMOLOGY_INDICES)[lig_id])
      sel_idxs['ligand/homology'] = lig_hom_idxs

      # set the selection for the ligand homology indices from the all atoms
      sel_idxs['all_atoms/ligand/homology'] = sel_idxs['all_atoms/ligand'][sel_idxs['ligand/homology']]


      # do the indices for the faulty main_rep

      # do for the indices within the all_atoms rep

      all_atoms_fields = {'positions' : np.array([all_atoms_positions]),
                         'box_vectors' : np.array([init_box_vectors])}

      # load the topology/reference structure PDB
      ref_traj = traj_fields_to_mdtraj(all_atoms_fields, all_atoms_top)

      # the ligand and protein indices for this ligand system
      all_atoms_main_rep_lig_idxs = old_ligand_idxs(ref_traj.top, "UNL")
      # we get the indices for the old way of getting protein indices,
      # this is missing atoms
      all_atoms_main_rep_prot_idxs = old_protein_idxs(ref_traj.top)

      all_atoms_main_rep_bs_idxs = old_binding_site_idxs(ref_traj.top, "UNL",
                                          all_atoms_positions * positions_unit,
                                          init_box_vectors * box_vectors_unit,
                                          BINDING_SITE_CUTOFF)


      # the indices of the main rep within the all_atoms rep
      sel_idxs['all_atoms/main_rep'] = np.concatenate((all_atoms_main_rep_lig_idxs,
                                                       all_atoms_main_rep_prot_idxs))
      sel_idxs['all_atoms/main_rep/ligand'] = all_atoms_main_rep_lig_idxs
      sel_idxs['all_atoms/main_rep/protein'] = all_atoms_main_rep_prot_idxs
      sel_idxs['all_atoms/main_rep/binding_site'] = all_atoms_main_rep_bs_idxs

      sel_idxs['all_atoms/main_rep/ligand/homology'] = sel_idxs['all_atoms/main_rep/ligand'][sel_idxs['ligand/homology']]



      # and the internal indices of the main rep


      main_rep_top = json_top_subset(all_atoms_top, main_rep_idxs)
      main_rep_positions = all_atoms_positions[main_rep_idxs]


      main_rep_fields = {'positions' : np.array([main_rep_positions]),
                         'box_vectors' : np.array([init_box_vectors])}

      # load the topology/reference structure PDB
      main_rep_traj = traj_fields_to_mdtraj(main_rep_fields, main_rep_top)

      # the ligand and protein indices for this ligand system
      main_rep_lig_idxs = old_ligand_idxs(main_rep_traj.top, "UNL")
      # we get the indices for the old way of getting protein indices,
      # this is missing atoms
      main_rep_prot_idxs = old_protein_idxs(main_rep_traj.top)

      main_rep_bs_idxs = old_binding_site_idxs(
          main_rep_traj.top,
          "UNL",
          main_rep_positions * positions_unit,
          init_box_vectors * box_vectors_unit,
          BINDING_SITE_CUTOFF,
      )


      # the indices of the main rep within the all_atoms rep
      sel_idxs['main_rep/ligand'] = main_rep_lig_idxs
      sel_idxs['main_rep/protein'] = main_rep_prot_idxs
      sel_idxs['main_rep/binding_site'] = main_rep_bs_idxs

      sel_idxs['main_rep/ligand/homology'] = sel_idxs['main_rep/ligand'][sel_idxs['ligand/homology']]

      # make a correct rep topology

      # the selections relative to the correct rep
      correct_rep_idxs = np.concatenate((all_atoms_ligand_idxs,
                                         all_atoms_protein_idxs))
      correct_rep_top = json_top_subset(all_atoms_top, correct_rep_idxs)
      correct_rep_positions = all_atoms_positions[correct_rep_idxs]

      # get the ligand, protein, and binding site indices for this
      correct_ligand_idxs = ligand_idxs(correct_rep_top)
      correct_protein_idxs = protein_idxs(correct_rep_top)
      correct_bs_idxs = binding_site_idxs(correct_rep_top,
                                          correct_rep_positions * positions_unit,
                                          init_box_vectors * box_vectors_unit,
                                          BINDING_SITE_CUTOFF)


      # determine the indices of the missing atoms from the main rep in
      # the all_atoms rep
      missing_atom_idxs = list(set(correct_rep_idxs).difference(main_rep_idxs))
      missing_atom_idxs.sort()

      # and the atoms they both have
      common_atom_idxs = list(set(correct_rep_idxs).intersection(main_rep_idxs))
      common_atom_idxs.sort()

      # then get them in terms of the correct rep
      correct_rep_missing_idxs = np.where(np.in1d(correct_rep_idxs, missing_atom_idxs))[0]
      correct_rep_common_idxs = np.where(np.in1d(correct_rep_idxs, common_atom_idxs))[0]

      # correct rep selections
      sel_idxs['all_atoms/correct_rep'] = correct_rep_idxs
      sel_idxs['all_atoms/correct_rep/missing'] = np.array(missing_atom_idxs)
      sel_idxs['all_atoms/correct_rep/main_rep'] = np.array(common_atom_idxs)

      # TODO
      # sel_idxs['all_atoms/correct_rep/ligand'] =
      # sel_idxs['all_atoms/correct_rep/ligand/homology'] = sel_idxs['all_atoms/correct_rep/ligand'][sel_idxs['ligand/homology']]

      sel_idxs['correct_rep/ligand'] = correct_ligand_idxs
      sel_idxs['correct_rep/protein'] = correct_protein_idxs
      sel_idxs['correct_rep/binding_site'] = correct_bs_idxs

      sel_idxs['correct_rep/missing'] = correct_rep_missing_idxs
      sel_idxs['correct_rep/main_rep'] = correct_rep_common_idxs
      sel_idxs['correct_rep/ligand/homology'] = sel_idxs['correct_rep/ligand'][sel_idxs['ligand/homology']]


      # image

      # combine the ligand and binding site to get the image idxs
      image_idxs = np.concatenate((all_atoms_ligand_idxs, all_atoms_bs_idxs))

      # get the topology for this
      image_top = json_top_subset(all_atoms_top, image_idxs)

      # get the ligand and protein from that
      image_ligand_idxs = ligand_idxs(image_top)
      image_protein_idxs = protein_idxs(image_top)

      sel_idxs['all_atoms/image'] = image_idxs
      sel_idxs['image/ligand'] = image_ligand_idxs
      sel_idxs['image/protein'] = image_protein_idxs
      # since the image is the binding site we also set this here which
      # is expected by other programs
      sel_idxs['image/binding_site'] = image_protein_idxs

      # ligand
      ligand_top = json_top_subset(all_atoms_top, all_atoms_ligand_idxs)

      sel_tops = {}

      sel_tops['all_atoms'] = all_atoms_top
      sel_tops['main_rep'] = main_rep_top
      sel_tops['ligand'] = ligand_top
      sel_tops['image'] = image_top
      sel_tops['correct_rep'] = correct_rep_top

      # add in stuff for legacy if necessary
      if gexp in LEGACY_GEXPS:

          leg_sel_idxs, leg_sel_tops = real_rep_selection_idxs_tops(
              sel_idxs,
              sel_tops,
          )

          sel_idxs.update(leg_sel_idxs)
          sel_tops.update(leg_sel_tops)

      return sel_idxs, sel_tops

  def lig_selection_idxs(gexp):

      sels, tops = lig_selection_idxs_tops(gexp)

      return sels

  def lig_selection_tops(gexp):

      sels, tops = lig_selection_idxs_tops(gexp)

      return tops

  def ligs_selection_tops():
      d = {}
      for gexp in GEXPS:
          d[gexp] = lig_selection_tops(gexp)
      return d

  def ligs_selection_idxs():
      d = {}
      for gexp in GEXPS:
          d[gexp] = lig_selection_idxs(gexp)

      return d


  # make a dataframe summarizing the counts of different selections for each ligand
  def ligs_selection_df():
      from collections import defaultdict

      selection_df = defaultdict(list)
      for gexp, selections in ligs_selection_idxs().items():

          selection_df['gexp'].append(gexp)
          for selection_key, idxs in selections.items():
              selection_df[selection_key].append(len(idxs))

      selection_df = pd.DataFrame(selection_df)

      return selection_df
#+END_SRC


**** Binding Site Anatomy

This is easier as its the same for all of them. We just need to make
sure we specifically mention which rep it is for.

We need the "main_rep" one because we need to calculate it over
everything and we need the "correct_rep" so we can carefully visualize
it.

I also set down some indices that will be useful for building this
up. That is the atoms and residues which are the active site etc.

From my old paper we have these identification of interesting
residues:

The coordinating residues in the binding site crystal structure of 4OD0.

| role       | name   | AA  | res idx | resid (ref_prot) | color  |
|------------+--------+-----+---------+------------------+--------|
| BS         | Tyr236 | tyr |     236 |              235 | green  |
| BS         | Asp105 | asp |     105 |              104 | orange |
| BS         | Tyr153 | tyr |     153 |              152 | blue   |
|------------+--------+-----+---------+------------------+--------|
| hbond path |        | gln |     154 |                  | black  |
| hbond path |        | met |     189 |                  | yellow |
| hbond path |        | phe |     267 |                  | pink   |
| hbond path |        | val |     268 |                  | purple |


The hbond path residues were those that we found had a significant
number of interactions with during the old simulations.


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  REF_TOP_DIR = "data/ref_top"

  # these are selections that can be made on 'ref_prot'. The name refers
  # to the name from my previous paper.
  BS_ROIS = {
      'Tyr236' : {
          'resid' : (235, 235,),
          'color' : 'green',
      },

      'Asp105' : {
          'resid' : (105, 105,),
          'color' : 'orange',
      },

      'Tyr153' : {
          'resid' : (152, 152,),
          'color' : 'blue',
      },
      'Met189' : {
          'resid' : (188, 188,),
          'color' : 'yellow',
      },
      'Ala134' : {
          'resid' : (134, 134,),
          'color' : 'pink',
      },

  }

#+END_SRC

***** COMMENT

old stuff for writing out redundant PDB files

#+begin_src pytho
ndef write_bs_anatomy_ref_structure():

    from wepy.util.mdtraj import (
        json_to_mdtraj_topology,
        mdtraj_to_json_topology,
    )
    from wepy.util.json_top import (
        json_top_atom_df,
        json_top_subset,
    )

    # load the full reference structure
    src_traj = mdj.load_pdb(BS_ANATOMY_TEMPLATE_COMPLEX_SRC_PATH)

    src_json_top = mdtraj_to_json_topology(src_traj.top)
    src_top_df = src_traj.top.to_dataframe()[0]

    ### Write out just the protein

    prot_idxs = src_top_df[src_top_df['segmentID'] == 'PROA'].index.values


    prot_json_top = json_top_subset(src_json_top, prot_idxs)

    prot_top = json_to_mdtraj_topology(prot_json_top)

    prot_traj = copy(src_traj)
    prot_traj.top = prot_top
    prot_traj.xyz = src_traj.xyz[:,prot_idxs,:]

    prot_traj.save_pdb(BS_ANATOMY_TEMPLATE_COMPLEX_PATH)


    ### Just the ligand

    lig_idxs = src_top_df[src_top_df['segmentID'] == 'HETA'].index.values


    prot_json_top = json_top_subset(src_json_top, lig_idxs)

    prot_top = json_to_mdtraj_topology(prot_json_top)

    prot_traj = copy(src_traj)
    prot_traj.top = prot_top
    prot_traj.xyz = src_traj.xyz[:,lig_idxs,:]

    prot_traj.save_pdb(BS_ANATOMY_TEMPLATE_COMPLEX_PATH)

#+end_src

**** Ligand Homology

For doing co-clustering we want to use metrics that are invariant
across the different chemical species so that we can have uniform
feature vectors.

This means picking specific atoms to pair up for both the ligand and
the binding site.


To that end we have manually identified a mapping of atoms between
different ligands.

These indices come from the main_rep of the systems in the
trajectories stored in the WepyHD5s.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  # TODO: double check and note here exactly which "rep" this is
  # for. Hopefully shouldn't make a difference since it is for the
  # ligand which was complete always

  # the order of the atom indices here is very important as this is what
  # maps them to each other. I.e. the index of the atom index is the key
  # for the abstract homological atom we are comparing.
  LIGAND_HOMOLOGY_INDICES = (
      ('17', (14, 5, 0, 19, 36, 37,)),
      ('3', (43, 40, 29, 0, 11, 13)),
      ('10', (10, 5, 0, 24, 35, 37)),
      ('18', (14, 11, 0, 19, 36, 37)),
      ('20', (46, 37, 32, 0, 11, 13)),
  )
#+END_SRC

**** BS-lig Atom Pairs

For this we need to get the atom pairs for each ligand.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_lig_bs_bs_atom_idxs(gexp,
                              rep_key='main_rep'):
      """This will return the indices of the atoms that are considered for
      the feature vector.

      Basically this is a selection of specific points in the binding
      site to make a reduced feature vector size.

      These indices are relative to the BS_ANATOMY_TEMPLATE_NAME: ref_prot

      """

      # NOTE: gexp is ignored, they are all the same


      # always load the centered one
      # ref_traj = mdj.load_pdb(str(Path(REF_TOP_DIR) / f"{rep_key}_center_ref.pdb") )

      ref_traj = lig_centered_ref_selection_trajs(
          gexp,
      )[rep_key]

      ref_top_df = ref_traj.top.to_dataframe()[0]

      # we will use a single backbone C-alpha atom ('CA' type) from
      # specific amino acid residues of interest (ROIs)

      # these are the ROI names for the amino acids
      aa_rois = (
          'Tyr236',
          'Asp105',
          'Tyr153',
          'Met189',
          'Ala134',
      )

      # for each of them get the topology of them and get the index of
      # the CA
      aa_ca_idxs = []
      for aa_roi in aa_rois:

          # get the resid, this is encoded as a range, so discard the
          # end
          resid, _ = BS_ROIS[aa_roi]['resid']

          aa_ca_idx = ref_top_df[
              (ref_top_df['resSeq'] == resid) &
              (ref_top_df['name'] == 'CA')
          ].index.values[0]

          aa_ca_idxs.append(aa_ca_idx)

      # thats it, just return these
      return np.array(aa_ca_idxs)


  def get_lig_bs_lig_atom_idxs(gexp,
                               rep_key='main_rep'):

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # these are the indices chosen on the ligand. They are
      # "homological" because they are matched across the different
      # ligands to be the same
      hom_idxs = list(dict(LIGAND_HOMOLOGY_INDICES)[lig_id])

      # these are in terms of the ligand itself we need the relative to
      # the rep
      lig_sel_idxs = lig_selection_idxs(lig_id)[f'{rep_key}/ligand'][hom_idxs]

      return lig_sel_idxs


  def lig_bs_atom_pairs(
          gexp,
          rep_key='main_rep',
  ):
      """Get the pairs of atoms between a selection of atoms on the ligand
      and the selected binding site atoms.

      This is always done in terms of the 'main_rep' since that is the
      only reliable metric.

      """

      from itertools import product

      # Get the atoms on the binding site that we are pairing with
      bs_sel_idxs = get_lig_bs_bs_atom_idxs(
          gexp,
          rep_key=rep_key
      )

      lig_sel_idxs = get_lig_bs_lig_atom_idxs(
          gexp,
          rep_key=rep_key,
      )

      # make the all to all pairs of them
      pairs = np.array(list(product(lig_sel_idxs, bs_sel_idxs)))

      return pairs


  def vmd_query_lig_bs_atom_pairs(
          gexp,
          rep_key='main_rep',
  ):
      """Print the VMD query for selecting the selected atoms for the ligand
      and binding site pairing.
      """

      lig_sel_idxs = get_lig_bs_lig_atom_idxs(
          gexp,
          rep_key=rep_key
      )

      bs_sel_idxs = get_lig_bs_bs_atom_idxs(
          gexp,
          rep_key=rep_key
      )

      # verify visually so print out the VMD query for the selection of
      # atoms for both the ligand and the protein
      vmd_q_str = "index {}"

      print("Protein selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in bs_sel_idxs])))

      print("Ligand selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in lig_sel_idxs])))


#+END_SRC


****** Report for indices of ligands

******* DONE Ligand 3:

#+BEGIN_EXAMPLE
Protein selection indices:  index 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 560 561 567 568 569 570 571 1012 1212 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1615 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1715 1717 1718 1970 1973 1975 1976 1977 1980 1981 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1999 2000 2001 2002 2003 2005 2007 2008 2009 2010 2011 2012 2013 2015 2016 2022 2034 2038 2046 2047 2048 2049 2153 2154 2159 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2291 2299 2303 2304 2305 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2354 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2417 2418 2420 2421 2422 2423 2424 2425 2426 2427 2560 2562 2563 2564 2565 2691 2730 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2763 2764 2765 2877 2878 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2893 2901 2902 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3037 3038 3039 3040 3041 3043 3044 3591 3593 3637 3641 3642 3643 3644 3645 3646 3647 3648 3649 3650 3651 3652 3696 3698 3700 3701 3702 3703 3704 3705 3706 3707 3760 4160 4162 4174 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4272 4275 4277 4278 4279 4280 4281 4559 4560 4561 4562 4563 4564 4565 4566 4567 4568 4569 4570
Ligand selection indices:  index 43 40 29 0 11 13
#+END_EXAMPLE

After visualization they look good



*** Data Exploration

**** Warp Tables

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def get_span_warps_df(
          gexp,
          span_id,
  ):

      contig = get_gexp_span_contig(gexp, span_id)

      with contig:
          return contig.warping_records_dataframe()


  def get_gexp_warps_df(
          gexp,
  ):
      """Make a table of all warping events across a gexp.

      Assign an idx to each warp event and add a column for the span
      that it is in.

      """

      span_dfs = []
      for span_id in get_gexp_span_ids(gexp):

          contig = get_gexp_span_contig(gexp, span_id)

          # get the warps df
          with contig:
              span_warps_df = contig.warping_records_dataframe()

          if len(span_warps_df) > 0:
              # add a column for the span_id
              span_warps_df['span_id'] = span_id

              span_dfs.append(span_warps_df)

      # concatenate to the master table
      warps_df = pd.concat(span_dfs)

      # then reindex
      warps_df.index = range(len(warps_df))

      return warps_df
#+end_src

**** Warp Trajectories

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  # @jlmem.cache
  def get_warp_lineage_traces(lig_id, span_id):

      contig = get_gexp_span_contig(lig_id, span_id)

      with contig:
          warp_trace = contig.warp_contig_trace()
          lineage_traces = contig.lineages(warp_trace)

      return lineage_traces


  def get_warp_lineage_traj_fields(gexp, span_id):
      """Generator for the warp lineages and do recentering if necessary.

      Trajectory fields only.

      Shouldn't have all in memory at once so we yield them.
      """

      lineage_traces = get_warp_lineage_traces(gexp, span_id)

      contig = get_gexp_span_contig(gexp, span_id)

      for warp_idx, lineage_trace in enumerate(lineage_traces):

          with contig:

              if gexp == '3':

                  traj_fields = contig.wepy_h5.get_trace_fields(
                      lineage_trace,
                      ['positions', 'box_vectors']
                  )

              else:
                  traj_fields = contig.wepy_h5.get_trace_fields(
                      lineage_trace,
                      ['positions', 'box_vectors', 'alt_reps/missing']
                  )


          # if we can convert the positions to the correct rep
          if gexp == '3':
              alt_rep = 'main_rep'

          else:
              alt_rep = 'correct_rep'
              # get the correct rep from this data
              traj_fields = traj_fields_to_correct_rep(traj_fields, gexp)
              gc.collect()

          ## do processing

          # 1. recentering
          traj_fields['positions'] = recenter_superimpose_traj(
              traj_fields,
              gexp,
              alt_rep,
          )[0]


          yield traj_fields

  def get_warp_lineage_trajs(gexp, span_id):
      """Generator for the warp lineages and do recentering if necessary.

      mdtraj trajectories

      Shouldn't have all in memory at once so we yield them.
      """

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      if lig_id == '3':
          json_top = lig_selection_tops(lig_id)['main_rep']

      else:
          json_top = lig_selection_tops(lig_id)['correct_rep']

      for traj_fields in get_warp_lineage_traj_fields(gexp, span_id):
          yield traj_fields_to_mdtraj(
             traj_fields,
             json_top
          )

#+end_src


**** Final Walker Trajectories

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  # @jlmem.cache
  def get_final_lineage_traces(lig_id, span_id):

      contig = get_gexp_span_contig(lig_id, span_id)

      with contig:
          final_trace = contig.final_contig_trace()
          lineage_traces = contig.lineages(final_trace)

      return lineage_traces


  def get_final_lineage_traj_fields(gexp, span_id):
      """Generator for the final walkers lineages and do recentering if necessary.

      Trajectory fields only.

      Shouldn't have all in memory at once so we yield them.
      """

      lineage_traces = get_final_lineage_traces(gexp, span_id)

      contig = get_gexp_span_contig(gexp, span_id)

      for lineage_trace in lineage_traces:


          if gexp == '3':
              alt_rep = 'main_rep'
              with contig:
                  traj_fields = contig.wepy_h5.get_trace_fields(
                      lineage_trace,
                      ['positions', 'box_vectors',]
                  )


          else:
              alt_rep = 'correct_rep'

              with contig:
                  traj_fields = contig.wepy_h5.get_trace_fields(
                      lineage_trace,
                      ['positions', 'box_vectors', 'alt_reps/missing']
                  )

              # get the correct rep from this data
              traj_fields = traj_fields_to_correct_rep(traj_fields, gexp)
              gc.collect()

          ## do processing

          # 1. recentering
          traj_fields['positions'] = recenter_superimpose_traj(
              traj_fields,
              gexp,
              alt_rep,
          )[0]


          yield traj_fields

  def get_final_lineage_trajs(gexp, span_id):
      """Generator for the final lineages and do recentering if necessary.

      mdtraj trajectories

      Shouldn't have all in memory at once so we yield them.
      """

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      json_top = lig_selection_tops(lig_id)['correct_rep']

      for traj_fields in get_final_lineage_traj_fields(gexp, span_id):
          yield traj_fields_to_mdtraj(
                             traj_fields,
                             json_top
          )

#+end_src


**** High Progress Walkers

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def get_high_progress_traces(
          gexp,
          span_id,
          top_n,
          progress_key):

      contig_trace = get_high_progress_contig_traces(
          gexp,
          span_id,
          top_n,
          progress_key)

      contig = get_gexp_span_contig(gexp, span_id)
      with contig:
          # convert to a run trace for getting data out of HDF5
          run_trace = contig.walker_trace_to_run_trace(
              contig_trace
              )

      return run_trace

  def get_high_progress_contig_traces(
          gexp,
          span_id,
          top_n,
          progress_key):

      contig = get_gexp_span_contig(gexp, span_id)

      with contig:

          # get the progress records for this contig as a big array
          prog_vals = np.array(
              [getattr(rec, progress_key)
               for rec in contig.progress_records()])

      # do a partial sort for top N values and slice the indices
      # of those off
      top_prog_val_flatidxs = np.argpartition(prog_vals.flat, -top_n)[-top_n:]

      # function to unflatten the idxs
      row_len = prog_vals.shape[1]
      unflatten_idx = (lambda flat_idx :
                       (flat_idx // row_len, flat_idx % row_len))


      with contig:

          # get the unflattenend contig walker trace indices of them
          # (walker, cycle)
          contig_trace = [(walker_idx, cycle_idx)
                          for cycle_idx, walker_idx in
                          [unflatten_idx(flatidx)
                           for flatidx in top_prog_val_flatidxs
                           ]
                          ]


      return contig_trace

  def get_high_progress_traj_fields(
          gexp,
          span_id,
          top_n,
          progress_key
  ):

      run_trace = get_high_progress_traces(
                  gexp,
                  span_id,
                  top_n,
                  progress_key,
              )

      contig = get_gexp_span_contig(gexp, span_id)

      with contig:

          # now use the trace to get the fields and traj for this with
          # recentering
          traj_fields = contig.wepy_h5.get_trace_fields(
              top_prog_val_run_trace,
              ['positions', 'box_vectors', 'alt_reps/missing'],
          )


      # get the correct rep from this data
      traj_fields = traj_fields_to_correct_rep(traj_fields, gexp)
      gc.collect()


      traj_fields['positions'] = recenter_superimpose_traj(
          traj_fields,
          gexp,
          'correct_rep',
      )[0]

      return traj_fields


  def get_high_progress_trajs(
          gexp,
          span_id,
          top_n,
          progress_key,
  ):

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      json_top = lig_selection_tops(lig_id)['correct_rep']


      traj_fields = get_high_progress_traj_fields(
                  gexp,
                  span_id,
                  top_n,
                  progress_key,
              )

      yield traj_fields_to_mdtraj(
              traj_fields,
              json_top
          )



  ### The lineages version


  def get_high_progress_lineage_traces(
          gexp,
          span_id,
          top_n,
          progress_key,
  ):

      # the trace of the final walkers in a lineage
      child_trace = get_high_progress_contig_traces(
          gexp,
          span_id,
          top_n,
          progress_key)

      contig = get_gexp_span_contig(gexp, span_id)

      with contig:
          lineage_traces = contig.lineages(child_trace)

      return lineage_traces

  def get_high_progress_lineages_traj_fields(
          gexp,
          span_id,
          top_n,
          progress_key,
  ):

      lineage_traces = get_high_progress_lineage_traces(
            gexp,
            span_id,
            top_n,
            progress_key,
        )

      contig = get_gexp_span_contig(gexp, span_id)

      for lineage_trace in lineage_traces:

          with contig:
              traj_fields = contig.wepy_h5.get_trace_fields(
                  lineage_trace,
                  ['positions', 'box_vectors', 'alt_reps/missing']
              )

          # get the correct rep from this data
          traj_fields = traj_fields_to_correct_rep(traj_fields, gexp)
          gc.collect()

          ## do processing

          # 1. recentering
          traj_fields['positions'] = recenter_superimpose_traj(
              traj_fields,
              gexp,
              'correct_rep'
          )[0]


          yield traj_fields

  def get_high_progress_lineages_trajs(
          gexp,
          span_id,
          top_n,
          progress_key,
  ):

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      json_top = lig_selection_tops(lig_id)['correct_rep']

      for traj_fields in get_high_progress_lineages_traj_fields(
              gexp,
              span_id,
              top_n,
              progress_key,
      ):

          yield traj_fields_to_mdtraj(
              traj_fields,
              json_top
          )
#+end_src


**** Spanning Contig Data Collection


#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py


  SPANS_TABLE_COLS = (
      'gexp',
      'span_id',
      'n_runs',
      'n_warps',
      'n_cycles',
      'total_sampling_time_ps', # picoseconds
      'warps_per_ps', # picoseconds
      'rate_ps', # picoseconds
      'total_warped_weight',
      'warp_weight_mean',
      'warp_weight_median',
      'warp_weight_min',
      'warp_weight_max',
      'warp_wait-time_mean_ps',
      'warp_wait-time_median_ps',
      'warp_wait-time_min_ps',
      'warp_wait-time_max_ps',
  )

  @jlmem.cache
  def get_span_stats_df(gexp):

      from wepy.analysis.rates import contig_warp_rates

      jobs_df = get_gexp_jobs_df(gexp)

      # extract the columns we need for a new dataframe
      spans_df = jobs_df[['span_id', 'segment_idx']]

      print(spans_df)
      # count the number of segments in the jobs
      spans_df = spans_df.groupby(['span_id']).count().reset_index().rename(
          columns={'segment_idx' : 'n_runs'})

      print(spans_df)

      # get the number of cycles and warps in the span by reading the
      # contig
      new_cols = {
          'n_warps' : [],
          'n_cycles' : [],

          'total_sampling_time_us' : [],
          'warps_per_ps' : [],
          'rate_s' : [],
          'rt_ms' : [],
          'total_warped_weight' : [],
          'warp_weight_mean' : [],
          'warp_weight_median' : [],
          'warp_weight_min' : [],
          'warp_weight_max' : [],
          'warp_wait-time_mean_us' : [],
          'warp_wait-time_median_us' : [],
          'warp_wait-time_min_us' : [],
          'warp_wait-time_max_us' : [],
      }

      for row_idx, row in spans_df.iterrows():
          print(row['span_id'])
          contig = get_gexp_span_contig(gexp, row['span_id'])

          with contig:

              n_cycles = contig.num_cycles

              # calculate rates and fluxes
              contig_rate = contig_warp_rates(
                  contig,
                  CYCLE_TIME
              )[0]

              try:
                  total_weight, rate, total_sampling_time = contig_rate[0]

              except KeyError:

                  print(f"No rates for {gexp}-{row['span_id']}")
                  total_weight = 0.0
                  rate = np.inf * (1 / CYCLE_TIME.unit)

                  total_sampling_time = contig.num_cycles * \
                      CYCLE_TIME * \
                      contig.num_walkers(contig.num_cycles-1)

              rt = 1 / rate

              warps_df = contig.warping_records_dataframe()



          n_warps = len(warps_df)

          if n_warps > 0:

              # the unweighted rate of warp events
              warps_per_sampling = n_warps / total_sampling_time
              total_warped_weight = warps_df['weight'].sum()

              # descriptive statistics on the warps weights
              mean_warp_weight = warps_df['weight'].mean()
              median_warp_weight = warps_df['weight'].median()
              min_warp_weight = warps_df['weight'].min()
              max_warp_weight = warps_df['weight'].max()


              wait_times_us = warps_df['cycle_idx'] * CYCLE_TIME.in_units_of(tkunit.microsecond)

              # descriptive statistics on the warps waiting times
              mean_warp_wait_time = wait_times_us.mean()
              median_warp_wait_time = wait_times_us.median()
              min_warp_wait_time = wait_times_us.min()
              max_warp_wait_time = wait_times_us.max()

          else:
              # the unweighted rate of warp events
              warps_per_sampling = n_warps / total_sampling_time
              total_warped_weight = 0.0
              mean_warp_weight = 0.0
              median_warp_weight = 0.0
              min_warp_weight = 0.0
              max_warp_weight = 0.0


              # descriptive statistics on the warps waiting times
              mean_warp_wait_time = np.inf * tkunit.microsecond
              median_warp_wait_time = np.inf * tkunit.microsecond
              min_warp_wait_time = np.inf * tkunit.microsecond
              max_warp_wait_time = np.inf * tkunit.microsecond


          new_cols['n_cycles'].append(n_cycles)
          new_cols['rate_s'].append(rate.value_in_unit((1/tkunit.second).unit))
          new_cols['rt_ms'].append(rt.value_in_unit(tkunit.millisecond))
          new_cols['total_sampling_time_us'].append(total_sampling_time.value_in_unit(tkunit.microsecond))
          new_cols['n_warps'].append(n_warps)
          new_cols['total_warped_weight'].append(total_warped_weight)
          new_cols['warps_per_ps'].append(warps_per_sampling.value_in_unit((1/tkunit.picosecond).unit))

          # warp weights statistics
          new_cols['warp_weight_mean'].append(mean_warp_weight)
          new_cols['warp_weight_median'].append(median_warp_weight)
          new_cols['warp_weight_min'].append(min_warp_weight)
          new_cols['warp_weight_max'].append(max_warp_weight)

          # warp wait time statistics
          new_cols['warp_wait-time_mean_us'].append(mean_warp_wait_time)
          new_cols['warp_wait-time_median_us'].append(median_warp_wait_time)
          new_cols['warp_wait-time_min_us'].append(min_warp_wait_time)
          new_cols['warp_wait-time_max_us'].append(max_warp_wait_time)

      # add the columns
      for colname, col in new_cols.items():
          spans_df[colname] = col

      return spans_df

  def span_stats_table_str(gexp):

      spans_df = get_span_stats_df(gexp)

      table_str = tabulate(spans_df,
                           headers=spans_df.columns,
                           tablefmt='orgtbl'
      )

      return table_str

  def get_master_span_stats_df():

      dfs = []
      for gexp in GEXPS:
          dfs.append(get_span_stats_df(gexp))

      return pd.concat(dfs)

  def get_master_span_stats_table_str():

      spans_df = get_master_span_stats_df(gexp)

      table_str = tabulate(spans_df,
                           headers=spans_df.columns,
                           tablefmt='orgtbl'
      )

      return table_str

#+end_src


***** COMMENT Old

Now I will look at some of the spanning contigs from those contig
trees

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  @jlmem.cache
  def lig_spans_data_df(lig_id):

      import itertools as it

      import pandas as pd
      import networkx as nx

      from wepy.analysis.parents import ParentForest

      spans_data = {}
      spans_df = {'lig_id' : [],
                  'span_id' : [],
                  'n_cycles' : [],
                  'n_exits' : [],
                  'span_hash' : []}


      contigtree = get_contigtree(lig_id)

      with contigtree:
          set_recursion_limit()
          lig_span_traces = contigtree.spanning_contig_traces()

      no_hash_count = 0
      for span_idx, span_trace in enumerate(lig_span_traces):

          with contigtree:
              # get the span ID from the span order by getting the hash of
              # this span from it's first run, if it doesn't have it the
              # span ID will be None-#
              first_run_idx = span_trace[0][0]
              try:
                  spanhash = contigtree.wepy_h5.run_end_snapshot_hash(first_run_idx)
                  span_id = spanhash
              except AttributeError:
                  print("No hash metadata for runs")
                  span_id = "None-{}".format(no_hash_count)
                  no_hash_count += 1

              # generate the contig
              contig = contigtree.make_contig(span_trace)


          span_data = {'warping_df' : None,
                       'parent_table' : None,
                       'exit_point_traces' : None}

          with contig:
              # get the warping records
              span_data['warping_df'] = contig.warping_records_dataframe()

              # add columns for the main df, which we will join
              span_data['warping_df']['lig_id'] = lig_id
              span_data['warping_df']['span_id'] = span_id

              # set the index as the "warp_idx"
              span_data['warping_df']['warp_idx'] = span_data['warping_df'].index

              # the parent table
              span_data['parent_table'] = contig.parent_table()

              # the exit point trajectory traces
              span_data['exit_point_traces'] = get_warp_lineage_traces(lig_id, span_id)

              # the full span walker tree traversal, we do a preorder and
              # post order traversal. The preoder one gives you from the
              # beginning to the end and the post order starts with the end
              # points and goes backwards to where they started

              # make the parent tree
              parent_forest = ParentForest(contig=contig)

              # preorder traversal
              preorder_traversal_trace = list(nx.dfs_preorder_nodes(parent_forest._graph))
              # convert to a walker trace
              preorder_traversal_trace = contig.walker_trace_to_run_trace(
                  [(traj_idx, cycle_idx) for cycle_idx, traj_idx in preorder_traversal_trace
                   if cycle_idx > -1])
              span_data['preorder_traversal_trace'] = preorder_traversal_trace

              # post order traversal
              postorder_traversal_trace = list(nx.dfs_postorder_nodes(parent_forest._graph))
              # convert to a walker trace
              postorder_traversal_trace = contig.walker_trace_to_run_trace(
                  [(traj_idx, cycle_idx) for cycle_idx, traj_idx in postorder_traversal_trace
                   if cycle_idx > -1])
              span_data['postorder_traversal_trace'] = postorder_traversal_trace


          spans_data[span_id] = span_data

          # add the dataframe data
          spans_df['lig_id'].append(lig_id)
          spans_df['span_hash'].append(spanhash)
          spans_df['span_id'].append(span_id)
          spans_df['n_cycles'].append(len(span_trace))
          spans_df['n_exits'].append(span_data['warping_df'].shape[0])


      spans_df = pd.DataFrame(spans_df)

      return spans_data, spans_df

  def lig_spans_data(lig_id):

      spans_data, _ = lig_spans_data_df(lig_id)

      return spans_data

  def lig_spans_df(lig_id):

      _, spans_df = lig_spans_data_df(lig_id)

      return spans_df

  def ligs_spans_data():
      d = {}
      for lig_id in LIG_IDS:
          d[lig_id] = lig_spans_data(lig_id)

      return d

  def master_spans_df():

      dfs = []
      for lig_id in LIG_IDS:
          dfs.append(lig_spans_df(lig_id))

      return pd.concat(dfs)

  # concatenate all of the warping dataframes
  def master_warping_df():
      master_warping_df = pd.concat([span_data['warping_df'] for span_data in
                                     it.chain(*[spans_data.values()
                                                for spans_data in ligs_spans_data().values()])
                                     if span_data['warping_df'].shape[0] > 0],
                                    ignore_index=True)

      return master_warping_df
#+END_SRC


*** Aggregate Probability Plots
**** Plotting function

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def plot_agg_prob(
          run_target_weights_rates,
          cycle_time,
          num_walkers,
          title=None,
          target_idx=0,
          time_unit=tkunit.picosecond,
          ylim=None,
          logscale=False,
  ):

      import numpy as np
      import scipy.stats
      import scipy.stats.mstats

      RUN_LABEL_TEMPLATE = "Run: {}"

      # collate data
      runs_weights = {}
      runs_times = {}
      for run_idx, target_weights_rates in run_target_weights_rates.items():
          run_cum_weights = []
          run_times = []

          for cycle_idx, timepoint in enumerate(target_weights_rates):

              # if there were no records (and thus no weights and rates
              # i.e. an empty dictionary) we set nans here so we can
              # easily discard them later but still keep track of time
              if not timepoint:
                  weight, _, total_sampling_time = 0.0, 0.0 * 1/time_unit, cycle_time * cycle_idx * num_walkers
              else:
                  weight, _, total_sampling_time = timepoint[target_idx]

              run_cum_weights.append(weight)
              run_times.append(total_sampling_time)

          runs_weights[run_idx] = run_cum_weights
          runs_times[run_idx] = run_times

      # compute the average unbound probability and MFPT across all runs

      # aggregate all of the runs weights into a single array and fill
      # with nans where missing, these will be ignored later

      num_runs = len(runs_times)

      # get the longest runs number of time points
      max_num_timepoints = max([len(run_times) for run_times in runs_times.values()])
      min_num_timepoints = min([len(run_times) for run_times in runs_times.values()])


      # get an array of the actual times for the longest one
      longest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                               reverse=True)[0][1]
      longest_run_times = runs_times[longest_run_idx]

      # the shortest one
      shortest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                                reverse=False)[0][1]
      shortest_run_times = runs_times[shortest_run_idx]



      # make an array for all of them where the longest we have is the
      # shortest one
      all_weights = np.empty((num_runs, min_num_timepoints))
      all_weights.fill(np.nan)
      # add the weights vectors to the array
      for i, run_weights in enumerate(runs_weights.values()):
          all_weights[i, 0:] = run_weights[0:min_num_timepoints]

      # compute the arithmetic mean, ignoring nans
      mean_weights = np.mean(all_weights, axis=0)


      # compute standard error of means
      sem_weights = scipy.stats.sem(all_weights, axis=0, nan_policy='raise')

      # get the bounds of the SEM envelope around the mean so we can
      # transform them
      upper_sem_bounds = mean_weights + sem_weights
      lower_sem_bounds = mean_weights - sem_weights

      # TODO compute harmonic mean as well of the rates

      # TODO: Alex wants me to calculate this just as a transformation
      # of the computed mean and std errors of the probability. Not sure
      # of the validity of this.

      shortest_run_time_values = np.array([time.value_in_unit(time_unit)
                                           for time in shortest_run_times])


      ### Plot
      fig, axes = plt.subplots(1, 1, constrained_layout=True)

      prob_ax = axes

      for run_idx in run_target_weights_rates.keys():

          # do the weights plot

          # get the times and weights converting if necessary
          run_weights = runs_weights[run_idx]
          run_times = [time.value_in_unit(time_unit) for time in runs_times[run_idx]]

          # slice them to the smallest run length
          run_weights = run_weights[0:min_num_timepoints]
          run_times = run_times[0:min_num_timepoints]

          label = RUN_LABEL_TEMPLATE.format(run_idx)

          replicate_color = dict(dict(REPLICATE_COLORS)[run_idx])['base']

          prob_ax.plot(run_times, run_weights, label=label, linewidth='3',
                       color=replicate_color)

          # then do the residence time plots


      # choose colors
      mean_color = dict(dict(REPLICATE_COLORS)['agg'])['base']
      sem_color = dict(dict(REPLICATE_COLORS)['agg'])['light']

      prob_ax.plot(shortest_run_time_values,
                   mean_weights,
                   label='Mean',
                   linewidth='3',
                   color=mean_color)

      prob_ax.fill_between(shortest_run_time_values,
                           mean_weights-sem_weights,
                           mean_weights+sem_weights,
                           label='Std. Error',
                           color=sem_color)

      # get the units from the value of the rates
      # rate_unit = runs_rates[list(runs_rates.keys())[0]][0].get_symbol()

      # # convert to the given unit as a rate
      # if time_unit is not None:
      #     rate_unit = 1 / (1 / rate_unit).in_units_of(time_unit)

      ## labels and fonts
      label_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '12'
      }

      ticks_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '10'
      }

      title_font = {
          'family' : 'sans-serif',
          'style' : 'normal',
          'size' : '12'
      }

      if title is None:
          prob_ax.set_title("Aggregate Probability",
                            fontdict=title_font)
      else:
          prob_ax.set_title(title,
                            fontdict=title_font)

      prob_ax.set_yscale('log')

      prob_ax.set_ylabel('Log Agg. Probability',
                         fontdict=label_font)

      # examples
      # prob_ax.set_ylim([1e-14, 1e-8])
      # prob_ax.set_ylim([1e-12, 10])
      if ylim is not None:
          prob_ax.set_ylim(ylim)


      if logscale:
          prob_ax.set_xscale('log')

          # TODO: set this intelligently
          prob_ax.set_xlim([50e-4, 1])

      prob_ax.set_xlabel('Log Simulation time (${}$)'.format(time_unit.get_symbol()),
                         fontdict=label_font)



      # set the tick fonts
      # for label in (prob_ax.get_xticklabels() + prob_ax.get_yticklabels()):
      #     label.set_fontname("serif")
      #     label.set_fontsize(10)

      prob_ax.legend(loc='best')

      return mean_weights, sem_weights, shortest_run_times, (fig, prob_ax)

#+END_SRC

**** Plot Data

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def gexp_plot_agg_prob(gexp):

      from wepy.analysis.rates import contig_warp_rates

      YLIMS = {
          '3' : [1e-12, 1e-4],
          '10' : None,
          '17' : None,
          '18' : None,
          '20' : None,
          'TPPU-legaxy' : None,
      }

      span_rates = {}
      for span_id in get_gexp_span_ids(gexp):
          contig = get_gexp_span_contig(gexp, span_id)

          num_walkers = get_gexp_span_num_walkers(gexp, span_id)

          with contig:

              # get the values as a series
              contig_rates = contig_warp_rates(contig, CYCLE_TIME, time_points=Ellipsis)

              span_rates[span_id] = contig_rates

      # make the plot for each span
      result = plot_agg_prob(span_rates,
                             CYCLE_TIME,
                             num_walkers,
                             target_idx=0,
                             title=gexp,
                             time_unit=tkunit.microsecond,
                             ylim=YLIMS[gexp],
                             logscale=False)

      return result
#+end_src


**** Rendering

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def gexp_show_plot_agg_prob(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, total_sampling_times, plots = \
                                                              gexp_plot_agg_prob(gexp)

      print("Gexp: {}".format(gexp))

      plt.show()

  def save_gexp_plot_agg_prob(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, total_sampling_times, plots = \
                                                              gexp_plot_agg_prob(gexp)

      print("Gexp: {}".format(gexp))

      fig, axes = plots

      save_gexp_fig(gexp, "agg_weights", fig)
#+end_src


*** Rate and RT Plots

We want to plot the rates and residence times over the course of the
simulations.

**** Plot RTs function

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def plot_rts(gexp,
               run_target_weights_rates,
               cycle_time,
               num_walkers,
               experimental_halftime,
               target_idx=0,
               mfpt_unit=tkunit.second,
               time_unit=tkunit.picosecond,
               ylim=None,
               halftime=False,
               logscale=False):
      """Plot the residence times

      Parameters
      ----------

      halftime : report the y axis as the halftime or the residence time

      experimental_halftime : The halftime of the ligand ln(2)/rate

      mfpt_unit : Unit of the y-axis, will convert values to this

      time_unit : Unit of the x-axis

      logscale : make the time axis logarithmic if True

      """

      import numpy as np
      import scipy.stats
      import scipy.stats.mstats

      RUN_LABEL_TEMPLATE = "Run: {}"


      # convert the experimental halftime to what is necessary
      if halftime:
          # already in this unit
          experimental_mfpt = experimental_halftime
      else:
          # conver to 1/rate residence time
          experimental_mfpt = convert_halftime_to_rt(experimental_halftime)

      # collate data
      runs_weights = {}
      runs_rates = {}
      runs_rts = {}
      runs_times = {}
      for run_idx, target_weights_rates in run_target_weights_rates.items():
          run_cum_weights = []
          run_rates = []
          run_times = []

          for cycle_idx, timepoint in enumerate(target_weights_rates):

              # if there were no records (and thus no weights and rates
              # i.e. an empty dictionary) we set nans here so we can
              # easily discard them later but still keep track of time
              if not timepoint:
                  #weight, rate, time = np.nan, np.nan, cycle_time * cycle_idx
                  weight, rate, total_sampling_time = (0.0,
                                                       0.0 * 1/time_unit,
                                                       cycle_time * cycle_idx * num_walkers)
              else:
                  weight, rate, total_sampling_time = timepoint[target_idx]

              run_cum_weights.append(weight)
              run_rates.append(rate)
              run_times.append(total_sampling_time)

          # calculate the residence times
          run_rts = []
          for rate in run_rates:

              # choose whether to plot the MFPT as halftime (ln(2)/rate)
              # or residence time (1/rate)
              if halftime:

                  run_rt = convert_rate_to_halftime(rate, time_base=mfpt_unit)

              else:
                  run_rt = convert_rate_to_rt(rate, time_base=mfpt_unit)

              # TODO remove. SHould be covered by our conversion functions

              # we can't divide by zero with simtk units so we get the
              # value and invert it with a numpy float64 then
              # redimensionalize it
              # run_rt = (np.float64(1.0) / rate.value_in_unit(rate.unit)) * (1/rate.unit)

              # if the rate is nan (which was 0.0 aggregated
              # probability) we catch this and explicitly set the
              # residence time to infinity
              # if np.isnan(rate):
              #     run_rt = np.inf * time_unit
              # # otherwise compute the mfpt/residence time
              # else:
              #     run_rt = 1/rate

              run_rts.append(run_rt)

          runs_weights[run_idx] = run_cum_weights
          runs_rates[run_idx] = run_rates
          runs_rts[run_idx] = run_rts
          runs_times[run_idx] = run_times


      # compute the average unbound probability and MFPT across all runs

      # aggregate all of the runs weights into a single array and fill
      # with nans where missing, these will be ignored later

      num_runs = len(runs_times)

      # get the longest runs number of time points
      max_num_timepoints = max([len(run_times) for run_times in runs_times.values()])
      min_num_timepoints = min([len(run_times) for run_times in runs_times.values()])


      # get an array of the actual times for the longest one
      longest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                               reverse=True)[0][1]
      longest_run_times = runs_times[longest_run_idx]

      # the shortest one
      shortest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                                reverse=False)[0][1]
      shortest_run_times = runs_times[shortest_run_idx]



      # make an array for all of them where the longest we have is the
      # shortest one
      all_weights = np.empty((num_runs, min_num_timepoints))
      all_weights.fill(np.nan)
      # add the weights vectors to the array
      for i, run_weights in enumerate(runs_weights.values()):
          all_weights[i, 0:] = run_weights[0:min_num_timepoints]

      # compute the arithmetic mean, ignoring nans
      mean_weights = np.mean(all_weights, axis=0)

      # compute standard error of means
      sem_weights = scipy.stats.sem(all_weights, axis=0, nan_policy='raise')

      # get the bounds of the SEM envelope around the mean so we can
      # transform them
      upper_sem_bounds = mean_weights + sem_weights
      lower_sem_bounds = mean_weights - sem_weights

      # TODO compute harmonic mean as well of the rates

      # TODO: Alex wants me to calculate this just as a transformation
      # of the computed mean and std errors of the probability. Not sure
      # of the validity of this.

      shortest_run_time_values = np.array([time.value_in_unit(time_unit)
                                           for time in shortest_run_times])


      # then compute MFPTs

      # mask the values for which no data was observed (nans)

      # calculate the rates for all the runs, masking the zero values
      # which need special tolerances due to the small values. These
      # small values are dependent upon the minimum probabilities
      # allowed by the simulation so this must be a parameter.
      #masked_mean_weights = np.ma.masked_invalid(mean_weights)

      # actually calculate the average mfpts from the mean weights over
      # time
      mfpts = shortest_run_time_values / mean_weights

      # compute the error in the rates using the relationship between
      # the MFPT and the weight: MFPT_error = weight_error * t * weight^2
      mfpt_sems = sem_weights * shortest_run_time_values * (mean_weights**-2)

      upper_sem_mfpts = mfpts + mfpt_sems
      lower_sem_mfpts = mfpts - mfpt_sems


      # clip the things that are close to zero or less than zero since
      # this is just a floating point subtraction error
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_mfpts, 0.),
                                                    lower_sem_mfpts < 0.))
      lower_sem_mfpts[close_to_zero_idx] = 0.



      # then compute the bounds for the MFPTs
      # upper_sem_mfpts = shortest_run_time_values / upper_sem_bounds
      # lower_sem_mfpts = shortest_run_time_values / lower_sem_bounds

      # because the lower standard error bound is lower than the average
      # we want to make sure the call to isclose doesn't count them as 0
      # so we make a new min_prob for the masking that takes into
      # account the lowest value. So we take the minimum value that is
      # not identically 0
      # lower_min_prob = lower_sem_bounds[lower_sem_bounds > 0.].min() * 1e-2

      # actually we are going to consder anythin 2 orders of magnitude
      # lower than the minimum probability to be zero so:
      # lower_sem_bound_min_value = min_probability * 1e-2

      # zero_masked_upper_sem_weights = np.ma.masked_values(upper_sem_bounds, 0.,
      #                                                     atol=min_probability)

      # zero_masked_lower_sem_weights = np.ma.masked_values(lower_sem_bounds, 0.,
      #                                                     atol=lower_min_prob)

      # # we calculate the standard error of the mean weights as well
      # zero_masked_upper_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_upper_sem_weights)
      # zero_masked_lower_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_lower_sem_weights)

      # make the plots, one for aggregate probability the other for
      # RT
      fig, axes = plt.subplots(1, 1, constrained_layout=True)

      rt_ax = axes

      for run_idx in run_target_weights_rates.keys():

          # do the weights plot

          # get the times and weights converting if necessary
          run_weights = runs_weights[run_idx]
          run_times = [time.value_in_unit(time_unit) for time in runs_times[run_idx]]

          # slice them to the smallest run length
          run_weights = run_weights[0:min_num_timepoints]
          run_times = run_times[0:min_num_timepoints]

          label = RUN_LABEL_TEMPLATE.format(run_idx)

          # then do the residence time plots

          run_rts = runs_rts[run_idx][0:min_num_timepoints]

          # convert the residence times to the time unit specified
          run_rts = [rt.value_in_unit(mfpt_unit) for rt in run_rts]

          replicate_color = dict(dict(REPLICATE_COLORS)[run_idx])['base']

          rt_ax.plot(run_times,
                     run_rts,
                     label=label,
                     linewidth='3',
                     color=replicate_color)



      shortest_run_times_values = [time.value_in_unit(time_unit) for time in shortest_run_times]
      # prob_ax.plot(shortest_run_times_values,
      #              mean_weights,
      #              label='Mean', linewidth='3', color='black')
      # prob_ax.fill_between(shortest_run_times_values,
      #                      mean_weights-sem_weights,
      #                      mean_weights+sem_weights,
      #                      label='Std. Error')

      # get the units from the value of the rates
      # rate_unit = runs_rates[list(runs_rates.keys())[0]][0].get_symbol()

      # # convert to the given unit as a rate
      # if time_unit is not None:
      #     rate_unit = 1 / (1 / rate_unit).in_units_of(time_unit)




      # plot the average rate

      # get the times and convert to the proper units
      times = [time.value_in_unit(time_unit) for time in shortest_run_times]

      # first because they were dedimensionalized we redimensionalize
      # them to the specified time_unit (x-axis) then convert them to
      # the MFPT unit, then dedimensionalize them to raw values for
      # plotting
      mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                              for mfpt in mfpts])

      upper_sem_mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                                        for mfpt in upper_sem_mfpts])

      lower_sem_mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                                        for mfpt in lower_sem_mfpts])

      # clip the things that are close to zero or less than zero since
      # this is just a floating point error from multiplying...
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_mfpt_values, 0.),
                                                    lower_sem_mfpts < 0.))
      lower_sem_mfpt_values[close_to_zero_idx] = 0.

      # choose colors
      mean_color = dict(dict(REPLICATE_COLORS)['agg'])['base']
      sem_color = dict(dict(REPLICATE_COLORS)['agg'])['light']

      rt_ax.plot(times,
                 mfpt_values,
                 label='Mean',
                 linewidth='3',
                 color=mean_color)

      rt_ax.fill_between(times,
                         upper_sem_mfpt_values,
                         lower_sem_mfpt_values,
                         label='Std. Error of Mean',
                         color=sem_color,
      )

      if logscale:
          rt_ax.set_yscale('log')


      ## labels and fonts
      label_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '12'
      }

      ticks_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '10'
      }

      title_font = {
          'family' : 'sans-serif',
          'style' : 'normal',
          'size' : '12'
      }

      # prob_ax.set_title("Aggregate Probability",
      #                   fontdict=title_font)
      # prob_ax.set_yscale('log')
      # prob_ax.set_ylabel('Aggregated unbound probability',
      #                    fontdict=label_font)

      # # TODO set this intelligently
      # prob_ax.set_ylim([1e-12, 10])

      # prob_ax.set_xlabel('Simulation time (${}$)'.format(time_unit.get_symbol()),
      #                    fontdict=label_font)

      # # set the tick fonts
      # # for label in (prob_ax.get_xticklabels() + prob_ax.get_yticklabels()):
      # #     label.set_fontname("serif")
      # #     label.set_fontsize(10)

      # prob_ax.legend(loc='best')

      rt_ax.set_title(f"Residence Time: {gexp}",
                      fontdict=title_font)

      # plt.xticks([400, 800, 1200, 1600], ['400', '800', '1200', '1600'])
      # plt.yticks([0.1, 10, 1000, 100000, 10000000, 1000000000])
      rt_ax.set_xlabel('Simulation time (${}$)'.format(time_unit.get_symbol()),
                       fontdict=label_font)

      if halftime:
          label_eq = "Half-life ($ln(2) / k_{off}$)"

      else:
          label_eq = "residence time ($1 / k_{off}$)"

      rt_ax.set_ylabel('Predicted {} ({})'.format(
          label_eq,
          mfpt_unit.get_symbol(),
      ),
                       fontdict=label_font)


      # suggestion
      # rt_ax.set_ylim([1e-14, 1e-8])
      if ylim is not None:
          rt_ax.set_ylim(ylim)


      # plot the experimental value
      experimental_points = [experimental_mfpt.value_in_unit(mfpt_unit) for _ in run_times]
      rt_ax.plot(times,
                 experimental_points,
                 color='red',
                 label='Experimental')

      rt_ax.legend()

      # return the actual computed values for the average aggregated
      # weights and residence times
      mfpts = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                        for mfpt in mfpts]) * mfpt_unit

      mfpt_sems = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                            for mfpt in mfpt_sems]) * mfpt_unit



      return mean_weights, sem_weights, mfpts, mfpt_sems, shortest_run_times, (fig, rt_ax)

#+END_SRC


**** Plot Rate function

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def plot_rates(gexp,
                 run_target_weights_rates,
                 cycle_time,
                 num_walkers,
                 experimental_rate,
                 target_idx=0,
                 rate_unit=(1/tkunit.second).unit,
                 time_unit=tkunit.picosecond,
                 ylim=None,
                 show_runs=True,
                 logscale=False):

      import numpy as np
      import scipy.stats
      import scipy.stats.mstats

      RUN_LABEL_TEMPLATE = "Run: {}"

      # collate data
      runs_weights = {}
      runs_rates = {}
      runs_rts = {}
      runs_times = {}
      for run_idx, target_weights_rates in run_target_weights_rates.items():
          run_cum_weights = []
          run_rates = []
          run_times = []

          for cycle_idx, timepoint in enumerate(target_weights_rates):

              # if there were no records (and thus no weights and rates
              # i.e. an empty dictionary) we set nans here so we can
              # easily discard them later but still keep track of time
              if not timepoint:
                  #weight, rate, time = np.nan, np.nan, cycle_time * cycle_idx
                  weight, rate, total_sampling_time = (0.0,
                                                       0.0 * (1/time_unit).unit,
                                                       cycle_time * cycle_idx * num_walkers)
              else:
                  weight, rate, total_sampling_time = timepoint[target_idx]

              run_cum_weights.append(weight)
              run_rates.append(rate)
              run_times.append(total_sampling_time)

          # calculate the residence times
          run_rts = []
          for rate in run_rates:

              # we can't divide by zero with simtk units so we get the
              # value and invert it with a numpy float64 then
              # redimensionalize it
              run_rt = (np.float64(1.0) / rate.value_in_unit(rate.unit)) * (1/rate.unit)

              # if the rate is nan (which was 0.0 aggregated
              # probability) we catch this and explicitly set the
              # residence time to infinity
              # if np.isnan(rate):
              #     run_rt = np.inf * time_unit
              # # otherwise compute the mfpt/residence time
              # else:
              #     run_rt = 1/rate

              run_rts.append(run_rt)

          runs_weights[run_idx] = run_cum_weights
          runs_rates[run_idx] = run_rates
          runs_rts[run_idx] = run_rts
          runs_times[run_idx] = run_times


      # compute the average unbound probability and MFPT across all runs

      # aggregate all of the runs weights into a single array and fill
      # with nans where missing, these will be ignored later

      num_runs = len(runs_times)

      # get the longest runs number of time points
      max_num_timepoints = max([len(run_times) for run_times in runs_times.values()])
      min_num_timepoints = min([len(run_times) for run_times in runs_times.values()])


      # get an array of the actual times for the longest one
      longest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                               reverse=True)[0][1]
      longest_run_times = runs_times[longest_run_idx]

      # the shortest one
      shortest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                                reverse=False)[0][1]
      shortest_run_times = runs_times[shortest_run_idx]



      # make an array for all of them where the longest we have is the
      # shortest one
      all_weights = np.empty((num_runs, min_num_timepoints))
      all_weights.fill(np.nan)
      # add the weights vectors to the array
      for i, run_weights in enumerate(runs_weights.values()):
          all_weights[i, 0:] = run_weights[0:min_num_timepoints]

      # compute the arithmetic mean, ignoring nans
      mean_weights = np.mean(all_weights, axis=0)

      # compute standard error of means
      sem_weights = scipy.stats.sem(all_weights, axis=0, nan_policy='raise')

      # get the bounds of the SEM envelope around the mean so we can
      # transform them
      upper_sem_bounds = mean_weights + sem_weights
      lower_sem_bounds = mean_weights - sem_weights

      # TODO compute harmonic mean as well of the rates

      # TODO: Alex wants me to calculate this just as a transformation
      # of the computed mean and std errors of the probability. Not sure
      # of the validity of this.

      shortest_run_time_values = np.array([time.value_in_unit(time_unit)
                                           for time in shortest_run_times])


      # then compute MFPTs

      # mask the values for which no data was observed (nans)

      # calculate the rates for all the runs, masking the zero values
      # which need special tolerances due to the small values. These
      # small values are dependent upon the minimum probabilities
      # allowed by the simulation so this must be a parameter.
      #masked_mean_weights = np.ma.masked_invalid(mean_weights)

      # actually calculate the average mfpts from the mean weights over
      # time
      rates = mean_weights / shortest_run_time_values

      # compute the error in the rates using the relationship between
      # the MFPT and the weight: MFPT_error = weight_error * t * weight^-2
      #rt_sems = sem_weights * shortest_run_time_values * (mean_weights**-2)
      rate_sems = sem_weights / shortest_run_time_values

      upper_sem_rates = rates + rate_sems
      lower_sem_rates = rates - rate_sems

      # clip the things that are close to zero or less than zero since
      # this is just a floating point subtraction error
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_rates, 0.),
                                                    lower_sem_rates < 0.))
      lower_sem_rates[close_to_zero_idx] = 0.

      # then compute the bounds for the MFPTs
      # upper_sem_mfpts = shortest_run_time_values / upper_sem_bounds
      # lower_sem_mfpts = shortest_run_time_values / lower_sem_bounds

      # because the lower standard error bound is lower than the average
      # we want to make sure the call to isclose doesn't count them as 0
      # so we make a new min_prob for the masking that takes into
      # account the lowest value. So we take the minimum value that is
      # not identically 0
      # lower_min_prob = lower_sem_bounds[lower_sem_bounds > 0.].min() * 1e-2

      # actually we are going to consder anythin 2 orders of magnitude
      # lower than the minimum probability to be zero so:
      # lower_sem_bound_min_value = min_probability * 1e-2

      # zero_masked_upper_sem_weights = np.ma.masked_values(upper_sem_bounds, 0.,
      #                                                     atol=min_probability)

      # zero_masked_lower_sem_weights = np.ma.masked_values(lower_sem_bounds, 0.,
      #                                                     atol=lower_min_prob)

      # # we calculate the standard error of the mean weights as well
      # zero_masked_upper_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_upper_sem_weights)
      # zero_masked_lower_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_lower_sem_weights)

      # make the plots, one for aggregate probability the other for
      # RT
      fig, axes = plt.subplots(1, 1, constrained_layout=True)

      # prob_ax = axes[0]
      rate_ax = axes
      for run_idx in run_target_weights_rates.keys():

          # do the weights plot

          # get the times and weights converting if necessary
          run_weights = runs_weights[run_idx]
          run_times = [time.value_in_unit(time_unit) for time in runs_times[run_idx]]

          # slice them to the smallest run length
          run_weights = run_weights[0:min_num_timepoints]
          run_times = run_times[0:min_num_timepoints]

          label = RUN_LABEL_TEMPLATE.format(run_idx)


          # then do the residence time plots

          # get the rates values previously calculated and truncate
          run_rates = runs_rates[run_idx][0:min_num_timepoints]

          # convert the rates to the time unit specified
          run_rates = [rate.value_in_unit(rate_unit)
                       for rate in run_rates]

          replicate_color = dict(dict(REPLICATE_COLORS)[run_idx])['base']

          # plot the rate estimates for each run
          rate_ax.plot(run_times,
                       run_rates,
                       label=label,
                       linewidth='3',
                       color=replicate_color)


      shortest_run_times_values = [time.value_in_unit(time_unit) for time in shortest_run_times]
      # prob_ax.plot(shortest_run_times_values,
      #              mean_weights,
      #              label='Mean', linewidth='3', color='black')
      # prob_ax.fill_between(shortest_run_times_values,
      #                      mean_weights-sem_weights,
      #                      mean_weights+sem_weights,
      #                      label='Std. Error')

      # get the units from the value of the rates
      # rate_unit = runs_rates[list(runs_rates.keys())[0]][0].get_symbol()

      # # convert to the given unit as a rate
      # if time_unit is not None:
      #     rate_unit = 1 / (1 / rate_unit).in_units_of(time_unit)




      # plot the average rate

      # get the times and convert to the proper units
      times = [time.value_in_unit(time_unit) for time in shortest_run_times]

      # first because they were dedimensionalized we redimensionalize
      # them to the specified time_unit (x-axis) then convert them to
      # the rate unit, then dedimensionalize them to raw values for
      # plotting
      rate_values = np.array([(rate / time_unit).value_in_unit(rate_unit)
                              for rate in rates])

      upper_sem_rate_values = np.array([(rate / time_unit).value_in_unit(rate_unit)
                                        for rate in upper_sem_rates])

      lower_sem_rate_values = np.array([(rate / time_unit).value_in_unit(rate_unit)
                                        for rate in lower_sem_rates])

      # clip the things that are close to zero or less than zero since
      # this is just a floating point error from multiplying...
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_rate_values, 0.),
                                                    lower_sem_rates < 0.))
      lower_sem_rate_values[close_to_zero_idx] = 0.


      # choose colors
      mean_color = dict(dict(REPLICATE_COLORS)['agg'])['base']
      sem_color = dict(dict(REPLICATE_COLORS)['agg'])['light']

      # plot means
      rate_ax.plot(times,
                   rate_values,
                   label='Mean',
                   linewidth='3',
                   color=mean_color)

      rate_ax.fill_between(times,
                           upper_sem_rate_values,
                           lower_sem_rate_values,
                           label='Std. Error of Mean',
                           color=sem_color,)

      if logscale:
          rate_ax.set_yscale('log')


      ## labels and fonts
      label_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '12'
      }

      ticks_font = {
          'family' : 'serif',
          'style' : 'normal',
          'size' : '10'
      }

      title_font = {
          'family' : 'sans-serif',
          'style' : 'normal',
          'size' : '12'
      }

      # prob_ax.set_title("Aggregate Probability",
      #                   fontdict=title_font)
      # prob_ax.set_yscale('log')
      # prob_ax.set_ylabel('Aggregated unbound probability',
      #                    fontdict=label_font)

      # # TODO set this intelligently
      # prob_ax.set_ylim([1e-12, 10])

      # prob_ax.set_xlabel('Simulation time (${}$)'.format(time_unit.get_symbol()),
      #                    fontdict=label_font)

      # # set the tick fonts
      # # for label in (prob_ax.get_xticklabels() + prob_ax.get_yticklabels()):
      # #     label.set_fontname("serif")
      # #     label.set_fontsize(10)

      # prob_ax.legend(loc='best')

      rate_ax.set_title(f"Probability Flux: {gexp}",
                      fontdict=title_font)

      # plt.xticks([400, 800, 1200, 1600], ['400', '800', '1200', '1600'])
      # plt.yticks([0.1, 10, 1000, 100000, 10000000, 1000000000])
      rate_ax.set_xlabel('Simulation time (${}$)'.format(time_unit.get_symbol()),
                       fontdict=label_font)

      rate_ax.set_ylabel('Probability Flux ($P\/{}$)'.format(rate_unit.get_symbol()),
                       fontdict=label_font)

      # was used as a default before [10e-4, 10e7]
      if ylim is not None:
          rate_ax.set_ylim(ylim)

      # plot the experimental value
      experimental_points = [experimental_rate.value_in_unit(rate_unit) for _ in run_times]
      rate_ax.plot(times,
                   experimental_points,
                   color='red',
                   label='Experimental')

      rate_ax.legend()

      # return the actual computed values for the average aggregated
      # weights and residence times
      rates = np.array([(rate / time_unit).value_in_unit(rate_unit)
                        for rate in rates]) * rate_unit

      rate_sems = np.array([(rate / time_unit).value_in_unit(rate_unit)
                            for rate in rate_sems]) * rate_unit



      return mean_weights, sem_weights, rates, rate_sems, shortest_run_times, (fig, rate_ax)

#+END_SRC


**** Plot Data

Actually perform the plotting for different ligands and runs.

For this we want to plot each run independently and then aggregate
them.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def gexp_plot_rates(gexp):

      from wepy.analysis.rates import contig_warp_rates

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      YLIMS = {
          '3' : None,
          '10' : None,
          '17' : None,
          '18' : None,
          '20' : None,
          'TPPU-legaxy' : None,
      }

      # get the experimental koff

      experimental_rate = dict(experimental_koffs())[lig_id]

      span_rates = {}
      for span_id in get_gexp_span_ids(gexp):
          contig = get_gexp_span_contig(gexp, span_id)

          num_walkers = get_gexp_span_num_walkers(gexp, span_id)

          with contig:

              # get the values as a series
              contig_rates = contig_warp_rates(contig, CYCLE_TIME, time_points=Ellipsis)

              span_rates[span_id] = contig_rates

      # make the plot for each span
      result = \
                                plot_rates(gexp,
                                           span_rates,
                                           CYCLE_TIME,
                                           num_walkers,
                                           experimental_rate,
                                           target_idx=0,
                                           rate_unit=(1/tkunit.second).unit,
                                           time_unit=tkunit.microsecond,
                                           ylim=YLIMS[gexp],
                                           logscale=False)

      return result

  def gexp_plot_rts(gexp):

      from wepy.analysis.rates import contig_warp_rates

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      YLIMS = {
          '3' : None,
          '10' : None,
          '17' : None,
          '18' : None,
          '20' : None,
          'TPPU-legaxy' : None,
      }

      # get the experimental mfpt/rt for this ligand in the correct unit
      experimental_halftime = dict(experimental_halftimes())[lig_id]

      # these are the "rates" structs from the function ignore that this
      # is a function for rts
      span_rates = {}
      for span_id in get_gexp_span_ids(gexp):
          contig = get_gexp_span_contig(gexp, span_id)

          num_walkers = get_gexp_span_num_walkers(gexp, span_id)

          with contig:

              # get the values as a series
              contig_rates = contig_warp_rates(contig, CYCLE_TIME, time_points=Ellipsis)

              span_rates[span_id] = contig_rates

      # make the plot for each span
      result = \
                                plot_rts(gexp,
                                         span_rates,
                                         CYCLE_TIME,
                                         num_walkers,
                                         experimental_halftime,
                                         target_idx=0,
                                         mfpt_unit=tkunit.minute,
                                         time_unit=tkunit.microsecond,
                                         ylim=YLIMS[gexp],
                                         logscale=True)

      return result


#+END_SRC


**** Rendering


#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def gexp_show_plot_rates(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, rates, rate_sems, shortest_run_times, plots = \
                                                              gexp_plot_rates(gexp)

      print("Gexp: {}".format(gexp))

      print("Final Rate: {}".format(rates[-1]))
      print("Final Rate SEM: {}".format(rate_sems[-1]))

      plt.show()

  def save_gexp_plot_rates(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, rates, rate_sems, shortest_run_times, plots = \
                                                              gexp_plot_rates(gexp)

      print("Gexp: {}".format(gexp))

      fig, axes = plots

      save_gexp_fig(gexp, "rates", fig)


  def gexp_show_plot_rts(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, rts, rt_sems, shortest_run_times, plots = \
                                                              gexp_plot_rts(gexp)

      # TODO: add option to get this as halftime

      print("Gexp: {}".format(gexp))

      print("Final Residence Time: {}".format(rts[-1]))
      print("Final Residence Time SEM: {}".format(rt_sems[-1]))

      plt.show()

  def save_gexp_plot_rts(gexp):

      import matplotlib.pyplot as plt

      mean_weights, sem_weights, rts, rt_sems, shortest_run_times, plots = \
                                                              gexp_plot_rts(gexp)


      # TODO: add option to get this as halftime

      print("Gexp: {}".format(gexp))

      fig, axes = plots

      save_gexp_fig(gexp, "residence_times", fig)
#+end_src



**** COMMENT Plotting Functions

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_rates_rt(run_target_weights_rates,
                 cycle_time,
                 num_walkers,
                 experimental_rt,
                 target_idx=0,
                 mfpt_unit=tkunit.second,
                 time_unit=tkunit.picosecond,
                 logscale=False):

      import numpy as np
      import scipy.stats
      import scipy.stats.mstats
      import matplotlib.pyplot as plt
      import simtk.unit as unit

      RUN_LABEL_TEMPLATE = "Run: {}"

      # collate data
      runs_weights = {}
      runs_rates = {}
      runs_rts = {}
      runs_times = {}
      for run_idx, target_weights_rates in run_target_weights_rates.items():
          run_cum_weights = []
          run_rates = []
          run_times = []

          for cycle_idx, timepoint in enumerate(target_weights_rates):

              # if there were no records (and thus no weights and rates
              # i.e. an empty dictionary) we set nans here so we can
              # easily discard them later but still keep track of time
              if not timepoint:

                  weight, rate, total_sampling_time = 0.0, 0.0 * 1/time_unit, cycle_time * cycle_idx * num_walkers
              else:
                  weight, rate, total_sampling_time = timepoint[target_idx]

              run_cum_weights.append(weight)
              run_rates.append(rate)
              run_times.append(total_sampling_time)

          # calculate the residence times
          run_rts = []
          for rate in run_rates:

              # we can't divide by zero with simtk units so we get the
              # value and invert it with a numpy float64 then
              # redimensionalize it
              run_rt = (np.float64(1.0) / rate.value_in_unit(rate.unit)) * (1/rate.unit)

              # if the rate is nan (which was 0.0 aggregated
              # probability) we catch this and explicitly set the
              # residence time to infinity
              # if np.isnan(rate):
              #     run_rt = np.inf * time_unit
              # # otherwise compute the mfpt/residence time
              # else:
              #     run_rt = 1/rate

              run_rts.append(run_rt)

          runs_weights[run_idx] = run_cum_weights
          runs_rates[run_idx] = run_rates
          runs_rts[run_idx] = run_rts
          runs_times[run_idx] = run_times


      # compute the average unbound probability and MFPT across all runs

      # aggregate all of the runs weights into a single array and fill
      # with nans where missing, these will be ignored later

      num_runs = len(runs_times)

      # get the longest runs number of time points
      max_num_timepoints = max([len(run_times) for run_times in runs_times.values()])
      min_num_timepoints = min([len(run_times) for run_times in runs_times.values()])


      # get an array of the actual times for the longest one
      longest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                               reverse=True)[0][1]
      longest_run_times = runs_times[longest_run_idx]

      # the shortest one
      shortest_run_idx = sorted([(len(run_times), run_idx)
                                for run_idx, run_times in runs_times.items()],
                                reverse=False)[0][1]
      shortest_run_times = runs_times[shortest_run_idx]



      # make an array for all of them where the longest we have is the
      # shortest one
      all_weights = np.empty((num_runs, min_num_timepoints))
      all_weights.fill(np.nan)
      # add the weights vectors to the array
      for i, run_weights in enumerate(runs_weights.values()):
          all_weights[i, 0:] = run_weights[0:min_num_timepoints]

      # compute the arithmetic mean, ignoring nans
      mean_weights = np.mean(all_weights, axis=0)

      # compute standard error of means
      sem_weights = scipy.stats.sem(all_weights, axis=0, nan_policy='raise')

      # get the bounds of the SEM envelope around the mean so we can
      # transform them
      upper_sem_bounds = mean_weights + sem_weights
      lower_sem_bounds = mean_weights - sem_weights

      # TODO compute harmonic mean as well of the rates

      # TODO: Alex wants me to calculate this just as a transformation
      # of the computed mean and std errors of the probability. Not sure
      # of the validity of this.

      shortest_run_time_values = np.array([time.value_in_unit(time_unit)
                                           for time in shortest_run_times])


      # then compute MFPTs

      # mask the values for which no data was observed (nans)

      # calculate the rates for all the runs, masking the zero values
      # which need special tolerances due to the small values. These
      # small values are dependent upon the minimum probabilities
      # allowed by the simulation so this must be a parameter.
      #masked_mean_weights = np.ma.masked_invalid(mean_weights)

      # actually calculate the average mfpts from the mean weights over
      # time
      mfpts = shortest_run_time_values / mean_weights

      # compute the error in the rates using the relationship between
      # the MFPT and the weight: MFPT_error = weight_error * t * weight^2
      mfpt_sems = sem_weights * shortest_run_time_values * (mean_weights**-2)

      upper_sem_mfpts = mfpts + mfpt_sems
      lower_sem_mfpts = mfpts - mfpt_sems


      # clip the things that are close to zero or less than zero since
      # this is just a floating point subtraction error
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_mfpts, 0.),
                                                    lower_sem_mfpts < 0.))
      lower_sem_mfpts[close_to_zero_idx] = 0.



      # then compute the bounds for the MFPTs
      # upper_sem_mfpts = shortest_run_time_values / upper_sem_bounds
      # lower_sem_mfpts = shortest_run_time_values / lower_sem_bounds

      # because the lower standard error bound is lower than the average
      # we want to make sure the call to isclose doesn't count them as 0
      # so we make a new min_prob for the masking that takes into
      # account the lowest value. So we take the minimum value that is
      # not identically 0
      # lower_min_prob = lower_sem_bounds[lower_sem_bounds > 0.].min() * 1e-2

      # actually we are going to consder anythin 2 orders of magnitude
      # lower than the minimum probability to be zero so:
      # lower_sem_bound_min_value = min_probability * 1e-2

      # zero_masked_upper_sem_weights = np.ma.masked_values(upper_sem_bounds, 0.,
      #                                                     atol=min_probability)

      # zero_masked_lower_sem_weights = np.ma.masked_values(lower_sem_bounds, 0.,
      #                                                     atol=lower_min_prob)

      # # we calculate the standard error of the mean weights as well
      # zero_masked_upper_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_upper_sem_weights)
      # zero_masked_lower_sem_mfpts = (shortest_run_time_values /
      #                                zero_masked_lower_sem_weights)

      # make the plots, one for aggregate probability the other for
      # RT
      fig, axes = plt.subplots(1, 2)

      prob_ax = axes[0]
      rt_ax = axes[1]

      for run_idx in run_target_weights_rates.keys():

          # do the weights plot

          # get the times and weights converting if necessary
          run_weights = runs_weights[run_idx]
          run_times = [time.value_in_unit(time_unit) for time in runs_times[run_idx]]

          # slice them to the smallest run length
          run_weights = run_weights[0:min_num_timepoints]
          run_times = run_times[0:min_num_timepoints]

          label = RUN_LABEL_TEMPLATE.format(run_idx)

          # TODO set color better
          prob_ax.plot(run_times, run_weights, label=label, linewidth='3')

          # then do the residence time plots

          run_rts = runs_rts[run_idx]

          # convert the residence times to the time unit specified
          run_rts = [rt * time_unit for rt in run_rts]

      shortest_run_times_values = [time.value_in_unit(time_unit) for time in shortest_run_times]
      prob_ax.plot(shortest_run_times_values,
                   mean_weights,
                   label='Mean', linewidth='3', color='black')
      prob_ax.fill_between(shortest_run_times_values,
                           mean_weights-sem_weights,
                           mean_weights+sem_weights,
                           label='Std. Error')

      # get the units from the value of the rates
      # rate_unit = runs_rates[list(runs_rates.keys())[0]][0].get_symbol()

      # # convert to the given unit as a rate
      # if time_unit is not None:
      #     rate_unit = 1 / (1 / rate_unit).in_units_of(time_unit)

      prob_ax.set_yscale('log')
      prob_ax.set_ylabel('Aggregated unbound probability')

      # TODO set this intelligently
      prob_ax.set_ylim([1e-13, 1e-2])

      prob_ax.set_xlabel('Simulation time (${}$)'.format(time_unit.get_symbol()))

      prob_ax.legend(loc='best')


      # plot the average rate

      # get the times and convert to the proper units
      times = [time.value_in_unit(time_unit) for time in shortest_run_times]

      # first because they were dedimensionalized we redimensionalize
      # them to the specified time_unit (x-axis) then convert them to
      # the MFPT unit, then dedimensionalize them to raw values for
      # plotting
      mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                              for mfpt in mfpts])

      upper_sem_mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                                        for mfpt in upper_sem_mfpts])

      lower_sem_mfpt_values = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                                        for mfpt in lower_sem_mfpts])

      # clip the things that are close to zero or less than zero since
      # this is just a floating point error from multiplying...
      close_to_zero_idx = np.argwhere(np.logical_or(np.isclose(lower_sem_mfpt_values, 0.),
                                                    lower_sem_mfpts < 0.))
      lower_sem_mfpt_values[close_to_zero_idx] = 0.


      rt_ax.plot(times, mfpt_values, label='Mean', linewidth='3', color='black')
      rt_ax.fill_between(times,
                         upper_sem_mfpt_values, lower_sem_mfpt_values,
                         label='Std. Error of Mean')

      if logscale:
          rt_ax.set_yscale('log')

      # plt.xticks([400, 800, 1200, 1600], ['400', '800', '1200', '1600'])
      # plt.yticks([0.1, 10, 1000, 100000, 10000000, 1000000000])
      rt_ax.set_xlabel('Simulation time ({})'.format(time_unit.get_symbol()))
      rt_ax.set_ylabel('Predicted residence time ({})'.format(mfpt_unit.get_symbol()))
      #rt_ax.set_ylim([1e-1, 1e9])

      # plot the experimental value
      experimental_points = [experimental_rt.value_in_unit(mfpt_unit) for _ in run_times]
      rt_ax.plot(times, experimental_points, color='r', label='Experimental')

      rt_ax.legend()

      # return the actual computed values for the average aggregated
      # weights and residence times
      mfpts = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                        for mfpt in mfpts]) * mfpt_unit

      mfpt_sems = np.array([(mfpt * time_unit).value_in_unit(mfpt_unit)
                            for mfpt in mfpt_sems]) * mfpt_unit



      return mean_weights, sem_weights, mfpts, mfpt_sems, shortest_run_times, (fig, prob_ax, rt_ax)

#+END_SRC

**** COMMENT Plot Data

Actually perform the plotting for different ligands and runs.

For this we want to plot each run independently and then aggregate
them.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def gexp_plot_rates_rt(gexp):

      import matplotlib.pyplot as plt

      from wepy.analysis.rates import contig_warp_rates

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # get the experimental mfpt/rt for this ligand in the correct unit
      experimental_rt = dict(experimental_mfpts())[lig_id] * EXPERIMENTAL_MFPT_UNIT

      contigtree = get_contigtree(gexp)
      set_recursion_limit()



      with contigtree:

          # get the rates struct for each spanning contig
          span_rates = {}
          for span_idx, span_trace in contigtree.span_traces.items():

              num_walkers = get_lig_num_walkers(gexp)

              # make the contig
              contig = contigtree.make_contig(span_trace)

              # get the values as a series
              contig_rates = contig_warp_rates(contig, CYCLE_TIME, time_points=Ellipsis)

              span_rates[span_idx] = contig_rates

          # make the plot for each span
          mean_weights, sem_weights, mfpts, mfpt_sems, total_sampling_times, axes = \
                                plot_rates(span_rates, CYCLE_TIME, num_walkers, experimental_rt,
                                           target_idx=0, min_probability=seh_params.PMIN,
                                           mfpt_unit=tkunit.minute,
                                           time_unit=tkunit.microsecond,
                                           logscale=True)
      print("Ligand: {}".format(lig_id))
      print("Final MFPT: {}".format(mfpts[-1]))
      print("Final MFPT SEM: {}".format(mfpt_sems[-1]))
      print("Total sampling time: {}".format(
          total_sampling_times[-1].value_in_unit(tkunit.microsecond)
          ,* tkunit.microsecond
          ,* len(contigtree.span_traces))
      )

      plt.show()
#+END_SRC



  def vmd_query_lig_bs_atom_pairs(
          gexp,
  ):
      """Print the VMD query for selecting the selected atoms for the ligand
      and binding site pairing.
      """

      # TODO

      # verify visually so print out the VMD query for the selection of
      # atoms for both the ligand and the protein
      vmd_q_str = "index {}"
      print("Protein selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in neighbors])))
      print("Ligand selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in query_idxs])))

  @jlmem.cache
  def lig_bs_atom_pairs(
          gexp,
  ):
      """Get the pairs of atoms between a selection of atoms on the ligand
      and the selected binding site atoms.

      """

      from itertools import product

      import numpy as np

      import simtk.unit as unit
      import mdtraj as mdj

      from wepy.util.mdtraj import json_to_mdtraj_topology
      from wepy.util.util import box_vectors_to_lengths_angles

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # Get the atoms on the binding site
      # TODO:

      # these are the indices chosen on the ligand. They are
      # "homological" because they are matched across the different
      # ligands to be the same
      hom_idxs = dict(LIGAND_HOMOLOGY_INDICES)[lig_id]
      hom_idxs = list(hom_idxs)

      pairs = np.array(list(product(hom_idxs, bs_idxs)))

      return pairs


  @jlmem.cache
  def lig_bs_cutoff_atom_pairs(
          gexp,
          cutoff=0.8 * tkunit.nanometer
  ):
      """Get the pairs of atoms between a selection of atoms on the ligand
      and any atoms on the protein within the given cutoff distance."""

      from itertools import product

      import numpy as np

      import simtk.unit as unit
      import mdtraj as mdj

      from wepy.util.mdtraj import json_to_mdtraj_topology
      from wepy.util.util import box_vectors_to_lengths_angles

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # these are the indices chosen on the ligand. They are
      # "homological" because they are matched across the different
      # ligands to be the same
      hom_idxs = dict(LIGAND_HOMOLOGY_INDICES)[lig_id]
      hom_idxs = list(hom_idxs)

      state = get_ref_state(gexp)

      # TODO: main_rep here is being used everywhere so we continue
      # using it.

      # positions are just the full atomic that way we can use them on full frames
      positions = state['positions'][lig_selection_idxs(lig_id)['all_atoms/main_rep']]

      # get the box lengths and angles
      box_lengths, box_angles = box_vectors_to_lengths_angles(state['box_vectors'])

      # get the right trajectory
      mdj_top = json_to_mdtraj_topology(lig_selection_tops(lig_id)['main_rep'])

      ref_frame = mdj.Trajectory(positions, mdj_top,
                                 unitcell_lengths=box_lengths,
                                 unitcell_angles=box_angles)

      # get the indices of the selected ligand atoms in the main_rep
      # structure
      query_idxs = lig_selection_idxs(lig_id)['main_rep/ligand'][hom_idxs]

      prot_idxs = lig_selection_idxs(lig_id)['main_rep/protein']

      # compute the neighboring atoms to the selected ones, the cutoff
      # should be converted to the appropriate units
      neighbors = mdj.compute_neighbors(
          ref_frame,
          cutoff.value_in_unit(state.positions_unit),
          query_idxs,
          haystack_indices=prot_idxs,
      )[0]

      # verify visually so print out the VMD query for the selection of
      # atoms for both the ligand and the protein
      vmd_q_str = "index {}"
      print("Protein selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in neighbors])))
      print("Ligand selection indices: ",
            vmd_q_str.format(' '.join([str(i) for i in query_idxs])))

      pairs = np.array(list(product(query_idxs, neighbors)))

      return pairs

*** Observables

**** Test: Box Volumes



#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def box_volume_observable(fields):

      import numpy as np

      from wepy.util.util import traj_box_vectors_to_lengths_angles

      all_lengths, _ = traj_box_vectors_to_lengths_angles(fields['box_vectors'])

      volumes = np.multiply.reduce(all_lengths, axis=1)

      print("Computing on a chunk")

      return volumes
#+END_SRC

**** Ligand RMSD

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def lig_bs_rmsd_observable(lig_id, ligand_idxs, fields):

      import numpy as np

      from geomm.rmsd import calc_rmsd

      from wepy.util.util import traj_box_vectors_to_lengths_angles

      print("starting chunk calculation")

      # recenter the whole protein-ligand complex into the center of
      # the periodic boundary conditions, this uses the whole set of
      # receptor atom indices and not just the binding site.

      # TODO: do we need main_rep here? doesn't matter too much and I
      # already computed this

      # then also superimpose everything to the reference state binding
      # site indices
      sup_positions, ref_positions = recenter_superimpose_traj(
          fields,
          lig_id,
          'main_rep'
      )

      # then calculate the RMSD of the ligand of all the frames to the

      # reference state
      lig_rmsds = np.array([
          calc_rmsd(
              ref_positions,
              frame,
              idxs=ligand_idxs,
          )
          for frame in sup_positions])

      print("ending chunk calculation")

      return lig_rmsds
#+END_SRC


**** Ligand SASA

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def lig_sasa_observable(
          lig_id,
          ligand_idxs,
          json_top,
          n_sphere_points,
          traj_fields):

      import time

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      import mdtraj as mdj

      shape = traj_fields['positions'].shape

      print(f"Computing SASAs for a chunk of size: {shape}")

      start = time.time()

      # ALERT: this needs to be main rep here for the calculation
      sup_positions, ref_positions = recenter_superimpose_traj(
          traj_fields,
          lig_id,
          'main_rep'
      )

      traj_fields['positions'] = sup_positions

      traj = traj_fields_to_mdtraj(traj_fields, json_top)

      # compute sasas with all atoms present then slice the ligand idxs
      # off, then sum up the values for each atom in a frame
      sasas = mdj.shrake_rupley(
          traj,
          n_sphere_points=n_sphere_points,
      )[:,ligand_idxs].sum(axis=1)

      end = time.time()

      duration = end - start

      print(f"Finished Chunk, took: {duration}")

      return sasas
#+END_SRC

**** BS-Lig Pair Distances

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def lig_prot_atom_pair_observable(pair_idxs, top, fields):

      from msmbuilder.featurizer import AtomPairsFeaturizer

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      n_frames = fields['positions'].shape[0]

      print(f"Starting a chunk, with # of frames: {n_frames} ")

      featurizer = AtomPairsFeaturizer(pair_idxs, periodic=True)

      # convert the fields to an mdtraj trajectory
      traj = traj_fields_to_mdtraj(fields, top)

      features = featurizer.partial_transform(traj)

      print(f"finished a chunk, with # of frames: {n_frames}")

      return features
#+END_SRC


**** TODO PCA Projections

NOTE: We eschew and defer a naming scheme for this particular analysis
even though this is parametrized by the specific TS model we are
using.


Just return it as a single vector. We will split it up etc as we need
to and separate the modes.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def pc_projections_observable(lig_id, hom_idxs, model, fields):

      from geomm.centroid import centroid

      # TODO: do we need the main reps here because this is about
      # homology and that is special
      hom_positions = recenter_superimpose_traj(
          fields,
          lig_id,
          'main_rep'
      )[0][:,hom_idxs,:]

      coms = np.array([centroid(frame) for frame in hom_positions])

      center_projections = model.transform(coms)

      return center_projections
#+END_SRC


**** TODO (Legacy) PCA Projections

We have a special function for the projection with the legacy data.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def legacy_pc_projections_observable(lig_id, hom_idxs, model, fields):

      import numpy as np

      import mdtraj as mdj

      from wepy.util.util import traj_box_vectors_to_lengths_angles

      from geomm.superimpose import superimpose
      from geomm.grouping import group_pair
      from geomm.centering import center_around
      from geomm.centroid import centroid

      sel_idxs = lig_selection_idxs(lig_id)

      lig_idxs = sel_idxs['correct_rep/ligand']
      prot_idxs = sel_idxs['correct_rep/protein']
      bs_idxs = sel_idxs['correct_rep/binding_site']

      box_lengths, _ = traj_box_vectors_to_lengths_angles(fields['box_vectors'])

      ref_traj = mdj.load_pdb(osp.join(data_path(), 'top/{}/real_rep_center_ref.pdb'.format(lig_id)))

      centered_ref_positions = ref_traj.xyz[0]

      ## regroup, center, and superimpose the frames

      # group the pair of ligand and binding site together in the same image
      grouped_positions = [group_pair(positions, box_lengths[idx],
                                          bs_idxs, lig_idxs)
                    for idx, positions in enumerate(fields['positions'])]

      # center all the positions around the binding site
      centered_positions = [center_around(positions, bs_idxs)
                            for idx, positions in enumerate(grouped_positions)]

      # then superimpose the binding sites
      sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=bs_idxs)[0]
                       for pos in centered_positions])


      hom_positions = sup_positions[:,hom_idxs,:]

      coms = np.array([centroid(frame) for frame in hom_positions])

      center_projections = model.transform(coms)

      return center_projections
#+END_SRC


*** Free Energy Profiles

A typical analysis is to plot the free-energy over some projection of
the data.

**** Plotting Functions

There are few different kinds of plots for Free Energy Profiles (FEP).

First we can plot data for a specific GEXP alone (this might require
all the data to get the bins the same though) this is.

Intra-GEXP plots:

- [X] Span FEPs :: FEPs for each span/replicate of a GEXP
- [X] Convergence Span FEPs :: FEPs over a time series of partial data to
  observe convergence.
- [ ] Agg. FEP :: The FEP from aggregating each replicate/span. Not an
  expected value estimate.
- [ ] Agg. Convergence FEPs :: The convergence of the aggregate of the
  replicates.
- [X] Trace FEP :: Plots the aggregate FEP for a subselection of a
  contigtree dataset. Not useful on its own.

Inter-GEXP plots:

- [X] Agg. FEP comparison :: Comparison of the aggregate FEPs across
  different contigtree GEXPs


These are all for 1D plots of free energy. For 2D we have

- [ ] 


***** Common Functions

Common functions for plotting free energy profiles. These are just for
actually rendering the plots and not calculating the free energy
profiles.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  MAX_FE_BUFFER = 5
  MIN_FE_AXIS_VALUE = 0.0


  def plot_fe_profile(
          fe_profile,
          bin_edges,
          max_fe=None,
          title="Free Energy Profile",
          observable_label="Observable",
  ):
      """Plots a single, already calculated free energy profile and
      binning spec.

      Used only for configuring the actual plot detailes like axes and
      labels.

      See Also
      --------

      plot_fe_profiles :: for plotting multiple curves at once
      """

      import numpy as np
      import matplotlib.pyplot as plt



      # if no maximum free energy is given set it to the maximum valid
      # value
      if max_fe is None:
          max_fe = np.ma.masked_invalid(fe_profile).max()

      bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                              for i in range(bin_edges.shape[0] - 1)])

      # construct the Figure and Axes objects. We want to be able to
      # collect the individual Axes objects into a single Figure later
      # or have a standalone figure
      fig, ax = plt.subplots()

      # add the figure Artists

      # add a super title to the figure
      title = fig.suptitle(title)

      # add the axes artists
      artists = ax.plot(bin_centers, fe_profile)

      ax.set_xlim(left=bin_edges[0], right=bin_edges[-1])
      ax.set_ylim(bottom=MIN_FE_AXIS_VALUE, top=max_fe+MAX_FE_BUFFER)

      ax.set_xlabel(observable_label)
      ax.set_ylabel("Free Energy ($-ln(p)$)")

      return fig, ax

  def plot_fe_profiles(
          fe_profiles,
          bin_edges,
          max_fe=None,
          labels=None,
          title="Free Energy Profiles",
          observable_label="Observable",
  ):
      """Plot multiple already calculated FE curves.

      This is just for configuring details of presentation.

      """

      import numpy as np
      import matplotlib.pyplot as plt

      # if no maximum free energy is given set it to the maximum valid
      # value
      if max_fe is None:
          max_fe = np.ma.masked_invalid(np.concatenate(fe_profiles)).max()

      # get the bin centers for plotting a line graph over the bins
      bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                              for i in range(bin_edges.shape[0] - 1)])


      # construct the Figure and Axes objects. We want to be able to
      # collect the individual Axes objects into a single Figure later
      # or have a standalone figure
      fig, ax = plt.subplots()
      title = fig.suptitle(title)

      # if this is the last curve make the label show the true number of
      # cycles as the upper limit
      for curve_idx, profile in enumerate(fe_profiles):

          if labels is None:
              label = str(curve_idx)
          else:
              label = labels[curve_idx]

          artists = ax.plot(bin_centers, profile,
                            label=label)


      ax.set_xlim(left=bin_edges[0], right=bin_edges[-1])
      ax.set_ylim(bottom=MIN_FE_AXIS_VALUE, top=max_fe + MAX_FE_BUFFER)

      ax.set_xlabel(observable_label)
      ax.set_ylabel("Free Energy ($-ln(p)$)")

      artists.append(ax.legend())

      return fig, ax

#+END_SRC


***** Contigtree Spans

Plot a free energy profile for each span (replicate) in a gexp dataset.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_lig_obs_fe_spans(
          gexp,
          field_key,
          observable_label,
          bin_method='auto',
  ):
      """Calculate a FE profile for each replicate/span in a GEXP dataset."""

      from wepy.analysis.profiles import ContigTreeProfiler, contigtrees_bin_edges

      contigtree = get_contigtree(gexp)
      set_recursion_limit()
      with contigtree:

          profiler = ContigTreeProfiler(contigtree)

          # get the bin edges for the whole contig
          bin_edges = profiler.bin_edges(bin_method, field_key)
          print("Num bins {}".format(len(bin_edges) - 1))

          bin_centers = bin_centers_from_edges(bin_edges)

          # profile the finale FE profile spans for the ligand
          span_profiles = []
          for span_idx in profiler.contigtree.span_traces.keys():
              print("span {}".format(span_idx))
              # then make a fe profile with those bins for the first span
              fe_profile = profiler.fe_profile(span_idx, field_key, bins=bin_edges)

              span_profiles.append(fe_profile)

          fig, ax = plot_fe_profiles(span_profiles, bin_edges,
                                     title="GEXP {} span FE profiles".format(gexp),
                                     observable_label=observable_label)

      return fig, ax




#+END_SRC


***** Spans Convergence Plots

Plot the free energy over a time series for each of the spans in the
contigtrees.

The contigtrees are taken as a collection so that we can generate the
bin edges properly.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_contigtrees_spans_observable_convergence(
          contigtrees,
          field_key,
          obs_label,
          num_partitions=5,
          bin_method='sqrt',
  ):
      """Plot a timeseries of free energy profiles for every
      span/replicate.

      Specify how many timepoints you want with the 'num_partitions'.

      """

      import numpy as np
      import matplotlib.pyplot as plt

      from wepy.analysis.profiles import ContigTreeProfiler, contigtrees_bin_edges


      # get the bin edges for all of the contigtrees for all ligands
      bin_edges = contigtrees_bin_edges(contigtrees,
                                        bin_method,
                                        field_key)

      # then get the profiles for each span of each contig
      contigtree_span_convergences = []
      contigtree_span_cum_num_cycles = []
      for contigtree in contigtrees:

          with contigtree.wepy_h5:

              profiler = ContigTreeProfiler(contigtree)

              # save the convergences for each span
              spans_convergences = []
              spans_cum_num_cycles = []

              # iterate over the spans in sorted order
              span_idxs = list(contigtree.span_traces.keys())
              span_idxs.sort()

              for span_idx in span_idxs:

                  fe_profiles, num_cycles = \
                      profiler.fe_cumulative_profiles(
                          span_idx,
                          field_key,
                          bins=bin_edges,
                          num_partitions=num_partitions,
                          ignore_truncate=True
                      )

                  spans_convergences.append(fe_profiles)
                  spans_cum_num_cycles.append(num_cycles)

              contigtree_span_convergences.append(spans_convergences)
              contigtree_span_cum_num_cycles.append(spans_cum_num_cycles)

      # get the maximum FE from all of the simulations
      max_fe = np.ma.masked_invalid(
          [np.concatenate(
              [np.concatenate(profiles) for profiles in spans])
           for spans in contigtree_span_convergences]
          ).max()

      figs = []
      for contigtree_idx, spans_convergences in enumerate(contigtree_span_convergences):

          for span_idx, fe_profiles in enumerate(spans_convergences):

              num_cycles = contigtree_span_cum_num_cycles[contigtree_idx][span_idx]

              labels = ["Cycles: {}-{}".format(0, num_cycles[curve_idx])
                        for curve_idx, fe_profile in enumerate(fe_profiles)]

              title = \
          f"Free Energy Convergence of {obs_label}: GEXP: {contigtree_idx} span {span_idx}"

              fig, ax = plot_fe_profiles(
                  fe_profiles,
                  bin_edges,
                  max_fe=max_fe,
                  title=title,
                  labels=labels)

              figs.append((fig, ax))

      return figs


#+END_SRC

***** Free Energy Profile all data

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  @jlmem.cache
  def contigtrees_observable_fe(
          contigtrees,
          field_key,
          bin_method='auto',
  ):

      import numpy as np
      import matplotlib.pyplot as plt

      from wepy.analysis.profiles import (
          ContigTreeProfiler,
          contigtrees_bin_edges,
      )

      # DEBUG
      print("calculating bin edges across all the gexps")

      # get the bin edges for all of the contigtrees for all ligands
      bin_edges = contigtrees_bin_edges(
          contigtrees,
          bin_method,
          field_key,
      )

      gc.collect()

      # DEBUG
      print("finished calculating bin edges across all the gexps")


      # DEBUG
      print("Calculating FE profiles for each GEXP contigtree")

      # then get the profiles for each span of each contig
      fe_profiles = []
      for idx, contigtree in enumerate(contigtrees):

          # DEBUG
          print(f"Calculating for GEXP contigtree: {idx}")


          with contigtree.wepy_h5:

              profiler = ContigTreeProfiler(contigtree)

              fe_profile = profiler.fe_profile_all(field_key, bins=bin_edges)

              fe_profiles.append(fe_profile)

          # DEBUG
          print(f"Finished calculating for GEXP contigtree: {idx}")

      # DEBUG
      print(f"Finished calculating for all gexps")


      return fe_profiles, bin_edges
#+END_SRC


***** Trace FE Profile

This is supposed to plot the FE for only a specific part of a
contigtree.

This can be used to select a subset of dataset to show a FEP of.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def plot_contigtrees_trace_observable_fe(
          contigtrees,
          traces,
          observable_key,
          bin_method='sqrt',
  ):
      """Generate a FE profile for each contigtree of only the provided
      trace.

      """

      import numpy as np
      import matplotlib.pyplot as plt

      from wepy.analysis.profiles import ContigTreeProfiler, contigtrees_bin_edges


      field_key = "observables/{}".format(observable_key)

      # get the bin edges for all of the contigtrees for all ligands
      bin_edges = contigtrees_bin_edges(contigtrees,
                                        bin_method, field_key)

      # then get the profiles for each span of each contig
      fe_profiles = []
      for tree_idx, contigtree in enumerate(contigtrees):

          with contigtree.wepy_h5:

              profiler = ContigTreeProfiler(contigtree)

              fe_profile = profiler.fe_profile_trace(traces[tree_idx], field_key,
                                                     bins=bin_edges)

              fe_profiles.append(fe_profile)

      return fe_profiles, bin_edges

  # TODO: not sure this is even necessary as its own function

  # def plot_contigtrees_trace_observable_fe(
  #         fe_profiles,
  #         bin_edges,
  #         observable_key,
  #         trace_title=None,
  # ):
  #     """Plot FE profiles for 
  #     """

  #     # get the maximum FE from all of the simulations
  #     max_fe = np.ma.masked_invalid(np.concatenate(fe_profiles)).max()

  #     for contigtree_idx, fe_profile in enumerate(fe_profiles):

  #         labels = ["{}".format(contigtree_idx)]

  #         title = "Free Energy {}".format(observable_key)

  #         fig, ax = plot_fe_profiles(
  #             fe_profiles,
  #             bin_edges,
  #             max_fe=max_fe,
  #             title=title,
  #             labels=labels,
  #         )

#+END_SRC



**** Plot Data

Data and function for plotting for specific observables. Generates
plots but doesn't render.

***** Intra-GEXP Profiles

****** Specs

Specifications for each observable about how to make profiles and plot
them.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  OBS_SPAN_FE_OPTS = {
        'lig_rmsd' : {
            'label' : "Ligand RMSD $\AA$",
            'bin_method' : 'auto',
        },

        'lig_sasa' : {
            'label' : "Ligand SASA $\AA^{2}$",
            'bin_method' : 'auto',
        },
  }

  GEXP_SPAN_FE_OPTS = {
      '3' : {
          'convergence_n_partitions' : 5,
      },

      '10' : {
          'convergence_n_partitions' : 5,
      },

      '17' : {
          'convergence_n_partitions' : 5,
      },

      '18' : {
          'convergence_n_partitions' : 5,
      },

      '20' : {
          'convergence_n_partitions' : 5,
      },
  }
#+end_src

****** Span FEPs

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_gexp_obs_span_fe(
          gexp,
          observable,
  ):

      field_key = f'observables/{observable}'

      OBS_OPTS = OBS_SPAN_FE_OPTS[observable]

      figs, axs = plot_lig_obs_fe_spans(
          gexp,
          field_key,
          OBS_OPTS['label'],
          bin_method=OBS_OPTS['bin_method'],
      )

      return figs, axs

#+END_SRC


****** Convergence Span FEPs

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_gexp_obs_span_convergence_fe(
          gexp,
          observable,
  ):

      field_key = f'observables/{observable}'

      OBS_OPTS = OBS_SPAN_FE_OPTS[observable]
      GEXP_OPTS = GEXP_SPAN_FE_OPTS[gexp]

      # get the contigtree
      contigtree = get_contigtree(gexp)

      figs = plot_contigtrees_spans_observable_convergence(
          [contigtree],
          field_key,
          OBS_OPTS['label'],
          num_partitions=GEXP_OPTS['convergence_n_partitions'],
          bin_method=OBS_OPTS['bin_method'],
      )

      # there is just one here so pop that out for return as: fig, ax
      return figs

#+END_SRC

****** TODO Agg. FEP

****** TODO Agg. Convergence FEPs

***** Inter-GEXP Profiles

****** Agg. FEP Comparison

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  FE_ALL_AGG_OPTS = {

      # which gexps to include
      'gexps' : ['3', '10', '17', '18', '20'],

      # options for each observable
      'obs_opts' : {

          'lig_rmsd' : {
              'bin_method' : 'auto',
          },
      },

  }

  def plot_all_agg_fe_obs(observable_key):

      field_key = "observables/{}".format(observable_key)

      # get the gexp contigtrees requested
      contigtrees = [get_contigtree(gexp) for gexp in FE_ALL_AGG_OPTS['gexps']]

      # get the options for this observable
      OBS_OPTS = FE_ALL_AGG_OPTS['obs_opts'][observable_key]

      # compute the profiles and the bin edges for all of GEXP
      # contigtrees
      fe_profiles, bin_edges = contigtrees_observable_fe(
          contigtrees,
          field_key,
          bin_method=OBS_OPTS['bin_method'],
      )

      # now plot the fe profiles

      title = "Free Energy {}".format(observable_key)
      labels = [f"{gexp}" for gexp in FE_ALL_AGG_OPTS['gexps']]

      # get the maximum FE from all of the simulations
      max_fe = np.ma.masked_invalid(np.concatenate(fe_profiles)).max()

      fig, ax = plot_fe_profiles(
          fe_profiles,
          bin_edges,
          max_fe=max_fe,
          title=title,
          labels=labels,
      )

      return fig, ax
#+end_src

**** Rendering

***** Intra-GEXP Profiles

Functions for the actual rendering of the generated plots, i.e. to the
screen with pyplot or to a media file.

****** Spans

This renders a single plot per GEXP with all spans on it.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def gexp_show_plot_spans_fe_obs(gexp, observable):
      """Render the Spans/Replicates chosen plot for the GEXP"""

      import matplotlib.pyplot as plt

      fig, axs = plot_gexp_obs_span_fe(gexp, observable)

      print(f"Obs: {observable}  Gexp: {gexp}")

      plt.show()

  def gexp_save_plot_spans_fe_obs(gexp, observable):

      import matplotlib.pyplot as plt

      fig, axs = plot_gexp_obs_span_fe(gexp, observable)

      print(f"Obs: {observable}  Gexp: {gexp}")

      save_gexp_fig(gexp, f'fe_profiles/spans/{observable}', fig)
#+end_src


****** Span Convergence

This renders a single plot per GEXP with all spans on it.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def gexp_show_plot_spans_convergence_fe_obs(gexp, observable):
      """Render the Spans/Replicates chosen plot for the GEXP"""

      import matplotlib.pyplot as plt

      figs = plot_gexp_obs_span_convergence_fe(gexp, observable)

      print(f"Obs: {observable}  Gexp: {gexp}")

      plt.show()

  def gexp_save_plot_spans_convergence_fe_obs(gexp, observable):

      import matplotlib.pyplot as plt

      figs = plot_gexp_obs_span_convergence_fe(gexp, observable)

      print(f"Obs: {observable}  Gexp: {gexp}")

      for span_idx, figax in enumerate(figs):
          fig, ax = figax
          print(f"Span Idx: {span_idx}")

          save_gexp_fig(
              gexp,
              f'fe_profiles/spans_convergence/{observable}',
              fig,
              tags={'span' : f'{span_idx}'}
          )
#+end_src


***** Inter-GEXP Profiles

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def all_render_plot_agg_fe_obs(observable,
                                 save=False,
                                 show=True,
  ):
      """Render the Aggregate FEP for the observable of all GEXPs
      together.

      If save and show are both turned on will show first, then save
      that way you can Ctrl-C to interrupt saving if you don't like what
      it showed. (or just rerun).

      """


      fig, ax = plot_all_agg_fe_obs(observable)

      print(f"All Agg FE profile for Obs: {observable}")

      if show:
          import matplotlib.pyplot as plt
          plt.show()

      if save:
          save_fig(
              f'fe_profiles/all_agg/{observable}',
              f"{observable}",
              fig,
          )

#+end_src

**** TS PCA Projections

***** Spans FE

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_ts_pc0_span_fe(lig_id):

      import matplotlib.pyplot as plt

      field_key = 'observables/TS-warp+featdist-0.07_pc-0'
      bin_method = 'auto'
      observable_label = "TS PC-0"

      print("Plotting")
      figs, axs = plot_lig_obs_fe_spans(lig_id, field_key, observable_label, bin_method=bin_method)

      plt.show()

  def plot_ts_pc1_span_fe(lig_id):

      import matplotlib.pyplot as plt

      field_key = 'observables/TS-warp+featdist-0.07_pc-1'
      bin_method = 'auto'
      observable_label = "TS PC-1"

      print("Plotting")
      figs, axs = plot_lig_obs_fe_spans(lig_id, field_key, observable_label, bin_method=bin_method)

      plt.show()


#+END_SRC


*** Clustering

**** Specs

There are a number of ways we can cluster things and we want to be
able to do some trial and error to see if we are getting a good one.

Here are the specs for the different methods along with there gexps,
clustering gexps will be called 'clust-gexps'.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  from sklearn.cluster import (
      MiniBatchKMeans,
      KMeans,
  )
  from msmbuilder.cluster.kcenters import _KCenters as KCenters


  # CLUST_METHODS = {
  #     'KCenters' : KCenters,
  #     'MiniBatchKMeans' : MiniBatchKMeans,
  # }

  # CLUST_METRICS = {
  #     'canberra' : 'canberra',
  #     'euclidean' : 'euclidean',
  # }

  # this is the configuration used in the ACS poster etc.
  OLD_CLUSTER_SPECS = {

      'subsampling' : {
          'test_proportion' : 0.5,
      },
      'clust_method' : 'KCenters',
      'params' : {
          'metric' : 'canberra',
          'random_state' : 1,
          'n_clusters' : 1000,
      },
  }


  # "Normal" Clustering Specs, the key is the classifier ID
  CLUSTER_SPECS = {

      '0' : {
          'clf_class' : KCenters,
          'clf_kwargs' : {
              'n_clusters' : 1000,
              'metric' : 'canberra',
              'random_state' : 1,
          },
          'splitter_class' : None,
          'splitter_kwargs' : None,
      },
  }
#+END_SRC


**** Performing Clustering From Specs

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  # this is used as a "null" splitting, so that we handle split and
  # unsplit data the same
  class NoSplit():
      """Splitter implementing API, but doesn't split at all. All features
      go to training."""

      def __init__(self, **kwargs):

          pass

      def split(self, X, **kwargs):

          train_idxs = list(range(len(X)))
          test_idxs = []

          # yield here to satisfy the splitter API, we only have one set
          # to do, unlike other splitters
          yield train_idxs, test_idxs

  class StaticSplit():
      """Split according to a predetermined set of splits."""

      def __init__(self,
                   splits,
                   ,**kwargs):
          """

          Parameters
          ----------

          splits : list of (list of int, list of int)
              The splits that you want to have it yield.

          """

          self._splits = splits

      def split(self, X, **kwargs):

          # just yield the ones we stored
          for split in self._splits:
              yield split


  def gexp_classify_observable(
          gexp,
          observable_key,
          clf,
          clf_id,
          splitter=None,
  ):
      """Classify an observable given a classifier.

      Optionally, provide a splitter to partition the data into train
      test. This isn't for testing, but just efficiency of not having to
      cluster everything. That means it will just grab the first splitting
      That the splitter generates.

      """

      # if no splitting was request we just use the NoSplit, which has
      # the same API.
      if splitter is None:
          splitter = NoSplit()

      # get the observables, reshape them to features, and vectorize them if needed
      all_features = unscalarize_features(
          reshape_run_to_features(
              get_observable(
                  observable_key,
                  gexp,
                  source='h5',
              )))

      # we only use the first splitting.
      train_idxs, test_idxs = next(splitter.split(all_features))

      # we return it as a dict
      train_test_d = {
          'training_idxs' : train_idxs,
          'testing_idxs' : test_idxs,
      }

      # we need to clean up memory so remove all_features from memory
      del all_features
      gc.collect()


      # we get the train features into memory
      train_features = unscalarize_features(
          reshape_run_to_features(
              get_observable(
                  observable_key,
                  gexp,
                  source='h5',
              )
          )
      )[train_idxs]

      ## do the classification
      print("Running the Fitting")
      start = time.time()

      clf.fit(train_features)

      end = time.time()

      duration_s = end - start
      duration_m = duration_s / 60
      duration_h = duration_m / 60

      print("Finished fitting")
      print(f"Took: {duration_s} s OR {duration_m} m OR {duration_h} h")

      # retrive and name training labels
      train_labels = clf.labels_

      del train_features
      gc.collect()

      # now we can get the test features and classify them
      test_features = unscalarize_features(
          reshape_run_to_features(
              get_observable(
                  observable_key,
                  gexp,
                  source='h5',
              )
          )
      )[test_idxs]

      # Classify the test features

      print("Classifying Test Features")

      # only do this if there are any to do it on
      if len(test_features) > 0:
          test_labels = clf.predict(test_features)
      else:
          test_labels = []

      print("Finished Classifying Test Features")

      del test_features
      gc.collect()

      # we can unshuffle the observables using the indices and put them
      # back how we need them for them to be observables.
      labels = unshuffle_features(
          train_idxs,
          test_idxs,
          train_labels,
          test_labels,
      )

      return labels, clf, train_test_d

  def do_gexp_classification(
          gexp,
          observable_key,
          clf_id,
  ):

      # get the spec for this classifier id
      clf_specs = CLUSTER_SPECS[clf_id]

      # create the classifier and splitter given the spec
      clf = clf_specs['clf_class'](**clf_specs['clf_kwargs'])

      if clf_specs['splitter_class'] is not None:
          splitter = clf_specs['splitter_class'](**clf_specs['splitter_kwargs'])
      else:
          splitter = None

      # perform the classification for an observable and a gexp
      assignments, clf, train_test_d = gexp_classify_observable(
          gexp,
          observable_key,
          clf,
          clf_id,
          splitter=splitter,
      )


      # make the assignment labels the proper shape to set as an
      # observable
      assignments_obs = reshape_features_to_run(
          assignments,
          gexp,
      )

      # save the assignments

      # as an observable file
      assg_obs_name = f"clust-assigs_clfid-{clf_id}"

      print(f"Saving assignments as observable {assg_obs_name}")
      save_observable(
          assg_obs_name,
          gexp,
          assignments_obs,
      )

      # then attach it to the HDF5 as an observable

      # first check if its in the HDF5, if it is delete it then add it
      wepy_h5 = get_gexp_wepy_h5(gexp)
      delete = False
      with wepy_h5:
          # see if its in the observable names
          if assg_obs_name in wepy_h5.observable_field_names:
              delete = True

      if delete:
          delete_wepy_h5_observable(
              gexp,
              assg_obs_name,
          )

      print("Saving to HDF5")
      attach_observable(
          assg_obs_name,
          gexp,
      )

      # serialize the splitting for this classifier so we can get the
      # cluster centers and stuff back out
      print("Saving train-test splits")
      save_clustering_traintest(
          clf_id,
          gexp,
          train_test_d['training_idxs'],
          train_test_d['testing_idxs'],
      )

      print("Saving clustering model")
      save_clustering_model(
          clf_id,
          gexp,
          clf,
      )
#+end_src

**** Mapping Classifier Feature Indices to WE traces

These are some methods for converting the indices of the classifier's
feature vectors to the indices and traces which are relevant for
actually extracting data from the WepyHDF5 datasets.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def unshuffle_features(
          train_idxs,
          test_idxs,
          train_features,
          test_features,
  ):
      """Unshuffle all of the features (which is both the full set of
      training and test features) features given the splitting indices.

      We need to shuffle our data for splitting training and testing
      data, this is to unshuffle them.

      """

      n_samples = len(train_idxs) + len(test_idxs)

      unshuffled_features = np.empty(
          (n_samples, *train_features.shape[1:]),
          dtype=train_features.dtype)

      for idx, shuf_idx in enumerate(train_idxs):
          unshuffled_features[shuf_idx] = train_features[idx]

      for idx, shuf_idx in enumerate(test_idxs):
          unshuffled_features[shuf_idx] = test_features[idx]

      return unshuffled_features

  def unshuffle_feature_idxs(
          shuffle_mapping,
          shuffled_features_idxs,
  ):
      """For a list feature idxs find their unshuffled feature idxs."""

      # feature_idxs = [None for i in range(len(shuffled_features_idxs))]

      feature_idxs = []
      for shuffled_feature_idx in shuffled_features_idxs:

          # find the feature index from the shuffled feature
          feature_idx = shuffle_mapping.index(shuffled_feature_idx)

          feature_idxs.append(feature_idx)

      return feature_idxs

  def get_unshuffled_feature_idxs(
          clf_id,
          gexp,
          clf_feature_idxs,
  ):

      # get the splitting and the training index, shuffling for this
      # classifier
      train_test_d = get_clustering_traintest(
          clf_id,
          gexp,
      )

      return unshuffle_feature_idxs(
          train_test_d['training_idxs'],
          clf_feature_idxs,
      )

  def clf_features_to_run_trace_idxs(
          gexp,
          clf_id,
          clf_feature_idxs,
  ):
      """Convert a list of features relative to the classifier to a
      equivalent run idxs (i.e. (run, traj, cycle)).

      """

      # because the clf_feature_idxs are in terms of the shuffled
      # features we need to get them unshuffled
      feature_idxs = get_unshuffled_feature_idxs(
          clf_id,
          gexp,
          clf_feature_idxs,
      )

      trace = []
      for node_idx, feature_idx in enumerate(feature_idxs):

          # using the unshuffled feature idx, get the trace index:
          # i.e. (run, traj, cycle)
          trace_idx = feature_idx_to_trace_idx(feature_idx, gexp)

          trace.append(trace_idx)

      return trace


#+end_src

*** Macro State Networks (MSNs)

This is the raw models of a network over the actual simulation data.

This will create the counts matrix, but will not calculate transition
probabilities for you, see this section for that: [[*Markov State Modelling (MSMs)][Markov State
Modelling (MSMs)]].

This also has other methods that only need MSNs and not MSMs to
calculate stuff on them such as:

- state trajectories
- state observables & stats
- free energies

**** Specs

Specs for making the MSN/CSNs

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  # MSM specs. Keys are 'msm_id'

  MSM_SPECS = {

      'cutoff-10' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder-unweighted',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 10,
          },
          'transition_prob_method' : 'nonreversible-normalize',
          'transition_prob_kwargs' : {
              'reversible' : False,
          },
      },

      'cutoff-5' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder-unweighted',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 5,
          },
          'transition_prob_method' : 'nonreversible-normalize',
          'transition_prob_kwargs' : {
              'reversible' : False,
          },
      },

      'cutoff-2' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder-unweighted',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 2,
          },
          'transition_prob_method' : 'nonreversible-normalize',
          'transition_prob_kwargs' : {
              'reversible' : False,
          },
      },

      'cutoff-1' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder-unweighted',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 1,
          },
          'transition_prob_method' : 'nonreversible-normalize',
          'transition_prob_kwargs' : {
              'reversible' : False,
          },
      },


  }

      ## weighted methods
  OLD_MSM_SPECS= {

      '0' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder', #'csnanalysis',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : seh_params.PMIN, # 1e-16
          },
          'transition_prob_method' : 'mle-pyemma',
          'transition_prob_kwargs' : {
              'reversible' : True,
              'eps_mu' : 1e-20,
              },
      },

      'pmin_irreversible' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder', #'csnanalysis',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : seh_params.PMIN, # 1e-16
          },
          'transition_prob_method' : 'mle-pyemma',
          'transition_prob_kwargs' : {
              'reversible' : False,
              'eps_mu' : 1e-20,
              },
      },

      'pmin_transpose' : {
          'csn_id' : '0',
          'trim_method' : 'msmbuilder', #'csnanalysis',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : seh_params.PMIN, # 1e-16
          },
          'transition_prob_method' : 'transpose',
          'transition_prob_kwargs' : {},
      },

      'big_cutoff' : {
          'csn_id' : '0',
          'lag_time' : 2,
          'obs_name' : 'clust-assigs_clfid-0',
          'clf_id' : '0',
          'trim_method' : 'msmbuilder', #'csnanalysis',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 1.0,
          },
          'transition_prob_method' : 'mle-pyemma',
          'transition_prob_kwargs' : {
              'reversible' : True,
              'eps_mu' : 1e-6,
          },
      },

      'small-cutoff_0' : {
          'csn_id' : '0',
          'lag_time' : 2,
          'obs_name' : 'clust-assigs_clfid-0',
          'clf_id' : '0',
          'trim_method' : 'msmbuilder', #'csnanalysis',
          'trim_kwargs' : {
              'ergodic_trim_cutoff' : 0.5e-11,
          },
          'transition_prob_method' : 'mle-pyemma',
          'transition_prob_kwargs' : {
              'reversible' : True,
              'eps_mu' : 1e-15,
          },
      },

  }
#+END_SRC


**** Constructing MSNs

A functional approach to making the CSN. See the loader for the cached
one with easy reference to saved labels etc.

For instance the functional one is used in the objective functions in
hyperparameter optimization.

***** Make MSNs from manually input labels

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def make_macro_state_network_label(
          gexp,
          label_features,
          lag_time=2
  ):
      """Create a MacroStateNetwork for a gexp data given some features
      which are labels.

      This is a minimal function and tries not to compute anything that
      isn't necessary over the network.

      Parameters
      ----------

      gexp : str

      label_features : nested lists of shape n_runs x n_trajs x n_frames
                       of arraylikes of shape feature_shape.

      lag_time : int

      """

      from wepy.analysis.network import MacroStateNetwork

      contigtree = get_contigtree(gexp)

      label_features = reshape_features_to_run(label_features,
                                               gexp)

      run_labels = label_features
      del label_features

      msn = MacroStateNetwork(contigtree,
                              transition_lag_time=lag_time,
                              assignments=run_labels,
                              )

      return msn

#+END_SRC


***** Make MSNs from observable labels

This is for making MSNs from the MSM_SPECS structures which are known
ahead of time.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def make_macro_state_network_obs(
          gexp,
          obs_name,
          lag_time=2
  ):
      """Create a MacroStateNetwork for a gexp data given a name of an
         observable that is a label.

      """

      from wepy.analysis.network import MacroStateNetwork

      assg_field_key = f"observables/{obs_name}"

      contigtree = get_contigtree(gexp)

      msn = MacroStateNetwork(contigtree,
                              transition_lag_time=lag_time,
                              assg_field_key=assg_field_key,
                              )

      return msn

#+END_SRC


**** CSN Specs

The CSN is the data associated with the MSN saved to different formats
for visualization.

The specs for them specify which labels to use and from which
classifier, and how to construct the counts of transitions.

#+begin_src python  :tangle src/seh_pathway_hopping/_tasks.py
  CSN_SPECS = {
      '0' : {
          'lag_time' : 2,
          'obs_name' : 'clust-assigs_clfid-0',
          'clf_id' : '0',
      }
  }
#+end_src


**** Create CSNs

This function will take the HDF5 data for the CSN specs and make the
MacroStateNetwork and then compute some basic CSN information for it.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  @jlmem.cache
  def make_basic_csn(
          gexp,
          csn_id,
  ):
      """Get the basic MSN that is derived directly from the classifier and
      simulation data.

      This will not have any additional computed fields and analyses for
      the nodes that may have been saved, see the `get_network` function for
      getting this one.

      """

      observable = CSN_SPECS[csn_id]['obs_name']
      lag_time = CSN_SPECS[csn_id]['lag_time']
      clf_id = CSN_SPECS[csn_id]['clf_id']

      # using the parameters in the CSN_SPEC make the msn
      msn = make_macro_state_network_obs(
          gexp,
          observable,
          lag_time=lag_time,
      )

      # then we do some basic post-processing

      # set the macrostate weights
      with msn:
          msn.set_macrostate_weights()

      # set the index of the cluster centers for each macrostate
      #
      # both the feature index (from the raw vector of features) and the
      # trace-like index (run, traj, frame)
      clf = get_clustering_model(
          clf_id,
          gexp,
      )

      # TODO: should probably just functionalize this as a pass that is
      # dependent on the clustering algorithm

      # the feature vector index (the concatenated features)

      # note that the feature idx is for how the features were
      # clustered, which may have been split and shuffled
      msn.set_nodes_attribute(
          'center_feature_idx',
          {
              node_idx : feature_idx
              for node_idx, feature_idx
              in enumerate(clf.cluster_ids_)
          }
      )

      # get the run trace index of each cluster center.

      # this is an index over the actual data set, i.e. (run, traj, cycle)
      msn.set_nodes_attribute(
          'center_idx',
          {
              node_idx : trace_idx
              for node_idx, trace_idx
              in enumerate(
                  clf_features_to_run_trace_idxs(
                      gexp,
                      clf_id,
                      clf.cluster_ids_
                  )
              )
          }
      )

      # calculate the free energies
      node_fes = calc_msn_free_energies(msn)

      # set them as node attributes
      msn.set_nodes_attribute('free_energy', node_fes)


      # then we do some basic groups

      # make a group which is the node with the native state

      # find the native node
      native_node_id = classify_native_state_cluster(
          clf_id,
          gexp,
      )

      # make a group of it
      msn.set_node_group('native_state', [native_node_id])

      # TODO: make the trimmed network groups

      return msn.base_network
#+END_SRC


**** Macrostate Free Energies

Function to get free energies across nodes.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def calc_msn_free_energies(
          msn,
  ):

      from geomm.free_energy import free_energy

      node_weights = [
          (node_id, weight)
          for node_id, weight in
          msn.get_nodes_attribute('_observables/total_weight').items()
      ]

      node_ids = [node_id for node_id, weight in node_weights]
      weights = [weight for node_id, weight in node_weights]

      node_fes = {
          node_id : fe
          for node_id, fe
          in zip(
              node_ids,
              free_energy(weights)
          )
      }

      return node_fes
#+END_SRC



**** Finding the Native State

This is just a convenience function to find the node that has the
native state in it.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  # @jlmem.cache
  def classify_native_state_cluster(
          clf_id,
          gexp,
  ):
      """Determine which cluster the starting state is from.

      It does this by (re)computing the pair distances for the reference
      state and then classifying them with the trained model.

      """

      # REVD: It should be main rep, explicit above now

      # get the feature vector for the reference structure
      ref_fields = get_centered_ref_state_traj_fields(
          gexp,
          rep_key='main_rep',
      )

      lig_id = dict(GEXP_LIG_IDS)[gexp]
      top = lig_selection_tops(lig_id)['main_rep']

      # get the atom pairs for the reference state

      pair_idxs = lig_bs_atom_pairs(
          gexp,
          rep_key='main_rep',
      )
      ref_feature = lig_prot_atom_pair_observable(
          pair_idxs,
          top,
          ref_fields,
      )

      # predict which cluster it would be in
      clf = get_clustering_model(
          clf_id,
          gexp,
      )

      node_id = clf.predict(ref_feature)[0]

      return node_id


#+end_src


**** Microstate Analyses

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def get_cluster_micro_trace(model_name, lig_id, node_id):

      # use lag time 2 since it is cached and not relevant here
      net = get_network(model_name, lig_id, 2)

      # get the assignments from the nodes in the specified cluster
      return net.node_assignments(node_id)

  def get_cluster_micro_traj(model_name, lig_id, node_id, downsample=None):

      import random

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      trace = get_cluster_micro_trace(model_name, lig_id, node_id)

      # downsample the trajectory if requested
      if downsample is not None or downsample < 1.0:
          assert downsample > 0

          n_samples = round(downsample * len(trace))

          trace = random.sample(trace, n_samples)

      with get_gexp_wepy_h5(lig_id) as wepy_h5:
          top = wepy_h5.get_topology()
          traj_fields = wepy_h5.get_trace_fields(
              trace,
              ['positions', 'box_vectors', 'alt_reps/missing'])

          traj_fields = traj_fields_to_correct_rep(traj_fields, lig_id)

      traj_fields['positions'] = recenter_superimpose_traj(
          traj_fields,
          lig_id,
          'correct_rep'
      )[0]

      traj = traj_fields_to_mdtraj(traj_fields, top)

      return traj

  def state_weight_ranking(net):
      """Get the ordered ranking of the clusters according to their weights."""

      import numpy as np

      weights = [node_attrs['_observables/total_weight'] for node_id, node_attrs
                 in net.graph.nodes.items()]

      ranking = np.array(net.graph.nodes)[np.argsort(weights)]

      return ranking

#+END_SRC


**** Cluster Centers

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  @jlmem.cache
  def get_cluster_center_traj(
          gexp,
          csn_id,
          alt_rep=Ellipsis,
  ):
      """Get the cluster center trajs for a gexp. If 'alt_rep' is given as a
      valid value it will return the traj in that representation (if
      possible error otherwise). If Ellipsis is given it will choose the
      "best" one for visualization ('correct_rep' if available 'main_rep' if
      not).

      If None it will default to the common denominator for all of them (i.e. 'main_rep')

      """

      from wepy.util.mdtraj import traj_fields_to_mdtraj

      # dispatch behavior on alt_rep
      if alt_rep is None:
          alt_rep = 'main_rep'

      elif alt_rep is Ellipsis:
          if gexp in ('3',):
              alt_rep = 'main_rep'
              field_names = ['positions', 'box_vectors']

          else:
              alt_rep = 'correct_rep'
              field_names = ['positions', 'box_vectors', 'alt_reps/missing']

      else:

          if alt_rep == 'main_rep':
              field_names = ['positions', 'box_vectors']

          elif alt_rep == 'correct_rep':
              field_names = ['positions', 'box_vectors', 'alt_reps/missing']

      clf_id = CSN_SPECS[csn_id]['clf_id']

      clf = get_clustering_model(clf_id, gexp)

      # convert the feature idxs of the cluster_ids to a nice trace
      trace = clf_features_to_run_trace_idxs(
          gexp,
          clf_id,
          clf.cluster_ids_,
      )

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      sel_tops = lig_selection_tops(lig_id)
      top = sel_tops[alt_rep]

      # get the traj fields
      with get_gexp_wepy_h5(lig_id) as wepy_h5:

          traj_fields = wepy_h5.get_trace_fields(
              trace,
              field_names,
          )

      # if we are getting the correct rep we need to transform them
      # to combine the missing positions
      if alt_rep == 'correct_rep':

          traj_fields = traj_fields_to_correct_rep(
              traj_fields,
              lig_id,
          )

      # recenter them
      traj_fields['positions'] = recenter_superimpose_traj(
          traj_fields,
          lig_id,
          alt_rep,
      )[0]

      gc.collect()

      # write them
      traj = traj_fields_to_mdtraj(traj_fields, top)

      return traj
#+end_src


**** Adding Observable Stats

After you calculate the observables for a dataset you want to have
these on the network.

You have different options for aggregation here, we provide a method
to put all of the common aggregates into the network:

- cluster center value
- mean
- median
- std. dev.
- range magnitude
- min and max

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def mappable_obs_node_stats(
          msn,
          field_key,
          node_id,
          node_attrs,
          node_fields,
  ):
      """Function compatible with the 'node_fields_map' function for a
      MacroStateNetwork, after partially evaluated for the field key
      """

      assert field_key in node_fields

      field_data = node_fields[field_key]

      # idx of microstate as a trace for the center
      center_trace = [msn.get_node_attribute(
          0,
          'center_idx',
      )]

      # ALERT: notice the hdf5 must already be opened since this could
      # be done in parallel

      # the value of the field for it
      center_value = msn.wepy_h5.get_trace_fields(
          center_trace,
          [field_key,],
      )[field_key][0]

      data_min = np.min(field_data)
      data_max = np.max(field_data)

      node_stats = {
          'mean' : np.mean(field_data),
          'median' : np.median(field_data),
          'std-dev' : np.std(field_data),
          'min' : data_min,
          'max' : data_max,
          'range' : data_max - data_min,
          'cluster_center' : center_value,
      }

      return node_stats

  def calc_msn_obs_stats(
          msn,
          observable_name,
          save=False,
  ):
      """Calculate the stats for an observable of a macrostate network.

      The MSN must be a full MacroStateNetwork so it has access to the
      microstates in the HDF5.

      If the 'save' parameter is True it will save the stats in a
      predetermined schema to the MacroStateNetwork.

      ,*This will not serialize to disk, it will just update the 'msn' object
      passed in.*

      If false this will be ignored.

      The schema is of the format (as argument to `set_nodes_observable`):

          'obs/{observable_name}/{stat_field_name}'

      Where 'stat_field_name' is the keys of the stats computed by the
      observable function, should be:

      - mean
      - median
      - std-dev
      - min
      - max
      - range
      - cluster_center

      """
      from functools import partial

      # get the full field key for the observable
      field_key = f'observables/{observable_name}'

      # partially evaluate the mapped function with the field_key
      stats_func = partial(
          mappable_obs_node_stats,
          msn,
          field_key,
      )

      # calculate the stats for each node

      # ALERT: must open for reading here and not inside the mapped
      # function since this could be in parallel
      with msn:
          node_stats = msn.node_fields_map(
              stats_func,
              [field_key,]
          )


      if save:

          # get the keys that were returned for each node
          stat_field_names = list(node_stats[list(node_stats.keys())[0]].keys())

          for stat_field_name in stat_field_names:

              # extract data for just one key
              stat_node_values = {
                  node_id : stats_d[stat_field_name]
                  for node_id, stats_d in node_stats.items()
              }

              # set that as an attribute
              attr_key = f'obs/{observable_name}/{stat_field_name}'

              msn.set_nodes_attribute(
                  attr_key,
                  stat_node_values,
              )

      return node_stats
#+end_src


*** Markov State Modelling (MSMs)
:BACKLINKS:
[2021-01-27 Wed 14:27] <- [[*Macro State Networks][Macro State Networks]]
:END:

**** Transition Probabilities & MSMs

These are functions for calculating transition probability matrices
from MacroStateNetworks (MSNs) and for making Markov State Models
(MSMs) from them.

There are a few different axes to think about:

- probability calculation
  - transpose
  - MLE
  - Bayesian
- trimming
  - strongly connected subgraph
  - ergodic trimming

They are available but the currently used one is dispatched from a
entry point function so basically just use this function once you have
a MacroStateNetwork to get the correct model:

***** Ergodic Trimming

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def subset_trim(countsmat, subset_idxs):

      # K is the input states, N is the output states
      in_n_states = countsmat.shape[0]
      out_n_states = len(subset_idxs)

      # allocate trimmed matrix
      trim_countsmat = np.zeros(
          (out_n_states, out_n_states,),
          dtype=countsmat.dtype,
      )

      # copy values for the subset
      for out_idx, in_idx in enumerate(subset_idxs):

          trim_countsmat[out_idx, :] = countsmat[in_idx, subset_idxs]
          trim_countsmat[:, out_idx] = countsmat[subset_idxs, in_idx]

      # make the mapping
      mapping = {in_idx : out_idx
                 for out_idx, in_idx in enumerate(subset_idxs)}

      return trim_countsmat, mapping

  def ergodic_trim(
          msn,
          method='csnanalysis',
          method_kwargs=None,
  ):
      """Trim a counts matrix and get the largest connected component.

      Parameters
      ----------

      msn : MacroStateNetwork or BaseMacroStateNetwork

      Returns
      -------

      """

      from msmtools.estimation import \
          connected_sets as pyemma_connected_set

      from csnanalysis.csn import CSN

      from msmbuilder.msm.core import \
          _strongly_connected_subgraph as msmb_strongly_connected_subgraph

      ## methods using the unweighted counts
      if method == 'csnanalysis-unweighted':

          # get the counts matrix according to this method
          countsmat = msn.edge_attribute_to_matrix(
              'unweighted_counts',
              fill_value=0,
          )

          # for CSNAnalysis the matrix is the reverse orientation
          countsmat_T = countsmat.T

          # make a CSN
          csn = CSN(countsmat_T)

          # trim it to the main components
          csn.trim(
              min_count=method_kwargs['ergodic_trim_cutoff'],
          )

          # use the trimmed off idxs to get which states remain
          chosen_subset = csn.trim_indices

      elif method == 'msmbuilder-unweighted':

          # get the counts matrix according to this method
          countsmat = msn.edge_attribute_to_matrix(
              'unweighted_counts',
              fill_value=0,
          )

          # this counts matrix is in the correct orientation for
          # msmbuilder method

          trimmed_countsmat, mapping, _ = \
                  msmb_strongly_connected_subgraph(
                      countsmat,
                      weight=method_kwargs['ergodic_trim_cutoff'],
                      verbose=False,
                      )

          # the mapping here is in terms of the node_idxs

          # chosen_subset is node_idx
          chosen_subset = list(mapping.keys())

      ### These are the weighted counts methods

      # The CSNAnalysis way
      elif method == "csnanalysis-weighted":

          # get the counts matrix according to this method
          countsmat = msn.edge_attribute_to_matrix(
              'weighted_counts',
              fill_value=0,
          )

          # for CSNAnalysis the matrix is the reverse orientation
          countsmat_T = countsmat.T

          # make a CSN
          csn = CSN(countsmat_T)

          # trim it to the main components
          csn.trim(
              min_count=method_kwargs['ergodic_trim_cutoff'],
          )

          # use the trimmed off idxs to get which states remain
          chosen_subset = csn.trim_indices

      # the MSMBuilder way
      elif method == "msmbuilder-weighted":

          # get the counts matrix according to this method
          countsmat = msn.edge_attribute_to_matrix(
              'weighted_counts',
              fill_value=0,
          )

          # this counts matrix is in the correct orientation for
          # msmbuilder method

          trimmed_countsmat, mapping, _ = \
                  msmb_strongly_connected_subgraph(
                      countsmat,
                      weight=method_kwargs['ergodic_trim_cutoff'],
                      verbose=False,
                      )

          chosen_subset = list(mapping.keys())


      # ALERT: the pyemma way, this wasn't working for me
      elif method == "pyemma":

          raise NotImplementedError()

          subsets = pyemma_connected_sets(msn.countsmat)

          # get the index of largest one and the set itself
          subset_sizes = [len(subset) for subset in subsets]

          largest_subset_idx = np.argmax(subset_sizes)

          # choose the one to use
          chosen_subset = subsets[largest_subset_idx]

      # get the submatrix of this along with the mapping for the
      # countsmat trimmings
      trim_countsmat, trimming_mapping = subset_trim(countsmat, chosen_subset)

      # trimming_mapping is in node_idxs

      return trim_countsmat, trimming_mapping

  # SNIPPET: the old way I was doing things to the CSNAnalysis
  # def net_csn(msn, min_count=1):
  #     """Convert a macrostate network to a CSN object."""

  #     from csnanalysis.csn import CSN

  #     # make a CSN
  #     csn = CSN(msn.countsmat)

  #     # trim it to the main components
  #     csn.trim(min_count=min_count)

  #     trimmed_group = [msn.node_idx_to_id(idx) for idx in
  #                      set(range(csn.nnodes)).difference(csn.trim_indices)]

  #     return csn, trimmed_group

#+end_src


***** Transition Probability Matrix

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  ## Transition Probabilities Method 1: Maximum Likelihood (MLE) PYEMMA
  ## version
  def transprob_mle_pyemma(
          counts_mat,
          method='auto',
          reversible=True,
          eps_mu=1e-6,
  ):

      from msmtools.estimation \
          import transition_matrix \
          as pyemma_transmat_mle

      print("Running the pyemma transition matrix")
      # this uses the MLE method
      transprob_mat, stationary_distribution = \
          pyemma_transmat_mle(
              counts_mat,
              # this should be calculated by this method, so we set to None
              # here
              mu=None,

              # this is which algorithms to use based on the sparsity of the
              # matrix, which we just set to auto since we should get the
              # same answer either way and let it figure it out.
              method=method,

              # this has a bunch of options that go with it, but
              # this makes the matrix reversible, which is what we
              # want, rest of the options assume this is True
              reversible=reversible,

              # this is the epsilon value you want to have for finding
              # convergence
              eps_mu=eps_mu,

              # return the stationary distribution as well
              return_statdist=True,

          )

      return transprob_mat, stationary_distribution

  ## Transition Probabilities Method 2: Maximum Likelihood (MLE)
  ## MSMBuilder version
  def transprob_mle_msmb(
          counts_mat,
          ,**kwargs,
  ):
      # MSMBuilder way

      from msmbuilder.msm._markovstatemodel \
          import _transmat_mle_prinz \
          as msmb_transmat_mle

      # no options here except the tolerance
      transprob_mat, stationary_distribution = \
          msmb_transmat_mle(counts_mat)


      return transprob_mat, stationary_distribution

  ## Transition Probabilities Method 3: Transpose method
  def transprob_transpose(
          counts_mat,
          ,**kwargs,
  ):

      rev_counts = 0.5 * (counts_mat + counts_mat.T)

      populations = rev_counts.sum(axis=0)
      populations /= populations.sum(dtype=float)
      transmat = rev_counts.astype(float) / rev_counts.sum(axis=1)[:, None]

      return transmat, populations

  ## Transition Probabilities Method 4: Nonreversible Normalize
  #
  # This method simply normalizes the outgoing probabilities of each
  # node so that they sum to 1 and are probabilities. Does not do
  # symmetrization
  def transprob_nonreversible_normalize(
          counts_mat,
          ,**kwargs,
  ):

      # the first axis is the sources, second axis is the targets

      # we want to normalize only the source->target transitions for
      # each source node

      # sum along the source axis
      out_total = counts_mat.sum(axis=0)

      # scale all the values down and renormalize such that the sum to 1
      # along the rows
      transmat = counts_mat / out_total

      # ALERT: not sure I'm doing this right, but I don't use it anyhow

      # compute the populations
      populations = out_total / out_total.sum()

      return transmat, populations

  def compute_transprob(
          counts_mat,
          method='mle-pyemma',
          method_kwargs=None,
  ):

      if method_kwargs is None:
          method_kwargs = {}

      # dispatch on the method

      if method == 'nonreversible-normalize':

          transprob_mat, stationary_dist = transprob_nonreversible_normalize(
              counts_mat,
              ,**method_kwargs,
          )

      elif method == 'mle-pyemma':

          transprob_mat, stationary_dist = transprob_mle_pyemma(
              counts_mat,
              ,**method_kwargs,
          )

      elif method == 'mle-msmb':

          transprob_mat, stationary_dist = transprob_mle_msmb(
              counts_mat,
              ,**method_kwargs,
          )

      elif method == 'transpose':

          transprob_mat, stationary_dist = transprob_transpose(
              counts_mat,
              ,**method_kwargs,
          )


      else:
          raise ValueError(f"Unkown method {method}")

      return transprob_mat, stationary_dist
#+end_src


***** Markov State Model Object

This is an object that has things like the stationary probabilities
and is able to run simulations etc. on.

Made by trimming and calculating the transition probability matrix
like above.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def make_pyemma_msm(
          msn,
          ergodic_trim_cutoff=1,
          trim_method='csnanalysis',
          trim_kwargs=None,
          transprob_method='transpose',
          transprob_kwargs=None,
  ):

      from pyemma.msm import MSM as MarkovStateModel

      # get the transition probability matrix of the strongly ergodic
      # component

      print(f"Doing Ergodic Trimming with method: {trim_method}")

      # Get the connected subset we are interested in.

      # trimming mapping is in terms of node_idx
      trim_countsmat, trimming_mapping = ergodic_trim(
          msn,
          method=trim_method,
          method_kwargs=trim_kwargs,
      )

      print("Making the Transition Probability Matrix")
      tprob_mat, populations = compute_transprob(
          trim_countsmat,
          method=transprob_method,
          method_kwargs=transprob_kwargs,
      )

      # for pyemma
      msm = MarkovStateModel(
          tprob_mat.T,
      )

      ## compose the state mapping for the MSN with the trimming mapping
      ## so we know that the idxs in the trimmed countsmat match the
      ## root state idxs
      dict_match = lambda dict1, dict2 : \
          {k: dict2.get(v) for k, v in dict1.items() if v in dict2}

      # the trimming_mapping is in terms of node_idx, this converts it to node_id
      mapping = dict_match(
          msn.node_id_to_idx_dict(),
          trimming_mapping,
      )

      # the mapping is: node_id -> trimmed_node_idx

      return msm, mapping


#+end_src


**** MSM MacroStateNetworks

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def add_msm_to_msn(
          msn,
          msm_id,
          pyemma_msm,
          trimming_mapping,
  ):
      """Adds basic information like the transition probability matrix
      values to the MSN model.

      Currently only the trimming information and the transition
      probabilities are stored.

      The trimming mapping is saved as two groups:

      - trimmed_nodes/msmid-{msm_id}/largest_component :: the nodes that are in the
        largest connected group, given by the 'trimming_mapping'
        dictionary.

      - trimmed_nodes/msmid-{msm_id}/not_largest_component :: the nodes that were left
        out of the largest group.

      The transition probability values are stored assymetrically for
      only those edges in the largest connected component. All edges not
      in this are set to np.nan. The edge attribute will be of the
      format: "transition_probability/msmid-{msm_id}"

      """

      # copy so that we don't screw up the original
      msn = deepcopy(msn)

      ## make the groups for the trimmings
      keep_node_ids = list(trimming_mapping.keys())
      trimmed_node_ids = set(msn.node_ids) - set(keep_node_ids)

      msn.set_node_group(
          f"trimmed_nodes/msmid-{msm_id}/largest_component",
          keep_node_ids,
      )

      msn.set_node_group(
          f"trimmed_nodes/msmid-{msm_id}/not_largest_component",
          trimmed_node_ids,
      )

      # ALERT: pay attention to this

      # get the transitions probability matrix, we have to transpose
      # since that is how they store it in PYEMMA
      transition_prob_mat = pyemma_msm.P.T

      prob_attr_key = f"transition_probability/msmid-{msm_id}"

      # iterate through every edge and see if there is a value for it
      for i, j in msn.graph.edges.keys():

          # if both nodes are in the trimming we will set the probability
          if ((i in trimming_mapping.keys()) and
              (j in trimming_mapping.keys())):

              # lookup the trimmed idxs
              trim_i = trimming_mapping[i]
              trim_j = trimming_mapping[j]

              prob = transition_prob_mat[trim_i, trim_j]

              # set it into the network
              msn.graph.edges[(i, j)][prob_attr_key] = prob

          # if it isn't assign nan
          else:
              msn.graph.edges[(i, j)][prob_attr_key] = np.nan

      return msn
#+end_src
**** Hyperparameter Optimization

***** GMRQ Calculation

Functions specifically for calculating GMRQs from MSMs.

This gets used in the scoring and cross-validation part.


#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  ## a little special error class
  class MSMError(Exception):
      """Error for propagating bad GMRQ calculations."""

      pass


  ### Calculating GMRQ for training and test sets

  # I've chopped it up into a number of different functions for clarity
  # of understanding rather than performance. This really is fast
  # compared to the clustering process so speed is not an issue here at
  # all.

  def map_eigenvectors(source_mapping,
                       source_eigvecs,
                       target_mapping,
  ):
      """Given a mapping of K states that has two sub-mappings N_s and N_t
      (N_s >= N_t) each of which has a set of eigenvectors we want to
      get the eigenvectors from N_s that are equivalent to the
      eigenvectors in N_t.

      Parameters
      ----------

      source_mapping : dict of int to int

      source_eigvecs : arraylike

      target_mapping : dict of int to int

      Returns
      -------

      mapped_eigvecs : arraylike

      """

      # def dict_compose
      dict_match = lambda dict1, dict2 : \
          {k: dict2.get(v) for k, v in dict1.items() if v in dict2}

      # make a mapping between the source states to the target states:
      # N_s : K match K : N_t --> N_s : N_t
      transform_mapping = dict_match(
          # reverse the source mapping so its N_s : K
          {v: k for k, v in source_mapping.items()},
          # this is K : N_t
          target_mapping
      )

      # get the source and value indices as two lists
      source_indices, dest_indices = zip(*transform_mapping.items())

      # make a new eigenvector array that is the size of the target
      mapped_eigvecs = np.zeros((
          len(target_mapping),
          source_eigvecs.shape[1]
      ))

      # if there are any eigenvectors in the source not in the target
      # than they will just be zero here

      # copy the source eigenvectors to the mapped eigenvectors
      mapped_eigvecs[dest_indices, :] = np.take(
          source_eigvecs,
          source_indices,
          axis=0,
      )

      return mapped_eigvecs


  def gmrq_overlap_matrix(
          stationary_distribution,
  ):
      """Compute the overlap matrix (sometimes abbreviated as S) from the
      stationary probability distribution (typically abbreviated as pi)
      for a transition probability matrix.

      Parameters
      ----------

      stationary_distribution : numpy.array of shape (N_s,)
         The stationary probability distribution (pi) of the transition
         probability matrix (T) where N_s is the number of states in the
         state model.

      Returns
      -------

      overlap_matrix : np.array of shape (N_s, N_s)
         The overlap matrix (S) of the stationary probability distribution.

      """

      # S is the typical single letter abbreviation
      return np.diag(stationary_distribution)


  def gmrq_correlation_matrix(
          overlap_matrix,
          transition_matrix,
  ):
      """Compute the correlation matrix for a transition matrix and the
      associated overlap matrix.

      N_s is the number of states in the MSM.

      Parameters
      ----------

      overlap_matrix : np.array of shape (N_s, N_s)
         The overlap matrix (S) of the stationary probability distribution.

      transition_matrix : np.array of shape (N_s, N_s)
         The transition probability matrix (T) of the MSM.

      Returns
      -------

      correlation_matrix : np.array of shape (N_s, N_s)
         The correlation matrix (C) of the stationary probability distribution.

      """

      # C is the common abbreviation here
      return overlap_matrix.dot(transition_matrix)


  def gmrq_score_train_msm(train_msm):
      """Score the MSM based on the eigenvalues of its transition matrix."""

      return np.sum(train_msm.eigenvalues())


  def gmrq_score_test_msm(
          train_msm,
          train_mapping,
          test_msm,
          test_mapping,
  ):
      """Score a test MSM given the training MSM eigenvectors.

      The test_msm must have the same state definitions as the train_msm.

      Parameters
      ----------

      train_msm : pyemma.MarkovStateModel

      train_mapping : dict of int to int

      test_msm : pyemma.MarkovStateModel

      test_mapping : dict of int to int

      Returns
      -------

      test_gmrq : float
          GMRQ score for the test set using the training model.

      """

      # calculate the eigenvectors for the training MSM which will be
      # used to score their performance on the testing MSM. This is A in
      # the McGibbon paper
      A = train_msm.eigenvectors_right()

      # we need to make sure we are comparing the correct eigenvectors
      # between the training and test MSMs because both can have
      # different sets of states which are included in their transition
      # probability matrices due to the need to do ergodic
      # trimming. That means that the root MSN will have K states as
      # specified from clustering but the training MSM has N_train and
      # the test MSM has N_test states which are both subsets of K but
      # are potentially disjoint sets between each other. To reconcile
      # this we have the mappings of indices from K to each N. We use
      # these to match states/eigenvectors from N_train to N_test. This
      # assumes that N_train >= N_test.

      # if the two mappings are identical we can just ignore this
      # mapping step
      if train_mapping != test_mapping:

          # otherwise go ahead and map them and replace the A
          # eigenvectors with the mapped one

          # ALERT: not sure this is correct here. Is it okay if N_train < N_test ?

          # this might fail if the training set is smaller than testing
          # set
          try:
              A = map_eigenvectors(
                  train_mapping,
                  A,
                  test_mapping
              )

          # TODO: -inf or NaN here?

          # if it does fail we automatically score this as -inf, which
          # will result in an infinite score during optimization
          except MSMError:
              breakpoint()
              return np.nan

      # from the test set we compute the overlap and correlation
      # matrices which will be used to calculate the score on the
      # training eigenvectors
      S = gmrq_overlap_matrix(
          test_msm.pi
      )
      C = gmrq_correlation_matrix(
          S,
          test_msm.transition_matrix,
      )

      # compute the GMRQ score of the training eigenvectors on the
      # testing overlap and correlation matrices.

      # we handle linear algebra answers and return NaN if necessary.
      try:
          test_gmrq = np.trace(
              A.T.dot(
                  C.dot(A)
              ).dot(
                  np.linalg.inv(
                      A.T.dot(
                          S.dot(A)
                      )
                  )
              )
          )
      except np.linalg.LinAlgError:
          test_gmrq = np.nan


      return test_gmrq


#+end_src


***** GMRQ Cross-Validation Scoring

These are functions that take all of the data and do cross-validation
using the GMRQ scores of training and test sets.

This contains the objective function for doing hyperparameter
optimization later.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  ## Training stuff

  # two layers to functions: one that applies a classfier to a
  # particular split of samples, and one that runs all of the splits.
  # another is for aggregating the split scores

  def score_msm_gmrq_split_classifier(
          clf,
          train_rep_idxs,
          test_rep_idxs,
          gexp=None,
          lag_time=None,
          all_features=None,
          span_runs=None,
  ):
      """Score a classifier using GMRQ given a single splitting of training
      and sample replicates (i.e. a single cross-validation resampling).

      Parameters
      ----------

      clf : classifier following scikit learn API

      train_rep_idxs : list of int
         The simulation replicates in the training set.

      test_rep_idxs : list of int
         The simulation replicates in the testing set.


      gexp : str

      lag_time : int
          Lag time for computing transitions must be 2 or greater.

      all_features : dict of int to features
          A dictionary of all the clustering features by run. Its easier
          to read from memory once and then take from here since there
          is a lot of redundancy in the splits.

      span_runs : dict of int to list of int
          A mapping of a spanning contig (replicate) to the runs it
          contains.

      Returns
      -------

      test_gmrq : float
         The GMRQ score for the test dataset.

      """

      # make sure the "globals" are given
      assert gexp is not None
      assert lag_time is not None
      assert all_features is not None
      assert span_runs is not None

      # use the reps for each split to get the runs for each rep
      train_run_idxs = it.chain(*[span_runs[rep_idx] for rep_idx in train_rep_idxs])
      test_run_idxs = it.chain(*[span_runs[rep_idx] for rep_idx in test_rep_idxs])

      # ALERT sort them here. we need to sort and order the run idxs
      # in such a way that we can restructure them later after
      # desctructuring them here.

      # sort them to get a stable ordering
      train_run_idxs = sorted(train_run_idxs)
      test_run_idxs = sorted(test_run_idxs)

      # we grab the features for the runs here and inline
      # restructure them so we don't have to deal with the garbage
      # collector

      train_samples = unscalarize_features(
        reshape_run_to_features(
            [all_features[run_idx] for run_idx in train_run_idxs]
        )
      )

      test_samples = unscalarize_features(
        reshape_run_to_features(
            [all_features[run_idx] for run_idx in test_run_idxs]
        )
      )

      # perform the clustering on the training data
      clf.fit(train_samples)

      # get the labels from this and reshape into an assignments
      # shape. Restructure them to run_idx -> traj, cycles, 1
      train_labels = {
        run_idx : run_obs
        for run_idx, run_obs
        in zip(train_run_idxs,
               reshape_features_to_run(
                   clf.labels_,
                   gexp,
                   run_idxs=train_run_idxs,
               )
        )
      }

      # we can also get the labels for the training data
      test_labels = clf.predict(test_samples)

      # restructure them
      test_labels = {
        run_idx : run_obs
        for run_idx, run_obs
        in zip(test_run_idxs,
               reshape_features_to_run(
                   test_labels,
                   gexp,
                   run_idxs=test_run_idxs,
               )
        )
      }

      # then we use the clustering data to make a network

      # first get the contigtree which is a subset of runs we are using

      # Training

      # make a sub-contigtree of just the training runs/spans
      train_contigtree = get_contigtree(gexp,
                                      runs=train_run_idxs)

      # make a network with all the labels
      train_msn = MacroStateNetwork(train_contigtree,
                                transition_lag_time=lag_time,
                                assignments=train_labels
      )

      # Testing
      test_contigtree = get_contigtree(gexp,
                                    runs=test_run_idxs)

      test_msn = MacroStateNetwork(test_contigtree,
                                transition_lag_time=lag_time,
                                assignments=test_labels
      )

      # get the counts matrix for both the training data and the test
      # data
      train_countsmat = train_msn.countsmat

      test_countsmat = test_msn.countsmat

      ## Compute the transition probabilities and stationary
      ## distribution, there are different ways to do this, choose the
      ## appropriate one

      # the mappings are relative to their MSNs count matrix

      # make the MSM for the training data
      train_msm, train_mapping = make_msm(train_msn)

      # score the training msm for fun, but this isn't actually used
      train_gmrq = gmrq_score_train_msm(train_msm)

      test_msm, test_mapping = make_msm(test_msn)

      # then score the testing data from this model
      test_gmrq = gmrq_score_test_msm(
        train_msm,
        train_mapping,
        test_msm,
        test_mapping,
      )

  #     # Report on results
  #     msg = f"""
  # Results for split
  # -----------------

  # Train   Test
  # {train} {test}

  # Total number of states: {train_msn.num_states}

  # - Training MSM:
  #   - num states :: {train_msm.nstates}
  #   - GMRQ :: {train_gmrq}

  # - Testing MSM:
  #   - num states :: {test_msm.nstates}
  #   - GMRQ :: {test_gmrq}
  # """

      return test_gmrq

  def score_msm_gmrq_crossval_classifier(
          gexp,
          observable_key,
          clf,
          splitter,
          lag_time=2,
  ):
      """Score a classifier using cross validation over the splitter using GMRQ."""

      # SNIPPET: these were some test parameters used

      ## Parameters
      # GEXP = '10'
      # LAG_TIME = 2
      # OBSERVABLE = 'lig_rmsd'
      # N_SPLITS = 3
      # splitter = KFold(n_splits=N_SPLITS)

      ## Code

      # split into training and testing based on the replicates (spans
      # in the contigtree)

      all_contigtree = get_contigtree(gexp)

      n_replicates = len(all_contigtree.span_traces)

      replicate_idxs = [i for i in range(n_replicates)]

      # split on the replicates using the splitter
      splits = list(splitter.split(replicate_idxs))

      # OPT: figure out where these should be loaded so as to reduce
      # loading from disk. Here it would would be once per
      # hyperparameter sample which is decent since it would get loaded
      # K times for each fold otherwise.

      # get all the labels for the runs and index them by the run in a
      # dict. Subsamples will draw from here
      all_features = {run_idx : run_obs
                    for run_idx, run_obs in
                    enumerate(get_observable(
                        observable_key,
                        gexp,
                        source='fs',))
      }

      # get a mapping of replicate/span to which runs it has
      span_runs = {rep_idx :
                   set((run_idx
                        for run_idx, cycle_idx
                        in span_trace))
                   for rep_idx, span_trace
                   in all_contigtree.span_traces.items()}

      split_scores = []
      # perform the process for each splitting
      for split_idx, split in enumerate(splits):

          train_rep_idxs = split[0]
          test_rep_idxs = split[1]


          test_gmrq = score_classifier_split(
              clf,
              train_rep_idxs,
              test_rep_idxs,
              gexp=gexp,
              lag_time=lag_time,
              all_features=all_features,
              span_runs=span_runs,
          )

          split_scores.append(test_gmrq)

      # aggregate the scores for each split in cross validation

      stats = {
          'mean' : np.mean(split_scores),
          'median' : np.median(split_scores),
          'min' : np.min(split_scores),
          'max' : np.max(split_scores),
          'std' : np.std(split_scores),
          'var' : np.var(split_scores),
          'scores' : split_scores,
      }

      # this is the final score for the classifier
      return stats
#+end_src


*** Committor Probabilities

For calculating committor probabilities and choosing basins over
networks.

**** Basins

Functions for finding the basins to use for computing committors.

We provide high-level wrappers that dispatch on the 'BASIN_SPECS'
using the 'basin_id' for each of them.

***** Specs

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  BASIN_SPECS = {}

  ## basin model for the topn bound and 0.7 unbound cutoff across all
  ## MSMs
  _basin_top2_7 ={

      f'msm-{msm_id}_basin-bound-top2-unbound-0.7-cutoff' : {
          'msm_id' : f'{msm_id}',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.7 * tkunit.nanometer,
              },
          },
      }

      for msm_id in MSM_SPECS.keys()
  }


  #
  _basin_top2_6 ={

      f'msm-{msm_id}_basin-bound-top2-unbound-0.6-cutoff' : {
          'msm_id' : f'{msm_id}',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.6 * tkunit.nanometer,
              },
          },
      }

      for msm_id in MSM_SPECS.keys()
  }

  #
  _basin_top2_5 = {

      f'msm-{msm_id}_basin-bound-top2-unbound-0.5-cutoff' : {
          'msm_id' : f'{msm_id}',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.5 * tkunit.nanometer,
              },
          },
      }

      for msm_id in MSM_SPECS.keys()
  }

  bound_cutoffs = [
      0.1 * tkunit.nanometer,
      0.2 * tkunit.nanometer,
      0.3 * tkunit.nanometer,
  ]

  unbound_cutoffs = [
      0.7 * tkunit.nanometer,
      0.6 * tkunit.nanometer,
      0.5 * tkunit.nanometer,
  ]

  msm_ids = [
      'cutoff-1',
      'cutoff-2',
      'cutoff-5',
      'cutoff-10',
  ]

  matrix_combos = [
      (cutoff, bound, unbound)
      for cutoff, bound, unbound
      in it.product(
          msm_ids,
          bound_cutoffs,
          unbound_cutoffs,
      )
  ]

  #
  _basin_matrix = {

      f'msm-{msm_id}_basin-bound-{bound.value_in_unit(bound.unit)}-unbound-{unbound.value_in_unit(unbound.unit)}-cutoff' : {
          'msm_id' : f"{msm_id}",
          'bound' : {
              'method' : 'rmsd',
              'method_kwargs' : {
                  'cutoff' : bound,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : unbound,
              },
          },
      }

      for msm_id, bound, unbound in matrix_combos

  }

  # add all the specs into the main one
  for basin_specs in [
          _basin_top2_7,
          _basin_top2_6,
          _basin_top2_5,
          _basin_matrix,
  ]:
      BASIN_SPECS.update(basin_specs)


  def print_basin_spec_shell():

      print(' '.join([f"'{basin_id}'" for basin_id in BASIN_SPECS.keys()]))

  OLD_BASIN_SPECS = {
      '0' : {
          'msm_id' : '0',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.7 * tkunit.nanometer,
              },
          },
      },

      '0_big_cutoff' : {
          'msm_id' : 'big_cutoff',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.7 * tkunit.nanometer,
              },
          },
      },


      '1' : {
          'msm_id' : '0',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'lig_dist',
              'method_kwargs' : {
                  'cutoff' : 0.24 * tkunit.nanometer,
              },
          },
      },


      'choose-unbound-0' : {
          'msm_id' : '0',
          'bound' : {
              'method' : 'weights',
              'method_kwargs' : {
                  'top_n' : 2,
              },
          },
          'unbound' : {
              'method' : 'chosen',
              'method_kwargs' : {
                  'node_ids' : [691,],
              },
          },
      },

  }


#+end_src

***** High-Level Functions

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def idxs_in_msm(
          msn,
          msm_id,
          query_node_ids,
  ):
      """Function checks that all the node idxs for an MSM and determines
      whether they are in the largest connected component or not.

      Uses the node groups from the MacroStateNetwork of the format:

          'trimmed_nodes/msmid-{msm_id}/largest_component'

      Returns
      -------

      in_msm_idxs
          The indices in the MSM

      not_in_msm_idxs
          The indices not in the MSM largest connected component

      Raises
      ------

      MSMError : if no largest component is available to use.

      """

      node_group_key = f'trimmed_nodes/msmid-{msm_id}/largest_component'

      # the nodes that are in the largest component
      try:
          good_nodes = msn.node_groups[node_group_key]
      except KeyError:
          raise MSMError(f"No node group for largest component for the MSM model: {msm_id}.")

      in_nodes = [node_id
                  for node_id in query_node_ids
                  if node_id in good_nodes]

      out_nodes = list(set(query_node_ids) - set(in_nodes))

      return in_nodes, out_nodes

  def compute_csn_bound_basin(
          basin_id,
          gexp,
          csn_id,
  ):

      # parse the basin specs
      method = BASIN_SPECS[basin_id]['bound']['method']
      basin_kwargs = BASIN_SPECS[basin_id]['bound']['method_kwargs']

      msm_id = BASIN_SPECS[basin_id]['msm_id']

      msn = get_msn(
          csn_id,
          gexp,
      )

      if method == 'weights':

          bound_basin_idxs = compute_csn_bound_basin_weights(
              msn,
              ,**basin_kwargs
          )

      elif method == 'rmsd':

          bound_basin_idxs = compute_csn_bound_basin_rmsd(
              msn,
              csn_id,
              gexp,
              ,**basin_kwargs,
          )


      # elif method == 'natives':

      #     bound_basin_idxs = compute_csn_bound_basin_natives(
      #         clustering_model,
      #         lig_id,
      #     )

      # elif method == 'feature_dist':

      #     bound_basin_idxs = compute_csn_bound_basin_feature_dist(
      #         clustering_model,
      #         lig_id,
      #         cutoff,
      #     )


      else:

          raise ValueError(f"method not recognized for basin_id: {basin_id}")

      # remove the idxs that are not in the largest connected component
      # for the MSM
      bound_basin_idxs, _ = idxs_in_msm(
          msn,
          msm_id,
          bound_basin_idxs,
      )

      return bound_basin_idxs

  def compute_csn_unbound_basin(
          basin_id,
          gexp,
          csn_id,
  ):

      # parse the basin specs
      method = BASIN_SPECS[basin_id]['unbound']['method']
      basin_kwargs = BASIN_SPECS[basin_id]['unbound']['method_kwargs']

      msm_id = BASIN_SPECS[basin_id]['msm_id']

      msn = get_msn(
          csn_id,
          gexp,
      )

      if method == 'warp':

          unbound_basin_idxs = compute_csn_unbound_basin_warp(
              msn,
              gexp,
              ,**basin_kwargs,
          )

      elif method == 'lig_dist':

          unbound_basin_idxs = compute_csn_unbound_basin_lig_dist(
              gexp,
              csn_id,
              ,**basin_kwargs,
          )

      elif method == 'chosen':

          unbound_basin_idxs = BASIN_SPECS[basin_id]['unbound']['method_kwargs']['node_ids']

      # elif method == 'lig_dist':

      #     unbound_basin_idxs = compute_csn_unbound_basin_lig_dist(
      #         clustering_model,
      #         lig_id,
      #         cutoff,
      #     )

      else:

          raise ValueError(f"method not recognized for basin_id: {basin_id}")

      # remove the idxs that are not in the largest connected component
      # for the MSM
      unbound_basin_idxs, _ = idxs_in_msm(
          msn,
          msm_id,
          unbound_basin_idxs,
      )

      return unbound_basin_idxs


  def basins_to_msn(
          basin_id,
          gexp,
          msn,
          bound_basin_idxs,
          unbound_basin_idxs,
  ):

      new_msn = deepcopy(msn)

      # remove old groups if they are in there
      if f'basin-{basin_id}/bound_basin' in new_msn._node_groups:
          del new_msn._node_groups[f'basin-{basin_id}/bound_basin']

      if f'basin-{basin_id}/unbound_basin' in new_msn._node_groups:
          del new_msn._node_groups[f'basin-{basin_id}/unbound_basin']

      # save the things in the groups
      new_msn.set_node_group(f'basin-{basin_id}/bound_basin', bound_basin_idxs)
      new_msn.set_node_group(f'basin-{basin_id}/unbound_basin', unbound_basin_idxs)

      return new_msn
#+end_src



***** Bound Basin

Different methods for finding the bound basin.


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def compute_csn_bound_basin_weights(
          msn,
          top_n=1,
  ):
      """This function ranks the weights of all the nodes in the network and
      gives the top N of them as the bound basin.
      """

      # just give the biggest weighted nodes according to the state
      # weight ranking
      return state_weight_ranking(msn)[-top_n:]


  def compute_csn_bound_basin_natives(
          msn,
          clf_id=None,
          gexp=None,
  ):
      """This function will simply get the native state as the bound
      basin.

      This may not be appropriate for docked or otherwise modelled
      structures.

      """

      assert clf_id is not None
      assert gexp is not None

      native_state = classify_native_state_cluster(
          clf_id,
          gexp,
      )

      return [native_state,]




  @jlmem.cache
  def compute_csn_bound_basin_rmsd(
          msn,
          csn_id,
          gexp,
          cutoff=None,
          ):
      """This returns all cluster centers that are within a certain cutoff
      to a certain state as the bound basin.
      """

      from geomm.rmsd import calc_rmsd

      assert cutoff is not None

      lig_id = dict(GEXP_LIG_IDS)[gexp]
      clf_id = CSN_SPECS[csn_id]['clf_id']

      sel_idxs = lig_selection_idxs(lig_id)

      lig_idxs = sel_idxs['main_rep/ligand']

      # coordinates are in nanometers, convert cutoff to this
      cutoff_unitless = cutoff.value_in_unit(tkunit.nanometer)

      # get the features from the model
      clf = get_clustering_model(
          clf_id,
          gexp,
      )

      # compute RMSDs of the ligand to the native state as an ad hoc
      # selection criterion

      # get the cluster center structures. Use the common denomitor
      # representation (which is 'main_rep') here for compatibility,
      # this isn't used for visualization.
      cluster_center_traj = get_cluster_center_traj(
          gexp,
          csn_id,
          alt_rep='main_rep',
      )

      # use the highest weight state for the bound state
      bound_state_id = state_weight_ranking(msn)[-1]

      dists_to_bound = []
      for center_idx in range(len(clf.cluster_centers_)):

          rmsd = calc_rmsd(cluster_center_traj[bound_state_id].xyz[0][lig_idxs],
                           cluster_center_traj[center_idx].xyz[0][lig_idxs])

          dists_to_bound.append(rmsd)

      matched_centers = [l[0] for l in
                         np.argwhere(
                             np.array(dists_to_bound) < cutoff_unitless
                         )
                         ]

      # add the centers that are within the cutoff along with the bound state
      bound_basin_ids = np.array(matched_centers)

      return bound_basin_ids

  # TODO: I don't want to fix this right now, I won't be using it.
  @jlmem.cache
  def compute_csn_bound_basin_feature_dist(clustering_model, lig_id, cutoff):

      raise NotImplementedError

      import numpy as np
      import scipy.spatial.distance as spdist

      # use lag time of 2 since it should be cached and isn't actually used.
      net = lig_state_network(lig_id, clustering_model, 2)

      # get the features from the model
      clf = get_clustering_model(clustering_model, lig_id)

      # use the highest weight state for the bound state
      bound_state_id = state_weight_ranking(net)[-1]
      other_idxs = list(set(range(len(clf.cluster_centers_))) -
                        set([bound_state_id]))


      bound_state_center = clf.cluster_centers_[bound_state_id]
      other_centers = clf.cluster_centers_[other_idxs]

      dists_to_bound = (spdist.cdist([bound_state_center], other_centers, 'canberra') /\
                        bound_state_center.shape[0])[0]

      matched_centers = [l[0] for l in np.argwhere(dists_to_bound < cutoff)]

      # add the centers that are within the cutoff along with the bound state
      bound_basin_ids = np.array([bound_state_id] + matched_centers)

      return bound_basin_ids
#+END_SRC

***** Unbound Basin

Methods for finding the unbound basin.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  @jlmem.cache
  def compute_csn_unbound_basin_warp(
          msn,
          gexp,
  ):
      """Report the nodes which had any of the warping events for the gexp
      simulations."""

      contigtree = get_contigtree(gexp)

      # get the trace of the frames which were warped
      trace = contigtree.warp_trace()

      warped_nodes = []
      # then find which cluster these were in
      for node_id in msn.graph.nodes:

          assignments = msn.get_node_attribute(node_id, 'assignments')

          # check if any of the trace is in the node assignments and if
          # they are record this node index and remove them from the
          # list since they won't be there in the future
          matches = [rec for rec in trace if rec in assignments]

          if len(matches) > 0:
              warped_nodes.append(node_id)
              for match in matches:
                  trace.remove(match)

      return warped_nodes

  @jlmem.cache
  def compute_csn_unbound_basin_lig_dist(
          gexp,
          csn_id,
          cutoff=None,
  ):
      """This gets the walkers which are beyond a certain distance much like
      the boundary conditions themselves but only takes into account the
      cluster centers.

      This is good if you didn't get any walkers unbound.

      Cutoff will be converted to nanometer

      """

      # compute the distances of the ligand to the ligand to the protein
      # for the cluster centers and then take the minimum and find if it
      # is above the cutoff

      assert cutoff is not None
      assert tkunit.is_quantity(cutoff)

      # convert cutoff to nanometer
      cutoff_nm = cutoff.value_in_unit(tkunit.nanometer)

      # get the cluster center structures
      cluster_center_traj = get_cluster_center_traj(
          gexp,
          csn_id,
          alt_rep='main_rep',
      )

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # get the ligand and protein idxs
      sel_idxs = lig_selection_idxs(lig_id)

      lig_idxs = sel_idxs['main_rep/ligand']
      prot_idxs = sel_idxs['main_rep/protein']

      atom_pairs = np.array(list(it.product(lig_idxs, prot_idxs)))

      cluster_pair_distances = mdj.compute_distances(cluster_center_traj, atom_pairs)

      unbound_basin_ids = [node_id for node_id, node_dists
                           in enumerate(cluster_pair_distances)
                           if node_dists.min() > cutoff_nm]

      return unbound_basin_ids

#+END_SRC


**** Computing Committor Probabilities & TS Ensemble

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  committor_methods = [
      'msmb',
      # 'pyemma',
      # 'csnanalysis',
  ]

  TS_SPECS = {}
  for method in committor_methods:

      for basin_id in BASIN_SPECS.keys():

          TS_SPECS[f'basin-{basin_id}_committor-{method}'] = {
              'committor_method' : method,
              'basin_id' : f'{basin_id}',
              'upper' : 0.6,
              'lower' : 0.4,
          }

  def print_ts_spec_shell():

      print(' '.join([f"'{ts_id}'" for ts_id in TS_SPECS.keys()]))

  def forward_committor_probabilities(
          transition_probability_matrix,
          source_basin,
          sink_basin,
          tolerance=1e-6,
          maxstep=20,
  ):

      raise NotImplementedError

      # make the sinks on the matrix by setting these columns to 0 & 1
      # for the source & sink respectively
      sink_prob_mat = copy(transition_probability_matrix)

      # for each idx in the sink basin set the column to identity vector
      for sink_node_idx in sink_basin:

          # set column to 0.0
          sink_prob_mat[:,sink_prob_mat] = 0.0

          # set the diagonal to 1.0
          sink_prob_mat[sink_node_idx, sink_node_idx] = 1.0


          pass


      return forward_committor_probs

  def committors_csnanalysis(
          countsmat,
          bound_basin,
          unbound_basin,
  ):

      from csnanalysis.csn import CSN

      csn = CSN(countsmat)

      committors = csn.calc_committors(
          [bound_basin, unbound_basin],
          labels=['bound', 'unbound']
      )

      forward_probs = committors[:,0]
      backward_probs = committors[:,1]

      return forward_probs, backward_probs

  def committors_pyemma(
          msm,
          bound_basin,
          unbound_basin,
  ):

      # this is a pyemma MSM which has a method
      forward_probs = msm.committor_forward(
          bound_basin,
          unbound_basin,
      )

      backward_probs = msm.committor_backward(
          bound_basin,
          unbound_basin,
      )

      return forward_probs, backward_probs

  def committors_msmb(
          transition_probability_mat,
          bound_basin,
          unbound_basin,
  ):
      from msmbuilder.tpt.committor import \
          _committors as msmb_forward_committors

      print("running msmbuilder function")

      forward_probs = msmb_forward_committors(
          bound_basin,
          unbound_basin,
          transition_probability_mat,
      )

      return forward_probs

  def calc_msm_committors(
          msm_id,
          gexp,
          bound_basin,
          unbound_basin,
          method='pyemma',
  ):
      """

      Parameters
      ----------

      bound_basin : list of node_id
          The node_id

  """

      if method == 'pyemma':

          pyemma_msm, trimming_mapping = load_msm(
              msm_id,
              gexp,
          )

          unbound_committors, bound_committors = committors_pyemma(
              pyemma_msm,
              bound_basin,
              unbound_basin,
          )

      elif method == 'msmb':

          print("Using MSMBuilder method")

          # this trimming mapping is a mapping of node_id -> trimmed_node_idx
          pyemma_msm, trimming_mapping = load_msm(
              msm_id,
              gexp,
          )

          # this matrix is trimmed
          transition_probability_mat = pyemma_msm.P

          # the bound and unbound basin are in terms of node_ids, so we
          # need to convert these to the trimmed node_idxs of the
          # trimmed matrix

          trim_bound_basin = [trimming_mapping[node_id]
                             for node_id in bound_basin]

          trim_unbound_basin = [trimming_mapping[node_id]
                               for node_id in unbound_basin]

          print("Running wrapper function")
          # here the basins are in node_idx and are in terms of the
          # transition probability matrix which is trimmed.
          unbound_committors = committors_msmb(
              transition_probability_mat,
              trim_bound_basin,
              trim_unbound_basin,
          )

          # just invert the unbound committors for the bound ones
          bound_committors = 1 - unbound_committors

          # NOTE: committors are in terms of node_idx

      elif method == 'csnanalysis':

          msn = get_msn(
              msm_id,
              gexp,
          )

          pyemma_msm, trimming_mapping = load_msm(
              msm_id,
              gexp,
          )

          subset_idxs = list(trimming_mapping.keys())

          trim_countsmat = np.zeros(
              (len(subset_idxs), len(subset_idxs),),
              dtype=msn.countsmat.dtype,
          )

          for full_idx, trimmed_idx in trimming_mapping.items():

              trim_countsmat[trimmed_idx, :] = msn.countsmat[full_idx, subset_idxs]
              trim_countsmat[:, trimmed_idx] = msn.countsmat[subset_idxs, full_idx]

          # calculate the committor probabilities
          unbound_committors, bound_committors = committors_csnanalysis(
              trim_countsmat,
              bound_basin,
              unbound_basin,
          )

      else:
          raise ValueError(f"Unkown method {method}")

      # in terms of node_idx
      return unbound_committors, bound_committors

  def predict_ts(
          msn,
          gexp,
          ts_id,
          committor_probs,
          trimming_mapping,
  ):

      ts_upper_bound = TS_SPECS[ts_id]['upper']
      ts_lower_bound = TS_SPECS[ts_id]['lower']

      # get nodes which are in the transition state, and convert to ids

      # boolean selection
      ts_sel_idxs = np.argwhere(
          (committor_probs >= ts_lower_bound) &
          (committor_probs <= ts_upper_bound)
      ).flatten()

      # ALERT: the committor probs are of the trimmed indices, so the
      # trimming mapping must be used to unconvert
      trimming_mapping_rev = {trim_idx : node_id
                              for node_id, trim_idx
                              in trimming_mapping.items()
                              }

      # convert these idxs to the node ids
      ts_node_ids = [
          trimming_mapping_rev[idx]
          for idx in ts_sel_idxs
      ]

      return ts_node_ids

  def msn_committors_ts(
          msn,
          gexp,
          ts_id,
  ):

      basin_id = TS_SPECS[ts_id]['basin_id']
      committor_method = TS_SPECS[ts_id]['committor_method']

      msm_id = BASIN_SPECS[basin_id]['msm_id']

      pyemma_msm, trimming_mapping = load_msm(
          msm_id,
          gexp,
      )

      # get the trimmed nodes and largest component nodes
      trimmed_node_ids = msn.node_groups[f'trimmed_nodes/msmid-{msm_id}/not_largest_component']
      untrimmed_node_ids = msn.node_groups[f'trimmed_nodes/msmid-{msm_id}/largest_component']

      # get the basins from the MSN; these are in node_id
      bound_basin = msn.node_groups[f'basin-{basin_id}/bound_basin']
      unbound_basin = msn.node_groups[f'basin-{basin_id}/unbound_basin']

      # calculate the committors

      # the unbound and bound committors are in terms of the node_idx
      unbound_committors, bound_committors = calc_msm_committors(
          msm_id,
          gexp,
          bound_basin,
          unbound_basin,
          method=committor_method,
      )

      # get the values for all the nodes as a dictionary
      # first initialize them with the trimmed nodes as nans
      bound_node_values = {
          node_id : np.nan
          for node_id
          in trimmed_node_ids
      }

      unbound_node_values = {
          node_id : np.nan
          for node_id
          in trimmed_node_ids
      }

      # then get the actual committors

      # convert the node_idx (given by the natural indexing)

      for node_id in untrimmed_node_ids:

          # then get the trimmed node id of them
          trim_node_idx = trimming_mapping[node_id]

          bound_node_values[node_id] = bound_committors[trim_node_idx]
          unbound_node_values[node_id] = unbound_committors[trim_node_idx]


      # unbound committors should be in terms of the node_idx.
      # the returned ts_node_ids are in terms of node_id
      ts_node_ids = predict_ts(
          msn,
          gexp,
          ts_id,
          unbound_committors,
          trimming_mapping,
      )

      # return as forward, backward, ts
      return unbound_node_values, bound_node_values, ts_node_ids



  def add_ts_to_msn(
          msn,
          ts_id,
          forward_node_probs,
          backward_node_probs,
          ts_node_ids,
  ):
      """Add data to the MSN for a transition state model.

      This will store a node group for the transition state model in a
      group of the format:

          'tsid-{ts_id}/TS'

      The forward/backwards committor probabilities will be saved as node
      attributes of the form:

          'tsid-{ts_id}/forward_committors'
          'tsid-{ts_id}/backwards_committors'

      We also make a convenience node attribute for coloring the network
      called the 'UBT'. This field is string valued and is one of:

          U : Unbound Basin
          B : Bound Basin
          T : Transisition State Ensemble Prediction
          N : Not in any of the other ones, but still in the MSM
          X : Not ergodically connected to the MSM

      This will be saved in a node attribute of the form:

          'tsid-{ts_id}/UBT'

      """

      new_msn = deepcopy(msn)

      # things we will be setting
      ts_node_group_key = f"tsid-{ts_id}/TS"
      forward_attr_key = f'tsid-{ts_id}/forward_committors'
      backward_attr_key = f'tsid-{ts_id}/backwards_committors'
      UBT_attr_key = f'tsid-{ts_id}/UBT'

      # get the basin id
      basin_id = TS_SPECS[ts_id]['basin_id']

      msm_id = BASIN_SPECS[basin_id]['msm_id']

      # set the group for the transition state nodes
      new_msn.set_node_group(
          ts_node_group_key,
          ts_node_ids,
      )

      # set the node attributes for committor probabilities

      # TODO: remove this. Should be done at the site of creation not
      # after passing it in here
      # # we have to pad it for the ones that don't have committors with
      # # nans.
      # missing_probs_node_ids = list(set(list(new_msn.graph.nodes.keys())) -
      #                           set(forward_node_probs.keys()))
      # for node_id in missing_probs_node_ids:
      #     forward_node_probs[node_id] = np.nan
      #     backward_node_probs[node_id] = np.nan

      new_msn.set_nodes_observable(
          forward_attr_key,
          forward_node_probs,
      )
      new_msn.set_nodes_observable(
          backward_attr_key,
          backward_node_probs,
      )

      # then we construct and set the UBT node attributes
      UBT_node_values = {}
      for node_id in new_msn.graph.nodes:

          if node_id in new_msn.node_groups[f'basin-{basin_id}/unbound_basin']:
              UBT_node_values[node_id] = 'U'

          elif node_id in new_msn.node_groups[f'basin-{basin_id}/bound_basin']:
              UBT_node_values[node_id] = 'B'

          # HACK: we can use the TS node group since we set it above
          elif node_id in new_msn.node_groups[ts_node_group_key]:
              UBT_node_values[node_id] = 'T'

          elif node_id in new_msn.node_groups[f'trimmed_nodes/msmid-{msm_id}/not_largest_component']:
              UBT_node_values[node_id] = 'X'

          else:
              UBT_node_values[node_id] = 'N'


      new_msn.set_nodes_observable(
          UBT_attr_key,
          UBT_node_values,
      )


      return new_msn
#+END_SRC


**** Basin Evaluation Plots

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def save_gexp_basin_eval_plots(
          gexp,
          case_0_key,
          case_1_key,
          ,**figs,
  ):

      for fig_key, fig in figs.items():

          save_gexp_fig(
              gexp,
              f"basin-eval/{fig_key}/case-0-{case_0_key}_case-1-{case_1_key}",
              fig,
          )
#+end_src


*** Transition State Analysis

**** TSE Ligand PCA

***** Getting & Computing Homology Atom COMS

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  @jlmem.cache
  def compute_legacy_all_lig_coms(
          node_ids,
          gexp,
  ):

      from geomm.centroid import centroid

      clust_assgs = get_legacy_cluster_canonical_traj_assignments()
      ts_microstates_trace = list(it.chain(*[
          clust_assgs[clust_id]
          for clust_id in node_ids
      ]))

      # get the HDF5
      wepy_h5 = get_legacy_h5()

      # get the microstates in batches by chunk and slice
      # recenter, slice homology, and get coms
      CHUNK_SIZE = 500

      chunks = [
          ts_microstates_trace[i:i + CHUNK_SIZE]
          for i in range(
                  0,
                  len(ts_microstates_trace),
                  CHUNK_SIZE,
          )
          ]

      com_chunks = []
      for i, chunk_trace in enumerate(chunks):

          print(f"Doing chunk {i}")

          with wepy_h5:

              traj_fields = wepy_h5.get_trace_fields(
                  chunk_trace,
                  [
                      'positions',
                      'box_vectors',
                  ],
              )

          chunk_coms = compute_traj_fields_hom_coms(
              gexp,
              traj_fields,
          )

          com_chunks.append(chunk_coms)


      coms = np.concatenate(com_chunks)


      return coms

  @jlmem.cache
  def compute_msn_all_lig_coms(
          gexp,
          csn_id,
          node_ids,
          homology_idxs,
  ):
      """Compute the centers of mass (coms) of the homologous ligand indices for
      a MacroStateNetwork.

      This computes from scratch rather than loading them.


      Returns
      -------

      coms : array of shape (N, 3)
         Where N is the number of all microstates across the node_ids.

      """

      from geomm.centroid import centroid

      # load the MSN with the H5
      print("Getting MSN H5")

      msn_h5 = get_h5_msn(
          csn_id,
          gexp,
      )

      # get the microstates from the TSE states, recenter and
      # superimpose them, then slice out only the homology indices
      # of the ligand from this
      with msn_h5:

          # we do this one node at a time since this will blow memory if we don't
          all_chunks = []
          for node_id in node_ids:

              print(f"Getting HOM COMS for node {node_id}")

              traj_fields = msn_h5.states_to_traj_fields([node_id])

              chunk_coms = compute_traj_fields_hom_coms(
                  gexp,
                  traj_fields,
              )

              all_chunks.append(chunk_coms)

      # combine them
      coms = np.concatenate(all_chunks)

      return coms
#+end_src

***** General Model Running

Functions that will compute PCA model for a selected set of gexps.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py

  def compute_tspca_model(
          coms,
          test_size=0.25,
  ):
      """Compute the PC model, centers of mass, and the score for the PCA
      model"""

      from sklearn.decomposition import PCA
      from sklearn.model_selection import train_test_split

      # split into training and test set
      train, test = train_test_split(coms, test_size=0.25)

      model = PCA(n_components=3)

      print(f"Fitting PCA model on {len(train)} samples")

      # train and get the mode projections
      train_projections = model.fit_transform(train)

      # keep a record of the individual mode scores
      mode_scores = []


      # loop over the modes and how much variance they explain
      for c_idx, c_var in enumerate(model.explained_variance_):

          # get the total explained variance of the mode
          var_ratio = model.explained_variance_ratio_[c_idx]

          mode_score = {
              'percent_explained_variance' : 100 * var_ratio,
              'percent_total_explained_variance' : 100 * c_var,
          }

          mode_scores.append(mode_score)

      print("Scoring the training set")

      # explain the model on the test set
      score = model.score(test)


      return model, score, mode_scores

  def compute_gexps_tspca_model(
          gexps,
          csn_id,
          ts_id,
          test_size=0.25,
  ):

      from geomm.centroid import centroid

      print("Getting the COMs for each gexp")

      # for each gexp get the COMs for the TSs
      all_ts_coms = []
      for gexp in gexps:

          print(gexp)

          sel_idxs = lig_selection_idxs(gexp)

          # UGLY: treat the legacy one differently
          if gexp in LEGACY_GEXPS:

              # get the microstate trace for the TSE nodes
              ts_cluster_idxs = get_legacy_ts_cluster_idxs()

              gexp_ts_coms = compute_legacy_all_lig_coms(
                  ts_cluster_idxs,
                  gexp,
              )

          # the modern gexps get handled this way
          else:

              # get the node_ids for the ts model
              msn_h5 = get_msn(
                  csn_id,
                  gexp,
              )

              tse_node_ids = msn_h5.node_groups[f"tsid-{ts_id}/TS"]

              print("Computing the COMs for the TSE")

              # calculate the COMs of the homology atoms in the ligand for all
              # of the microstates in the TS nodes
              gexp_ts_coms = compute_msn_all_lig_coms(
                  gexp,
                  csn_id,
                  tse_node_ids,
              )

          all_ts_coms.append(gexp_ts_coms)

      # concatenate the arrays of coms for each gexp
      all_ts_coms = np.concatenate(all_ts_coms)

      print("Computing the TSPCA model")

      pca_model, model_score, mode_scores = compute_tspca_model(
          all_ts_coms,
          test_size=test_size,
      )

      return pca_model, model_score, mode_scores

#+end_src

***** TS PCA Model Selection

Functions that will do the full model selection process.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def ts_pca_model_selection(
          gexps,
          csn_id,
          ts_id,
          test_size=0.25,
  ):

      # then generate the test matrix of subsamples
      num_samples = len(gexps)
      subsamples = []
      for subsample_size in range(1, num_samples + 1):

          combs = it.combinations(gexps, subsample_size)

          subsamples.extend(combs)

      # then we will make the tspca_id for each of them
      tspca_id_tmpl = "tsid-{ts_id}_gexps-{gexp_list}_testsize-{test_size}"

      tspca_ids = []
      for subsample in subsamples:

          gexp_list = '-'.join([str(gexp) for gexp in subsample])

          tspca_id = tspca_id_tmpl.format(
              ts_id=ts_id,
              gexp_list=gexp_list,
              test_size=test_size,
          )

          tspca_ids.append(tspca_id)

      # then we go through and make/retrieve the models for the tspca_ids
      model_results = {}
      for tspca_id, gexps in zip(tspca_ids, subsamples):

          # ALERT: don't do manual "caching" just let joblib take care of it

          print("Getting the PCA model for tspca_id:")
          print(tspca_id)

          # the 'coms' are the centers of mass
          model, model_score, mode_scores = compute_gexps_tspca_model(
              gexps,
              csn_id,
              ts_id,
              test_size=test_size,
          )

          model_results[tspca_id] = {
              'model' : model,
              'gexps' : gexps,
              'test_size' : test_size,
              'model_score' : model_score,
              'mode_scores' : mode_scores,
          }


      return model_results

#+end_src

***** Model Selection Plots

Functions to plot scores as a boxplot for the model scores.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py

  def plot_tspca_model_score(
          model_results,
  ):

      import matplotlib.pyplot as plt

      fig, axes = plt.subplots(1,1)

      scores = [res['model_score'] for res in model_results.values()]

      axes.boxplot(scores)

      return fig, axes

  def make_tspca_model_score_table(
          model_results,
  ):

      from collections import defaultdict

      df_d = defaultdict(list)
      for tspca_id, result_d in model_results.items():

          df_d['tspca_id'].append(
              tspca_id,
          )

          df_d['model_score'].append(
              result_d['model_score'],
          )

          df_d['gexps'].append(
              ','.join([gexp for gexp in result_d['gexps']]),
          )

          df_d['test_size'].append(
              result_d['test_size'],
          )

          for i, mode_score_d in enumerate(result_d['mode_scores']):

              df_d[f'mode_{i}_per-explained-variance'].append(
                  mode_score_d['percent_explained_variance'],
              )

              df_d[f'mode_{i}_per-total-explained-variance'].append(
                  mode_score_d['percent_total_explained_variance'],
              )

      model_score_df = pd.DataFrame(df_d)

      return model_score_df

#+END_SRC


***** Compute Projections for a PCA model

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_tasks.py
  def observable_traj_fields_hom_com_projections(
          model,
          gexp,
          traj_fields
  ):
      """Function for computing the projections of a TSPCA model over the
      coms of the ligand homology atoms.

      """

      coms = compute_traj_fields_hom_coms(
          gexp,
          traj_fields,
      )

      pc_projections = compute_traj_hom_com_projections(
          model,
          coms,
      )

      return pc_projections

  def compute_traj_fields_hom_coms(
          gexp,
          traj_fields,
  ):
      """Returns the centers of mass (COMS) for trajectory fields for a
      given gexp."""

      from geomm.centroid import centroid

      if gexp in LEGACY_GEXPS:
          rep = 'real_rep'
      else:
          rep = 'main_rep'

      # get the ligand homology atoms for the rep chosen
      sel_idxs = lig_selection_idxs(gexp)
      homology_idxs = sel_idxs[f'{rep}/ligand/homology']

      hom_positions = recenter_superimpose_traj(
          traj_fields,
          gexp,
          rep,
      )[0][:,homology_idxs,:]

      gc.collect()

      coms = np.array([centroid(frame) for frame in hom_positions])

      return coms

  def compute_traj_com_projections(
          model,
          coms,
  ):
      """For a model and some centers of mass make a trajectory of the
      centers of mass using a phony topology and then calculate the
      projection values of these points for each PC.

      Parameters
      ----------

      model : scikit learn PCA model

      coms : array of shape (N, 3)
          Center of mass positions.


      Returns
      -------

      traj : mdtraj.Trajectory
          A trajectory of the COMs for the ligands.

      pc_projections : list of array of float
          Each element of the list is for each of the modes (in order of
          the model). The values of each array correspond to the frames
          of the trajectory and are scalar values which are the
          projected value of that frame onto the PC.

      """

      # first we get their projections which is just easier to re-transform
      # all of them
      com_projections = model.transform(coms)

      pc_projections = []
      # make a pdb traj for each component
      for pc_idx in range(model.n_components_):

          # reshape the projection value for this PC, which can be used
          # to color things
          pc_projection = com_projections[:, pc_idx].reshape((1, com_projections.shape[0]))

          pc_projections.append(pc_projections)


      return pc_projections


  def make_coms_traj(coms):
      """Make an mdtraj trajectory object from single particle center of mass positions."""

      # we get a topology for the coms as atoms in a single topology
      top = n_atom_mdj_top(coms.shape[0])

      # then make a trajectory using the coms xyzs, after we reshape to trajectory style
      coms_frames = np.reshape(
          coms,
          (1, coms.shape[0], 3)
      )

      traj = mdj.Trajectory(coms_frames, top)

      return traj



  def compute_gexp_tspca_projections_csn(
          msn,
          tse_node_ids,
          contigtree,
          homology_idxs,
          lig_id,
          pca_model,
  ):

      ## Part 2. Getting projections onto TS cluster centers
      #---------------------------------------------------------------------------
      # get the actual cluster centers only so we can project them

      # get the trace of all of the node centers
      node_center_trace_d = msn.get_nodes_attribute('center_idx')


      # First get only the TS centers

      # reshaping this into two lists ordered the same

      # zip the recs and their nodes
      ts_center_recs = [
          (node_id, rec)
          for node_id, rec
          in node_center_trace_d.items()
          if node_id in tse_node_ids
      ]

      # then unzip so we can reference them separately
      ts_center_trace_node_ids = [node_id for node_id, rec in ts_center_recs]
      ts_center_trace = [rec for node_id, rec in ts_center_recs]

      # get the trajectory fields, this is 'main_rep'
      with contigtree.wepy_h5 as wepy_h5:

          ts_center_traj_fields = wepy_h5.get_trace_fields(
              ts_center_trace,
              ['positions', 'box_vectors'],
          )

      ts_center_hom_positions = recenter_superimpose_traj(
          ts_center_traj_fields,
          lig_id,
          'main_rep'
      )[0][:,homology_idxs,:]

      # get their ligand COMs
      ts_center_coms = np.array([
          centroid(frame)
          for frame
          in ts_center_hom_positions
      ])

      # project them onto the modes
      ts_center_projections = model.transform(ts_center_coms)


      # then write out a trajectory for all the COMs as single atoms

      # first we get their projections which is just easier to re-transform
      # all of them
      ts_center_com_projections = model.transform(ts_center_coms)

      # we get a topology for the coms as atoms in a single topology
      ts_center_top = n_atom_mdj_top(ts_center_com_projections.shape[0])

      # then make a trajectory using the coms xyzs, after we reshape to trajectory style
      ts_center_coms_frames = np.reshape(ts_center_coms, (1, ts_center_coms.shape[0], 3))
      traj = mdj.Trajectory(ts_center_coms_frames, ts_center_top)

      # make a pdb traj for each component
      for pc_idx in range(model.n_components_):

          # add in the extra field for each mode so we can visualize, in pdb
          # mode so we can have the bfactors
          traj.save_pdb('data/ts_pca/ts_center_coms_pc_{}_lig-{}.pdb'.format(pc_idx, LIG_ID),
                        bfactors=ts_center_projections[:, pc_idx].reshape((1, ts_center_projections.shape[0])))



      # Option B: All of the centers

      # zip the recs and their nodes
      center_recs = [(node_id, rec) for node_id, rec in node_center_recs.items()]

      # then unzip so we can reference them separately
      center_trace_node_ids = [node_id for node_id, rec in center_recs]
      center_trace = [rec for node_id, rec in center_recs]

      # get the trajectory fields
      with contigtree.wepy_h5 as wepy_h5:
          center_traj_fields = wepy_h5.get_trace_fields(center_trace,
                                                        ['positions', 'box_vectors'])

      # TODO: audit main_rep here
      center_hom_positions = recenter_superimpose_traj(center_traj_fields,
                                                       LIG_ID, 'main_rep')[0][:,hom_idxs,:]
      # get their ligand COMs
      center_coms = np.array([centroid(frame) for frame in center_hom_positions])

      # project them onto the modes
      center_projections = model.transform(center_coms)


      return
#+END_SRC


***** Network Colorings

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  def junk():
      ## Params

      ## Part 3a: Render onto network
      #---------------------------------------------------------------------------

      # now set these as observables in the network
      # pcs_obs = {}
      for pc_idx in range(model.n_components_):
          pc_obs = {}
          for idx, node_id in enumerate(center_trace_node_ids):

              # # if it is not a ts node then just set it as a nan
              # if node_id not in ts_center_trace_node_ids:
              #     pc_obs[node_id] = 0. #np.nan

              # # get the projections of the cluster centers from the transition state
              # else:
              #     ts_idx = ts_center_trace_node_ids.index(node_id)
              #     pc_obs[node_id] = ts_center_projections[ts_idx, pc_idx]

              pc_obs[node_id] = center_projections[idx, pc_idx]


          # pcs_obs[pc_idx] = pc_obs

          # TODO use the key for the TS model, i'm not doing it here because
          # it is too busy for this data exploration task

          # set as an observable in the network
          net.set_nodes_observable('ts_PC-{}'.format(pc_idx), pc_obs)


      # save the network with this visualization
      gephi_graph = load_gephi_graph(MODEL, LIG_ID, LAG_TIME, name_pattern='main')
      net = update_network_from_gephi(net, gephi_graph, layout_name='main')

      save_gexf(MODEL, LIG_ID, LAG_TIME, net)

#+end_src

***** free energy plots

For this you must have already calculated the projections and set them
as observables:

#+BEGIN_SRC bash
python -m seh_pathway_hopping.execution.pc_projections_observable 
#+END_SRC

#+BEGIN_SRC python
  ## Part 4: Probability and Sampling Density Plots

  from seh_pathway_hopping._tasks import *

  LIG_ID = 3
  MODEL = 'kcenters_canberra_bs_lig_pair_dists'
  LAG_TIME = 2
  BOUND_CUTOFF = 0.07
  TS_KEY = 'TS/warp+featdist-0.07'

  # load the legacy 


  contigtree = get_contigtree(LIG_ID)
  net = lig_state_network_ts(LIG_ID, MODEL, LAG_TIME, bound_cutoff=BOUND_CUTOFF)

  # see them for each span
  # plot_ts_pc0_span_fe(LIG_ID)
  # plot_ts_pc1_span_fe(LIG_ID)

  # plot the entire thing
  plot_contigtrees_observable_fe([contigtree], 'TS-warp+featdist-0.07_pc-0')
  plot_contigtrees_observable_fe([contigtree], 'TS-warp+featdist-0.07_pc-1')

  # plot only the distributions for the transition state
  ts_trace = []
  for node_id in net.node_groups[TS_KEY]:
      assgs = net.node_assignments(node_id)
      ts_trace.extend(assgs)

  # get the trace for the transition state

  plot_contigtrees_trace_observable_fe([contigtree], [ts_trace],
                                       'TS-warp+featdist-0.07_pc-0')
  plot_contigtrees_trace_observable_fe([contigtree], [ts_trace],
                                       'TS-warp+featdist-0.07_pc-1')
#+END_SRC


***** (legacy) free energy plots

For this you must have already calculated the projections and set them
as observables:

#+BEGIN_SRC python
  ## Part 4: Probability and Sampling Density Plots
  import numpy as np
  import matplotlib.pyplot as plt

  from seh_pathway_hopping._tasks import *

  from wepy.analysis.profiles import free_energy_profile, contigtrees_bin_edges

  LIG_ID = 3
  MODEL = 'kcenters_canberra_bs_lig_pair_dists'
  LAG_TIME = 2
  BOUND_CUTOFF = 0.07
  TS_KEY = 'TS/warp+featdist-0.07'

  # load the legacy HDF5
  wepy_h5 = get_legacy_h5()

  contigtree = get_contigtree(LIG_ID)


  pc0_obs_name = 'lig-3_TS-warp+featdist-0.07_pc-0'
  pc0_field_key = 'observables/lig-3_TS-warp+featdist-0.07_pc-0'

  target_pc0_obs_name = 'TS-warp+featdist-0.07_pc-0'
  target_pc0_field_key = 'observables/TS-warp+featdist-0.07_pc-0'

  pc1_obs_name = 'lig-3_TS-warp+featdist-0.07_pc-1'
  pc1_field_key = 'observables/lig-3_TS-warp+featdist-0.07_pc-1'

  target_pc1_obs_name = 'TS-warp+featdist-0.07_pc-1'
  target_pc1_field_key = 'observables/TS-warp+featdist-0.07_pc-1'


  ### Full Profile


  ## PC 0

  # get the bin edges by combining all of the data together
  pc0_all_values = []
  pc0_all_weights = []
  # the legacy values
  with wepy_h5:

      pc0_legacy_weights = []
      pc0_legacy_values = []
      for run_idx in wepy_h5.run_idxs:
          for traj_idx in range(wepy_h5.num_run_trajs(run_idx)):
              pc0_legacy_weights.append(wepy_h5.traj(run_idx, traj_idx)['weights'][:])
              pc0_legacy_values.append(wepy_h5.traj(run_idx, traj_idx)[pc0_field_key][:])

      pc0_legacy_values = np.concatenate(pc0_legacy_values)
      pc0_legacy_weights = np.concatenate(pc0_legacy_weights).flatten()


  # the contigtree
  with contigtree:
      pc0_target_weights = np.concatenate([fields['weights']
                                        for fields
                                  in contigtree.wepy_h5.iter_trajs_fields(['weights'])])
      pc0_target_values = np.concatenate([fields[target_pc0_field_key]
                                        for fields
                                  in contigtree.wepy_h5.iter_trajs_fields([target_pc0_field_key])])

      pc0_target_weights = pc0_target_weights.flatten()


  pc0_all_values.append(pc0_legacy_values)
  pc0_all_values.append(pc0_target_values)

  pc0_all_values = np.concatenate(pc0_all_values)

  pc0_bin_edges = np.histogram_bin_edges(pc0_all_values, bins='auto')


  pc0_target_profile, _ = free_energy_profile(pc0_target_weights, pc0_target_values, bins=pc0_bin_edges)

  pc0_legacy_profile, _ = free_energy_profile(pc0_legacy_weights, pc0_legacy_values, bins=pc0_bin_edges)

  plot_fe_profiles([pc0_target_profile, pc0_legacy_profile], pc0_bin_edges,
                   title='Free Energies',
                   observable_label='PC-0',
                   labels=['Lig 3', 'TPPU'])

  # plot_fe_profile(target_profile, bin_edges)
  # plot_fe_profile(legacy_profile, bin_edges)

  ## PC 1


  # get the bin edges by combining all of the data together
  pc1_all_values = []
  pc1_all_weights = []
  # the legacy values
  with wepy_h5:

      pc1_legacy_weights = []
      pc1_legacy_values = []
      for run_idx in wepy_h5.run_idxs:
          for traj_idx in range(wepy_h5.num_run_trajs(run_idx)):
              pc1_legacy_weights.append(wepy_h5.traj(run_idx, traj_idx)['weights'][:])
              pc1_legacy_values.append(wepy_h5.traj(run_idx, traj_idx)[pc1_field_key][:])

      pc1_legacy_values = np.concatenate(pc1_legacy_values)
      pc1_legacy_weights = np.concatenate(pc1_legacy_weights).flatten()


  # the contigtree
  with contigtree:
      pc1_target_weights = np.concatenate([fields['weights']
                                        for fields
                                  in contigtree.wepy_h5.iter_trajs_fields(['weights'])])
      pc1_target_values = np.concatenate([fields[target_pc1_field_key]
                                        for fields
                                  in contigtree.wepy_h5.iter_trajs_fields([target_pc1_field_key])])

      pc1_target_weights = pc1_target_weights.flatten()


  pc1_all_values.append(pc1_legacy_values)
  pc1_all_values.append(pc1_target_values)

  pc1_all_values = np.concatenate(pc1_all_values)

  pc1_bin_edges = np.histogram_bin_edges(pc1_all_values, bins='auto')


  pc1_target_profile, _ = free_energy_profile(pc1_target_weights, pc1_target_values, bins=pc1_bin_edges)

  pc1_legacy_profile, _ = free_energy_profile(pc1_legacy_weights, pc1_legacy_values, bins=pc1_bin_edges)

  plot_fe_profiles([pc1_target_profile, pc1_legacy_profile], pc1_bin_edges,
                   title='Free Energies',
                   observable_label='PC-1',
                   labels=['Lig 3', 'TPPU'])


  ### TS only FE profiles

  # get the ts trace for legacy TPPU

  ts_trace = []

  clust_assgs = get_legacy_cluster_canonical_traj_assignments()
  for node_id in get_legacy_ts_cluster_idxs():
      ts_trace.extend(clust_assgs[node_id])

  # get the target ligand ts  trace
  net = lig_state_network_ts(LIG_ID, MODEL, LAG_TIME, bound_cutoff=BOUND_CUTOFF)
  target_ts_trace = []
  for node_id in net.node_groups[TS_KEY]:
      assgs = net.node_assignments(node_id)
      target_ts_trace.extend(assgs)

  ## PC 0

  # now retrieve the values for only these things

  # get the bin edges by combining all of the data together
  pc0_ts_all_values = []
  pc0_ts_all_weights = []
  # the legacy values
  with wepy_h5:


      pc0_ts_legacy_weights = wepy_h5.get_trace_fields(ts_trace,
                                                                  ['weights'])['weights']

      pc0_ts_legacy_values = wepy_h5.get_trace_fields(ts_trace,
                                                      [pc0_field_key])[pc0_field_key]

      pc0_ts_legacy_weights = pc0_ts_legacy_weights.flatten()

  # the contigtree
  with contigtree:
      pc0_ts_target_weights = contigtree.wepy_h5.get_trace_fields(target_ts_trace,
                                                                  ['weights'])['weights']

      pc0_ts_target_values = contigtree.wepy_h5.get_trace_fields(target_ts_trace,
                                                 [target_pc0_field_key])[target_pc0_field_key]

      pc0_ts_target_weights = pc0_ts_target_weights.flatten()


  pc0_ts_all_values.append(pc0_ts_legacy_values)
  pc0_ts_all_values.append(pc0_ts_target_values)

  pc0_ts_all_values = np.concatenate(pc0_ts_all_values)

  pc0_ts_bin_edges = np.histogram_bin_edges(pc0_ts_all_values, bins='auto')


  # plot

  pc0_ts_target_profile, _ = free_energy_profile(pc0_ts_target_weights, pc0_ts_target_values,
                                                 bins=pc0_ts_bin_edges)

  pc0_ts_legacy_profile, _ = free_energy_profile(pc0_ts_legacy_weights, pc0_ts_legacy_values,
                                                 bins=pc0_ts_bin_edges)

  plot_fe_profiles([pc0_ts_target_profile, pc0_ts_legacy_profile], pc0_ts_bin_edges,
                   title='TS Free Energies',
                   observable_label='PC-0',
                   labels=['Lig 3', 'TPPU'])



  ## PC 1

  # now retrieve the values for only these things

  # get the bin edges by combining all of the data together
  pc1_ts_all_values = []
  pc1_ts_all_weights = []
  # the legacy values
  with wepy_h5:


      pc1_ts_legacy_weights = wepy_h5.get_trace_fields(ts_trace,
                                                                  ['weights'])['weights']

      pc1_ts_legacy_values = wepy_h5.get_trace_fields(ts_trace,
                                                      [pc1_field_key])[pc1_field_key]

      pc1_ts_legacy_weights = pc1_ts_legacy_weights.flatten()

  # the contigtree
  with contigtree:
      pc1_ts_target_weights = contigtree.wepy_h5.get_trace_fields(target_ts_trace,
                                                                  ['weights'])['weights']

      pc1_ts_target_values = contigtree.wepy_h5.get_trace_fields(target_ts_trace,
                                                 [target_pc1_field_key])[target_pc1_field_key]

      pc1_ts_target_weights = pc1_ts_target_weights.flatten()


  pc1_ts_all_values.append(pc1_ts_legacy_values)
  pc1_ts_all_values.append(pc1_ts_target_values)

  pc1_ts_all_values = np.concatenate(pc1_ts_all_values)

  pc1_ts_bin_edges = np.histogram_bin_edges(pc1_ts_all_values, bins='auto')


  # plot

  pc1_ts_target_profile, _ = free_energy_profile(pc1_ts_target_weights, pc1_ts_target_values,
                                                 bins=pc1_ts_bin_edges)

  pc1_ts_legacy_profile, _ = free_energy_profile(pc1_ts_legacy_weights, pc1_ts_legacy_values,
                                                 bins=pc1_ts_bin_edges)

  plot_fe_profiles([pc1_ts_target_profile, pc1_ts_legacy_profile], pc1_ts_bin_edges,
                   title='TS Free Energies',
                   observable_label='PC-1',
                   labels=['Lig 3', 'TPPU'])


#+END_SRC



***** (legacy) centers

#+BEGIN_SRC python
  import gc
  import os
  import os.path as osp
  import functools

  import numpy as np
  import joblib
  import pandas as pd

  from sklearn.decomposition import PCA
  from sklearn.model_selection import train_test_split

  import mdtraj as mdj

  from wepy.analysis.network import MacroStateNetwork
  from wepy.hdf5 import WepyHDF5
  from geomm.centroid import centroid

  import mdtraj as mdj

  from wepy.util.util import traj_box_vectors_to_lengths_angles

  from geomm.superimpose import superimpose
  from geomm.grouping import group_pair
  from geomm.centering import center_around
  from geomm.centroid import centroid


  from seh_pathway_hopping._tasks import *


  ## Params

  MODEL = 'kcenters_canberra_bs_lig_pair_dists'
  LIG_ID = 17
  LAG_TIME = 2

  BOUND_CUTOFF = 0.07

  TS_MODEL = 'TS/warp+featdist-0.07'

  wepy_h5 = get_legacy_h5()

  sel_idxs = lig_selection_idxs(LIG_ID)

  model = get_model(TS_MODEL, 3)

  bs_idxs = sel_idxs['real_rep/binding_site']
  lig_idxs = sel_idxs['real_rep/ligand']
  hom_idxs = sel_idxs['real_rep/ligand/homology']

  ## Part 1: Get COMs of the legacy data

  # ts_centers_trace = []
  # # use only one structure from each ts node, we just grab the first one
  # clust_assgs = get_legacy_cluster_canonical_traj_assignments()
  # for node_id in get_legacy_ts_cluster_idxs():
  #     ts_centers_trace.append(clust_assgs[node_id][0])

  ts_trace = []
  ts_center_trace = []

  # use only one structure from each ts node, we just grab the first one
  clust_assgs = get_legacy_cluster_canonical_traj_assignments()
  for node_id in get_legacy_ts_cluster_idxs():
      ts_trace.extend(clust_assgs[node_id])
      ts_center_trace.append(clust_assgs[node_id][0])

  # get the trajectory fields
  with wepy_h5 as wepy_h5:
      center_traj_fields = wepy_h5.get_trace_fields(ts_trace,
                                                    ['positions', 'box_vectors'])


  box_lengths, _ = traj_box_vectors_to_lengths_angles(center_traj_fields['box_vectors'])

  ref_traj = mdj.load_pdb(osp.join(data_path(), 'top/{}/real_rep_center_ref.pdb'.format(LIG_ID)))

  centered_ref_positions = ref_traj.xyz[0]

  ## regroup, center, and superimpose the frames

  # group the pair of ligand and binding site together in the same image
  grouped_positions = [group_pair(positions, box_lengths[idx],
                                      bs_idxs, lig_idxs)
                for idx, positions in enumerate(center_traj_fields['positions'])]

  # center all the positions around the binding site
  centered_positions = [center_around(positions, bs_idxs)
                        for idx, positions in enumerate(grouped_positions)]

  # then superimpose the binding sites
  sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=bs_idxs)[0]
                   for pos in centered_positions])


  center_hom_positions = sup_positions[:,hom_idxs,:]

  # get their ligand COMs
  coms = np.array([centroid(frame) for frame in center_hom_positions])

  # project them onto the modes
  projections = model.transform(coms)

  ## Part 1b: Render into trajectories for 3D viz
  #---------------------------------------------------------------------------

  # then write out a trajectory for all the COMs as single atoms

  # first we get their projections which is just easier to re-transform
  # all of them
  com_projections = model.transform(coms)

  # we get a topology for the coms as atoms in a single topology
  top = n_atom_mdj_top(com_projections.shape[0])

  # then make a trajectory using the coms xyzs, after we reshape to trajectory style
  coms_frames = np.reshape(coms, (1, coms.shape[0], 3))
  traj = mdj.Trajectory(coms_frames, top)

  # make a pdb traj for each component
  for pc_idx in range(model.n_components_):

      # add in the extra field for each mode so we can visualize, in pdb
      # mode so we can have the bfactors
      traj.save_pdb('data/ts_pca/ts_coms_pc_{}_lig-{}.pdb'.format(pc_idx, LIG_ID),
                    bfactors=com_projections[:, pc_idx].reshape((1, com_projections.shape[0])))



  # do just the centers

  # get the trajectory fields
  with wepy_h5 as wepy_h5:
      only_center_traj_fields = wepy_h5.get_trace_fields(ts_center_trace,
                                                    ['positions', 'box_vectors'])


  center_box_lengths, _ = traj_box_vectors_to_lengths_angles(only_center_traj_fields['box_vectors'])

  ## regroup, center, and superimpose the frames

  # group the pair of ligand and binding site together in the same image
  grouped_positions = [group_pair(positions, box_lengths[idx],
                                      bs_idxs, lig_idxs)
                for idx, positions in enumerate(only_center_traj_fields['positions'])]

  # center all the positions around the binding site
  centered_positions = [center_around(positions, bs_idxs)
                        for idx, positions in enumerate(grouped_positions)]

  # then superimpose the binding sites
  sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=bs_idxs)[0]
                   for pos in centered_positions])


  only_center_hom_positions = sup_positions[:,hom_idxs,:]

  # get their ligand COMs
  center_coms = np.array([centroid(frame) for frame in only_center_hom_positions])

  # project them onto the modes
  center_projections = model.transform(center_coms)

  ## Part 1b: Render into trajectories for 3D viz
  #---------------------------------------------------------------------------

  # then write out a trajectory for all the COMs as single atoms

  # we get a topology for the coms as atoms in a single topology
  centers_top = n_atom_mdj_top(center_projections.shape[0])

  # then make a trajectory using the coms xyzs, after we reshape to trajectory style
  centers_coms_frames = np.reshape(center_coms, (1, center_coms.shape[0], 3))
  centers_traj = mdj.Trajectory(centers_coms_frames, centers_top)

  # make a pdb traj for each component
  for pc_idx in range(model.n_components_):

      # add in the extra field for each mode so we can visualize, in pdb
      # mode so we can have the bfactors
      centers_traj.save_pdb('data/ts_pca/ts_center_coms_pc_{}_lig-{}.pdb'.format(pc_idx, LIG_ID),
                    bfactors=center_projections[:, pc_idx].reshape((1, center_projections.shape[0])))



  ## Part 3a: Render onto network
  #---------------------------------------------------------------------------


  # get a trace for all of the cluster centers

  trace_node_ids = []
  center_trace = []

  # use only one structure from each ts node, we just grab the first one
  clust_assgs = get_legacy_cluster_canonical_traj_assignments()

  for node_id, assgs in clust_assgs.items():
      center_trace.append(assgs[0])
      trace_node_ids.append(node_id)

  with wepy_h5 as wepy_h5:
      center_traj_fields = wepy_h5.get_trace_fields(center_trace,
                                                    ['positions', 'box_vectors'])


  center_box_lengths, _ = traj_box_vectors_to_lengths_angles(center_traj_fields['box_vectors'])

  ## regroup, center, and superimpose the frames

  # group the pair of ligand and binding site together in the same image
  grouped_positions = [group_pair(positions, center_box_lengths[idx],
                                      bs_idxs, lig_idxs)
                for idx, positions in enumerate(center_traj_fields['positions'])]

  # center all the positions around the binding site
  centered_positions = [center_around(positions, bs_idxs)
                        for idx, positions in enumerate(grouped_positions)]

  # then superimpose the binding sites
  sup_positions = np.array([superimpose(centered_ref_positions, pos, idxs=bs_idxs)[0]
                   for pos in centered_positions])


  center_hom_positions = sup_positions[:,hom_idxs,:]

  # get their ligand COMs
  center_coms = np.array([centroid(frame) for frame in center_hom_positions])

  # project them onto the modes
  center_projections = model.transform(center_coms)


  # load the nodes table and add them in
  nodes_table_path = osp.join(legacy_project_path(), 'tppu_net_nodes.csv')
  nodes_df = pd.read_csv(nodes_table_path, index_col=0)

  # now set these as observables in the network
  # pcs_obs = {}
  for pc_idx in range(model.n_components_):
      pc_col = []
      for node_id, row in nodes_df.iterrows():

          idx = trace_node_ids.index(node_id)
          pc_col.append(center_projections[idx, pc_idx])

      nodes_df['ts_PC-{}'.format(pc_idx)] = pc_col


  nodes_df.to_csv(nodes_table_path)
#+END_SRC



** Execution

You can run them like this:

#+begin_src bash
python -m project_name.execution.my_execution_script
#+end_src

**** Executors

***** Execute Function

Either connect to an existing dask cluster or start one up locally.

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/__init__.py
  import sys

  import click

  from dask.distributed import Client, LocalCluster

  def parse_kwargs(kwarg_strings):
      """Parse kwargs on command line of the form key=value.

      Parameters
      ----------

      kwarg_strings : list of str
          The kwarg strings each is like 'key=value'

      Returns
      -------
      kwargs : dict of str : str

      """

      kwargs = {}
      for kwarg_str in kwarg_strings:
          key, value = kwarg_str.split('=')
          kwargs[key] = value

      return kwargs


  def execute(ctx, func):
      """Function to execute the work function. This is what accepts command
      line arguments to connect a function to an executor.

      You may run immediately with this in the __main__ block:

      execute(func)

      Or partially evaluate as a thunk and invoke later in a __main__ block:

      functools.partial(execute, func)

      """

      if sys.argv[-1] == '-h' or sys.argv[-1] == '--help':
          print("Usage: execute <method> [key=value, ...]")

      cluster_address = sys.argv[1]
      kwargs = parse_kwargs(sys.argv[2:])

      DASHBOARD_PORT = 9998
      N_WORKERS = 2
      PROCESSES = False

      worker_kwargs = {
          'memory_limit' : '8GB',
          'memory_spill_fraction' : 1.0,
      }

      # if the address is None just start a local cluster, with the default options
      if cluster_address == ':local':

          # start a local cluster
          cluster = LocalCluster(processes=PROCESSES,
                                 n_workers=N_WORKERS,
                                 dashboard_address=f":{DASHBOARD_PORT}",
                                 ,**worker_kwargs)
          print(f"Ad hoc cluster online. Dashboard on port {DASHBOARD_PORT}")

          client = Client(cluster)

      # otherwise just connect
      else:
          client = Client(cluster_address)


      func(client, **kwargs)
#+END_SRC

***** Schedulers
:BACKLINKS:
[2020-10-21 Wed 16:00] <- [[*Test: Compute Box Volumes][Test: Compute Box Volumes]]
:END:

The servers set up a service that you can then connect to with the
execute function.

****** TODO Local Scheduler

This is a bare dask scheduler that does the scheduling on its own.

#+begin_src bash

#+end_src

****** Dask SLURM Scheduler
:BACKLINKS:
[2020-10-21 Wed 15:57] <- [[*Test: Compute Box Volumes][Test: Compute Box Volumes]]
:END:

Manage dask servers and stuff.

******* Starting the scheduler: from sbatch script

On HPCC just submit this job:

#+begin_src bash
  pyenv shell miniconda3-latest
  (cd hpcc/analysis/dask_server && sbatch submissions/start_server.sbatch)
#+end_src

******* Connecting to Scheduler and Dashboard on HPCC

WARNING: this doesnt' work from a local computer probably because of
HPCC firewalls and such. You must connect from HPCC.

You can test your connection if you know the node by running this:

#+begin_src bash
  test_dask_connection_hpcc () {

      node=$1
      scheduler_port=$2

      sched_address="${node}:${scheduler_port}"

      echo "${sched_address}"

      python -c "from dask.distributed import Client; print(Client(\"${sched_address}\"))"

  }
#+end_src

e.g.:

#+begin_src bash
test_dask_connection_hpcc nvl-002 43881
#+end_src

To use it for analysis just give the address of the scheduler.

You will still want to tunnel to the node for the dashboard. This is
how to do that:

NOTE: this will also set up the tunnel for the scheduler but it won't
work.

If you want to see the dashboard or connect to the scheduler locally
we will need to forward ports to the local machine.

#+begin_src bash
  start_dask_slurm_tunnel_gen () {

      # The general version of starting a tunnel with a scheduler on any
      # port

      host="$1"
      scheduler_port="$2"
      loc_scheduler_port="$3"

      dash_port="$4"
      loc_dash_port="$5"


      echo "Dashboard: http://localhost:${loc_dash_port}"

      autossh \
          -M 50500 -- \
          -N \
          -F $HOME/.ssh/config \
          -J hpcc.dev \
          -L "${loc_scheduler_port}:127.0.0.1:${scheduler_port}" \
          -L "${loc_dash_port}:localhost:${dash_port}" \
          "${host}"

  }



  start_dask_slurm_tunnel_main () {

      # start the tunnel with the ports configured in the main
      # submission script and to the standard local ports

      host="$1"
      # port on the remote
      scheduler_port="$2"

      # always the same port here
      local_scheduler_port=42805
      dash_port=45708
      start_dask_slurm_tunnel_gen "$host" \
                                  "$scheduler_port" "$local_scheduler_port" \
                                  "$dash_port" "$dash_port"
  }
  test_dask_connection_main () {

      scheduler_port=42805

      sched_address="localhost:${scheduler_port}"

      python -c "from dask.distributed import Client; Client(\"${sched_address}\")"

  }

#+end_src

Then make the autossh tunnel like this mapping to reasonable ports locally:

#+begin_src bash
start_dask_slurm_tunnel lac-001 123212 12001 2343 12002
#+end_src

Or just use the prefilled in defaults which only requires the port of
the scheduler on the remote:

#+begin_src bash
start_dask_slurm_tunnel_main nvl-002 43881
#+end_src


******** COMMENT Long Explanation

For example to get it to the host "superior":

#+BEGIN_SRC bash
local_port='9999'
ssh -N -J "lotzsamu@hpcc.msu.edu" -L "${local_port}:localhost:38941" "lotzsamu@dev-intel16-k80"
#+END_SRC

And to run in the background:

#+BEGIN_SRC bash
ssh -f -N -J "lotzsamu@hpcc.msu.edu" -L "${local_port}:localhost:38941" "lotzsamu@dev-intel16-k80"
#+END_SRC




For convenience and instead of having to forward ports on every
machine, we just set up a URL accessible from anywhere using
localtunnel from my machine on the same network as HPCC.

#+BEGIN_SRC bash
lt --port "$local_port"
#+END_SRC

Which will give you an accessible URL.

THis is in some functions which are in my config. 

To make a tunnel:

#+BEGIN_SRC bash
hpcc_superior_tunnel $remote_port $local_port
#+END_SRC

To list current tunnels:

#+BEGIN_SRC bash
ls_tunnels
#+END_SRC

To shut a tunnel down:

#+BEGIN_SRC bash
clear_tunnel $remote_port $local_port
#+END_SRC







******* Starting a Scheduler: Full example

We can launch a server on SLURM using the following command executed
on HPCC:

#+BEGIN_SRC bash
python -m seh_pathway_hopping.scheduler
#+END_SRC

There are options, which you can see with a ~--help~ flag.

We typically run with at least these set for consistency and in adapt
mode so it will automatically spawn workers without having to specify
them:

#+begin_src bash
  python -m seh_pathway_hopping.scheduler \
         --adapt-max 20 \
         --dash-port 45708 \
         --scheduler-port 42805
#+end_src

Once that is running you will some output like:

#+BEGIN_EXAMPLE
Scheduler address: tcp://10.3.8.48:42805
Dashboard port: http://10.3.8.48:45708/status
#+END_EXAMPLE

For the endpoints.

On HPCC you might set the address of it as a shell variable:
#+BEGIN_SRC bash
scheduler_address="10.3.8.48:42805"
#+END_SRC

You would then set the local and remote ports to something like this
for the dashboard port forwarding on your local machine:

#+BEGIN_SRC bash
remote_port='45708'
local_port='9999'
#+END_SRC

Where the local port is just where you want to have the dashboard
locally.

**** Execution Scripts

***** Data Preparation and Cleaning

****** Generate Legacy HDF5 linker

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/make_legacy_results.py
  if __name__ == "__main__":

      from seh_pathway_hopping._tasks import (
          save_real_rep_top,
          make_legacy_results_linker_h5,
      )

      # create the reference topology from the source materials in the
      # current project
      save_real_rep_top()

      # NOTE: Don't do this
      # and the centered one (must come after the first one)
      # save_centered_real_rep_top()

      # using that topology and some other stuff make a linker HDF5 with
      # the proper headers and such for the new dataset
      make_legacy_results_linker_h5()
#+END_SRC

***** Data Post Processing

****** Patch Continuations to HDF5

#+begin_src python :tangle src/seh_pathway_hopping/execution/patch_in_continuations.py
  import click


  def match_run_idx(wepy_h5, start_hash, end_hash):
      """Get the run idx of the run in the HDF5 using the start and end hash"""

      with wepy_h5:

          for run_idx in wepy_h5.run_idxs:

              if (start_hash == wepy_h5.run_start_snapshot_hash(run_idx) and
                  end_hash == wepy_h5.run_end_snapshot_hash(run_idx)):

                  # a match, short circuit
                  return run_idx

      return None

  @click.argument('gexp')
  @click.command()
  def main(gexp):

      from seh_pathway_hopping._tasks import (
          get_gexp_jobs_df,
          get_gexp_wepy_h5,
          get_gexp_master_orch,
      )

      orch = get_gexp_master_orch(gexp)
      jobs_df = get_gexp_jobs_df(gexp)
      wepy_h5 = get_gexp_wepy_h5(gexp)

      # we need to add continuations so set this here
      wepy_h5.set_mode('r+')

      # clear the continuations
      with wepy_h5:

          # delete the dset
          del wepy_h5.h5['_settings/continuations']

          # re-initialize
          wepy_h5._init_continuations()

      for row_idx, row in jobs_df.iterrows():

          # use the real start hash to get the continuations
          continued_run = orch.run_continues(
               row['start_hash'],
               row['end_hash'])


          if continued_run is None:
              continue

          cont_patched_start_hash, cont_end_hash = continued_run

          # get the run idxs in the HDF5 file so we can add the continuation

          # the idx of this run
          curr_run_idx = match_run_idx(wepy_h5,
                                       row['patched_start_hash'],
                                       row['end_hash'])

          # the run that was being continued
          cont_run_idx = match_run_idx(wepy_h5,
                                       cont_patched_start_hash,
                                       cont_end_hash)

          print("----------------------------------------")
          print("Continuing Run")
          print("start_hash:", row['start_hash'])
          print("patched_start_hash:", row['patched_start_hash'])
          print("end_hash:", row['end_hash'])
          print("HDF5 run idx", curr_run_idx)
          print("--------------------")
          print("Continued Run")
          # print("start_hash:", row['start_hash'])
          print("patched_start_hash:", cont_patched_start_hash)
          print("end_hash:", cont_end_hash)
          print("HDF5 run idx", cont_run_idx)
          print("--------------------")

          print("Continuation is:", curr_run_idx, cont_run_idx)
          print("Setting to HDF5")


          with wepy_h5:
              wepy_h5.add_continuation(curr_run_idx, cont_run_idx)


          print("----------------------------------------")

          print("Continuations after")
          with wepy_h5:
              print(wepy_h5.continuations)




  if __name__ == "__main__":

      main()

#+end_src


***** Reference State Topologies

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_ref_tops.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_ref_tops(gexp):


      from seh_pathway_hopping._tasks import (
        write_lig_ref_selection_pdbs,
        write_lig_centered_ref_selection_pdbs,
        GEXP_LIG_IDS
      )

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      write_lig_ref_selection_pdbs(lig_id)
      write_lig_centered_ref_selection_pdbs(lig_id)

  if __name__ == "__main__":

      gen_ref_tops()

#+end_src

***** Warping Tables

****** By Span

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_spans_warp_table.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_span_warp_table(gexp):


      from seh_pathway_hopping._tasks import (
          save_span_warp_table,
          get_gexp_span_ids,
      )

      for span_id in get_gexp_span_ids(gexp):

          save_span_warp_table(gexp, span_id, overwrite=True)


  if __name__ == "__main__":

      gen_span_warp_table()
#+end_src


****** By gexp

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_gexp_warp_table.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_gexp_warp_table(gexp):


      from seh_pathway_hopping._tasks import (
          save_gexp_warp_table,
      )

      save_gexp_warp_table(gexp, overwrite=True)


  if __name__ == "__main__":

      gen_gexp_warp_table()
#+end_src

***** Warping lineage trajectories

Each warp gets a single trajectory file, they are indexed to match the
warp table.

This is usually better since the big one won't fit into memory if
there are many warps.

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_warp_lineages_multi.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_warp_lineages_multi(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_warp_lineages_dcds,
      )

      save_gexp_warp_lineages_dcds(gexp)

  if __name__ == "__main__":

      gen_warp_lineages_multi()
#+end_src



***** Final Walker lineage trajectories

Each final walker gets a single trajectory file.

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_final_lineages_multi.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_final_lineages_multi(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_final_lineages_dcds,
      )

      save_gexp_final_lineages_dcds(gexp)

  if __name__ == "__main__":

      gen_final_lineages_multi()
#+end_src


***** Highest Progress Walkers trajectories


****** Just The walkers

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_high-progress_walkers.py
  import click

  @click.command()
  @click.option('--top-n', '-n', type=int, default=5)
  @click.argument('gexp')
  def gen_high_progress_walkers(
          top_n,
          gexp,
  ):

      from seh_pathway_hopping._tasks import (
          save_gexp_high_progress_walkers,
      )

      save_gexp_high_progress_walkers(
          gexp,
          top_n,
          progress_key='min_distances',
      )

  if __name__ == "__main__":

      gen_high_progress_walkers()
#+end_src


****** Lineages of those walkers

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_high-progress_walker_lineages.py
  import click

  @click.command()
  @click.option('--top-n', '-n', type=int, default=5)
  @click.argument('gexp')
  def gen_high_progress_walker_lineages(
          top_n,
          gexp,
  ):

      from seh_pathway_hopping._tasks import (
          save_gexp_high_progress_walkers_lineages_dcds,
      )
      save_gexp_high_progress_walkers_lineages_dcds(
          gexp,
          top_n,
          progress_key='min_distances',
      )

  if __name__ == "__main__":

      gen_high_progress_walker_lineages()
#+end_src



***** Contig Stats Tables

#+begin_src python :tangle src/seh_pathway_hopping/execution/gen_contig_stats_table.py
  import click

  @click.command()
  @click.argument('gexp')
  def gen_contig_stats_table(gexp):


      from seh_pathway_hopping._tasks import (
          save_span_stats_table,
          span_stats_table_str,
      )

      save_span_stats_table(gexp, overwrite=True)

      click.echo(span_stats_table_str(gexp))


  if __name__ == "__main__":

      gen_contig_stats_table()
#+end_src



***** Rates & Weights Plots

****** Aggregate Probabilities

******* Interactive

#+begin_src python :tangle src/seh_pathway_hopping/execution/show_agg_prob_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def show_agg_prob_plots(gexp):

      from seh_pathway_hopping._tasks import (
          gexp_show_plot_agg_prob,
      )

      gexp_show_plot_agg_prob(gexp)

  if __name__ == "__main__":

      show_agg_prob_plots()
#+end_src



******* Save Plots

#+begin_src python :tangle src/seh_pathway_hopping/execution/save_agg_prob_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def save_agg_plots(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_plot_agg_prob,
      )

      save_gexp_plot_agg_prob(gexp)

  if __name__ == "__main__":

      save_agg_plots()
#+end_src


****** Rates

******* Show
#+begin_src python :tangle src/seh_pathway_hopping/execution/show_rate_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def show_rate_plots(gexp):

      from seh_pathway_hopping._tasks import (
          gexp_show_plot_rates,
      )

      gexp_show_plot_rates(gexp)

  if __name__ == "__main__":

      show_rate_plots()
#+end_src

******* Save

#+begin_src python :tangle src/seh_pathway_hopping/execution/save_rate_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def save_rate_plots(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_plot_rates,
      )

      save_gexp_plot_rates(gexp)

  if __name__ == "__main__":

      save_rate_plots()
#+end_src

****** Residence times

******* Show
#+begin_src python :tangle src/seh_pathway_hopping/execution/show_rt_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def show_rt_plots(gexp):

      from seh_pathway_hopping._tasks import (
          gexp_show_plot_rts,
      )

      gexp_show_plot_rts(gexp)

  if __name__ == "__main__":

      show_rt_plots()
#+end_src



******* Save

#+begin_src python :tangle src/seh_pathway_hopping/execution/save_rt_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def save_rt_plots(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_plot_rts,
      )

      save_gexp_plot_rts(gexp)

  if __name__ == "__main__":

      save_rt_plots()
#+end_src


****** COMMENT Save both

#+begin_src python :tangle src/seh_pathway_hopping/execution/save_rate_plots.py
  import click

  @click.command()
  @click.argument('gexp')
  def save_rate_plots(gexp):

      from seh_pathway_hopping._tasks import (
          save_gexp_plot_rates_rt,
      )

      save_gexp_plot_rates_rt(gexp)

  if __name__ == "__main__":

      save_rate_plots()
#+end_src


***** Observables

****** Test: compute box volumes

******* Pool

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/box_volume_observable_pool.py

  def observable_box_volume(map_func, **kwargs):

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      from seh_pathway_hopping._tasks import get_gexp_wepy_h5

      from seh_pathway_hopping._tasks import box_volume_observable

      wepy_h5 = get_gexp_wepy_h5(kwargs['gexp'])

      with wepy_h5:

          observable = wepy_h5.compute_observable(
              box_volume_observable,
              ['box_vectors'],
              (),
              map_func=map_func,
          )

      # save the observable in the hdf5
      # with WepyHDF5(wepy_h5_path, mode='r+') as wepy_h5:
      #     wepy_h5.add_observable('box_volume', observable)

  #----------------------------------------
  import click

  from seh_pathway_hopping.execution import parse_kwargs

  from multiprocessing.pool import Pool as MPPool
  from ray.util.multiprocessing import Pool as RayPool

  func = observable_box_volume

  @click.command(context_settings=dict(
  ignore_unknown_options=True,))
  @click.option('--n-cores', '-n', type=int, default=1)
  @click.option('--mapper', '-m',
                type=click.Choice(['mp', 'ray']),
                default='mp')
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(n_cores, mapper, specs):

      kwargs = parse_kwargs(specs)

      if mapper is 'mp':
          pool = MPPool(n_cores)
      elif mapper is 'ray':
          pool = RayPool(n_cores)

      func(pool.map, **kwargs)



  if __name__ == "__main__":

      cli()


#+END_SRC

******* Dask
#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/box_volume_observable_dask.py

  def observable_box_volume(client, **kwargs):

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      from seh_pathway_hopping._tasks import gexp_wepy_h5_path

      from seh_pathway_hopping._tasks import box_volume_observable

      wepy_h5_path = gexp_wepy_h5_path(kwargs['gexp'])


      with client:

          observable = compute_observable(
              box_volume_observable,
              str(wepy_h5_path),
              client,
              ['box_vectors']
          )

      # save the observable in the hdf5
      with WepyHDF5(wepy_h5_path, mode='r+') as wepy_h5:
          wepy_h5.add_observable('box_volume', observable)


  if __name__ == "__main__":

      from seh_pathway_hopping.execution import execute

      execute(observable_box_volume)

#+END_SRC


****** Ligand RMSD Observable

******* Pool

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/lig_rmsd_observable_pool.py

  def observable_lig_bs_rmsd(map_func, **kwargs):

      from functools import partial

      from wepy.hdf5 import WepyHDF5

      from seh_pathway_hopping._tasks import get_gexp_wepy_h5

      # the task or flow
      from seh_pathway_hopping._tasks import (
          lig_bs_rmsd_observable,
          lig_selection_idxs,
      )

      FIELD = 'lig_rmsd'
      FIELDS = ['positions', 'box_vectors']
      RUN_IDXS = Ellipsis

      wepy_h5 = get_gexp_wepy_h5(kwargs['gexp'])

      # generate the function we need to make this run

      # TODO: audit main_rep here
      ligand_idxs = lig_selection_idxs(kwargs['gexp'])['main_rep/ligand']

      # do a partial evaluation of the function so we can use
      # the multiprocessing map which doesn't accept multiple
      # iterables for arguments
      func = partial(lig_bs_rmsd_observable, kwargs['gexp'], ligand_idxs)

      print("starting calculation")
      wepy_h5.set_mode('r+')
      with wepy_h5:

          observable = wepy_h5.compute_observable(
              func,
              FIELDS,
              (),
              map_func=map_func,
              save_to_hdf5=FIELD,
          )

      print("finished calculation")

  # ----------------------------------------
  from functools import partial

  import click

  from seh_pathway_hopping.execution import parse_kwargs

  from multiprocessing.pool import Pool as MPPool
  from ray.util.multiprocessing import Pool as RayPool

  func = observable_lig_bs_rmsd

  @click.command(context_settings=dict(
  ignore_unknown_options=True,))
  @click.option('--n-cores', '-n', type=int, default=1)
  @click.option('--mapper', '-m',
                type=click.Choice(['mp', 'ray']),
                default='mp')
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(n_cores, mapper, specs):

      kwargs = parse_kwargs(specs)

      if mapper is 'mp':
          pool = MPPool(n_cores)

          CHUNKSIZE = 10
          def map_func(func, *args):
              return pool.imap(func,
                               ,*args,
                               CHUNKSIZE,
              )

      elif mapper is 'ray':
          pool = RayPool(n_cores)
          map_func = pool.map

      func(map_func, **kwargs)



  if __name__ == "__main__":

      cli()
#+END_SRC


******* TODO Dask

****** Ligand-BS Atom Pair Distances

******* Pool

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/lig_bs_atom_pair_dist_observable_pool.py

  def observable_lig_bs_atom_pair_dist(map_func, **kwargs):

      from functools import partial
      import os.path as osp

      from seh_pathway_hopping._tasks import (
          get_gexp_wepy_h5,
          lig_selection_idxs,
          lig_selection_tops,
          lig_prot_atom_pair_observable,
          lig_bs_atom_pairs,
          GEXP_LIG_IDS,
      )


      FIELD = 'bs_lig_pair_dists'
      FIELDS = ['positions', 'box_vectors']
      RUN_IDXS = Ellipsis

      gexp = kwargs['gexp']

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      wepy_h5 = get_gexp_wepy_h5(gexp)

      atom_pairs = lig_bs_atom_pairs(
          gexp,
          rep_key='main_rep',
      )

      # do a partial evaluation of the function so we can use
      # the multiprocessing map which doesn't accept multiple
      # iterables for arguments
      func = partial(
          lig_prot_atom_pair_observable, # the function
          atom_pairs,
          lig_selection_tops(lig_id)['main_rep'], # topology
      )

      wepy_h5.set_mode('r+')
      with wepy_h5:

          observable = wepy_h5.compute_observable(
              func,
              FIELDS,
              (),
              map_func=map_func,
              save_to_hdf5=FIELD,
          )

      print("finished calculation")


  # ----------------------------------------

  import click

  from seh_pathway_hopping.execution import parse_kwargs

  from multiprocessing.pool import Pool as MPPool
  from ray.util.multiprocessing import Pool as RayPool

  func = observable_lig_bs_atom_pair_dist

  @click.command(context_settings=dict(
  ignore_unknown_options=True,))
  @click.option('--n-cores', '-n', type=int, default=1)
  @click.option('--mapper', '-m',
                type=click.Choice(['mp', 'ray']),
                default='mp')
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(n_cores, mapper, specs):

      kwargs = parse_kwargs(specs)

      if mapper is 'mp':
          pool = MPPool(n_cores)

          CHUNKSIZE = 10
          def map_func(func, *args):
              return pool.imap(func,
                               ,*args,
                               CHUNKSIZE,
              )

      elif mapper is 'ray':
          pool = RayPool(n_cores)
          map_func = pool.map

      func(map_func, **kwargs)



  if __name__ == "__main__":

      cli()


#+END_SRC


******* Dask

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/lig_bs_atom_pair_dist_observable_dask.py

  def observable_lib_bs_atom_pair_dist(client, **kwargs):

      from functools import partial
      import os.path as osp

      import joblib
      import numpy as np

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      from seh_pathway_hopping._tasks import (
          data_path,
          gexp_wepy_h5_path,
          lig_selection_idxs,
          lig_selection_tops,
          lig_prot_atom_pair_observable,
          lig_prot_atom_pairs,
      )


      FIELD = 'bs_lig_pair_dists'
      FIELDS = ['positions', 'box_vectors']
      RUN_IDXS = Ellipsis
      CHUNK_SIZE = 100

      lig_id = kwargs['gexp']

      wepy_h5_path = gexp_wepy_h5_path(kwargs['gexp'])
      # TODO: audit main_rep here
      ligand_idxs = lig_selection_idxs(lig_id)['main_rep/ligand']

      # do a partial evaluation of the function so we can use
      # the multiprocessing map which doesn't accept multiple
      # iterables for arguments
      func = partial(lig_prot_atom_pair_observable,
                     lig_prot_atom_pairs(lig_id),
                     lig_selection_tops(lig_id)['main_rep'])

      print("starting calculation")
      observable = compute_observable(func,
                                      wepy_h5_path,
                                      client,
                                      ['positions', 'box_vectors'],
                                      chunk_size=100,
                                      run_idxs=Ellipsis)

      print("finished calculation")

      with WepyHDF5(wepy_h5_path, mode='r+') as wepy_h5:

          wepy_h5.add_observable(FIELD, observable)


  if __name__ == '__main__':
      from seh_pathway_hopping.execution import execute

      execute(observable_lib_bs_atom_pair_dist)
#+END_SRC


****** Ligand SASA

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/lig_sasa_observable.py
  N_SPHERE_POINTS = 960

  def observable_lig_sasa(map_func, **kwargs):

      from functools import partial

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      # the task or flow
      from seh_pathway_hopping._tasks import (
          GEXP_LIG_IDS,
          get_gexp_wepy_h5,
          lig_sasa_observable,
          lig_selection_idxs,
          lig_selection_tops,
      )

      gexp = kwargs['gexp']

      # params for the sasa func
      if 'n_sphere_points'in kwargs:
          n_sphere_points = int(kwargs['n_sphere_points'])

      else:
          n_sphere_points = N_SPHERE_POINTS

      FIELD = f'lig-sasa_npoints-{n_sphere_points}'
      FIELDS = ['positions', 'box_vectors']

      # RUN_IDXS = [0]
      RUN_IDXS = Ellipsis


      wepy_h5 = get_gexp_wepy_h5(gexp,
                                  mode='r+')

      lig_id = dict(GEXP_LIG_IDS)[gexp]

      # generate the function we need to make this run
      # TODO: audit main_rep here
      # REVD: main rep is good here since it has to do data for all of the runs.
      ligand_idxs = lig_selection_idxs(lig_id)['main_rep/ligand']
      top = lig_selection_tops(lig_id)['main_rep']

      # do a partial evaluation of the function so we can use
      # the multiprocessing map which doesn't accept multiple
      # iterables for arguments
      func = partial(
          lig_sasa_observable,
          lig_id,
          ligand_idxs,
          top,
          n_sphere_points,
      )

      print(f"starting calculation for gexp: {gexp}")

      with wepy_h5:
          observable = wepy_h5.compute_observable(
              func,
              FIELDS,
              (),
              map_func=map_func,
              save_to_hdf5=FIELD,
           )

      print("finished calculation")

  # ----------------------------------------
  import time

  import click

  from seh_pathway_hopping.execution import parse_kwargs

  from multiprocessing.pool import Pool as MPPool
  from ray.util.multiprocessing import Pool as RayPool

  func = observable_lig_sasa

  @click.command(context_settings=dict(
  ignore_unknown_options=True,))
  @click.option('--n-cores', '-n', type=int, default=1)
  @click.option('--mapper', '-m',
            type=click.Choice(['mp', 'ray']),
            default='mp')
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(n_cores, mapper, specs):

      kwargs = parse_kwargs(specs)

      if mapper is 'mp':
          pool = MPPool(n_cores)

          CHUNKSIZE = 1
          def map_func(func, *args):
              return pool.imap(func,
                               ,*args,
                               CHUNKSIZE,
              )

      elif mapper is 'ray':
          pool = RayPool(n_cores)
          map_func = pool.map

      start = time.time()

      func(map_func, **kwargs)

      end = time.time()

      duration = end - start

      print(f"Took a total of: {duration} s")



  if __name__ == "__main__":

      cli()
#+END_SRC




****** PCA Projections

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/pc_projections_observable.py
  def observable_pc_projections(client, lig_id, **kwargs):

      import functools

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      from seh_pathway_hopping._tasks import (
          gexp_wepy_h5_path,
          lig_selection_idxs,
          get_model,
      )

      from seh_pathway_hopping._tasks import pc_projections_observable

      TS_MODEL = 'TS/warp+featdist-0.07'

      wepy_h5_path = gexp_wepy_h5_path(lig_id)

      # TODO: audit main_rep here
      sel_idxs = lig_selection_idxs(lig_id)
      hom_idxs = sel_idxs['main_rep/ligand/homology']

      model = get_model(TS_MODEL, lig_id)

      obs_func = functools.partial(pc_projections_observable, lig_id, hom_idxs, model)


      with client:

          observable = compute_observable(obs_func, wepy_h5_path, client,
                                          ['positions', 'box_vectors'])

      # split the observable vector into 3 different observables
      pc_observables = []
      for pc_idx in range(model.n_components_):
          pc_observables.append([])

          for run_idx, run in enumerate(observable):
              pc_observables[pc_idx][run_idx].append([])

              for traj in run:

                  pc_observables[pc_idx][run_idx].append(traj[:,pc_idx])





      # save the observable in the hdf5
      with WepyHDF5(wepy_h5_path, mode='r+') as wepy_h5:
          for pc_idx, pc_obs in enumerate(pc_observables):

              model_obs_name = TS_MODEL.replace('/', '-')
              obs_name = "{}_pc-{}".format(model_obs_name, pc_idx)

              # save the object for the observable
              save_observable(obs_name, lig_id, pc_obs)

              # and save to the HDF5
              wepy_h5.add_observable(obs_name, pc_obs)


  if __name__ == "__main__":

      from seh_pathway_hopping.execution import execute

      execute(observable_pc_projections)

#+END_SRC



****** (Legacy) PCA Projections

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/legacy_pc_projections_observable.py
  def observable_pc_projections(client, lig_id, **kwargs):

      import functools
      import os.path as osp

      import joblib

      from wepy.hdf5 import WepyHDF5
      from wepy.analysis.distributed import compute_observable

      from seh_pathway_hopping._tasks import (
          gexp_wepy_h5_path,
          lig_selection_idxs,
          get_model,
          legacy_linker_file_path,
          save_observable,
          data_path,
      )

      from seh_pathway_hopping._tasks import legacy_pc_projections_observable

      TS_MODEL = 'TS/warp+featdist-0.07'
      LEGACY_LIG_ID = 17

      wepy_h5_path = legacy_linker_file_path()

      sel_idxs = lig_selection_idxs(LEGACY_LIG_ID)
      hom_idxs = sel_idxs['real_rep/ligand/homology']

      model = get_model(TS_MODEL, lig_id)

      obs_func = functools.partial(legacy_pc_projections_observable, LEGACY_LIG_ID,
                                   hom_idxs, model)


      # DEBUG: for debugging
      with WepyHDF5(wepy_h5_path, mode='r') as wepy_h5:
          observable = wepy_h5.compute_observable(obs_func, ['positions', 'box_vectors'],
                                                  (),
          )

      # DEBUG: failsafe so I don't lose work
      failsafe_path = osp.join(data_path(), 'tmp/legacy_projections_observable_failsafe.jl.pkl')
      joblib.dump(observable, failsafe_path)
      print("failsafe worked")


      # with client:

      #     observable = compute_observable(obs_func, wepy_h5_path, client,
      #                                     ['positions', 'box_vectors'])

      # split the observable vector into 3 different observables
      pc_observables = []
      for pc_idx in range(model.n_components_):
          pc_observables.append([])

          for traj_idx, traj in enumerate(observable):

              pc_observables[pc_idx].append(traj[:,pc_idx])

      for pc_idx, pc_obs in enumerate(pc_observables):

          model_obs_name = TS_MODEL.replace('/', '-')
          obs_name = "lig-{}_{}_pc-{}".format(lig_id, model_obs_name, pc_idx)

          # save the object for the observable
          save_observable(obs_name, LEGACY_LIG_ID, pc_obs)


  if __name__ == "__main__":

      from seh_pathway_hopping.execution import execute

      execute(observable_pc_projections)

#+END_SRC


If the saveing of the obsrvables into HDF5 didn't work do it manually:

#+BEGIN_SRC python
  from seh_pathway_hopping._tasks import *

  from wepy.hdf5 import WepyHDF5

  import joblib


  # use a custom set observable where we force the setting of the field
  # to not check that it fits, since our data is screwy and messy, but I
  # know that it is okay.
  def add_traj_observable(wepy_h5, observable_name, data):

      obs_path = '{}/{}'.format('observables', observable_name)

      run_results = []

      for run_idx in range(wepy_h5.num_runs):

          run_num_trajs = wepy_h5.num_run_trajs(run_idx)
          run_results.append([])

          for traj_idx in range(run_num_trajs):
              run_results[run_idx].append(data[(run_idx * run_num_trajs) + traj_idx])

      wepy_h5._add_field(obs_path, run_results,
                         force=True)


  wepy_h5_path = legacy_linker_file_path()

  pc0 = get_observable('lig-3_TS-warp+featdist-0.07_pc-0', 17)
  pc1 = get_observable('lig-3_TS-warp+featdist-0.07_pc-1', 17)
  pc2 = get_observable('lig-3_TS-warp+featdist-0.07_pc-2', 17)

  # save the observable in the hdf5
  with WepyHDF5(wepy_h5_path, mode='r+') as wepy_h5:

      # and save to the HDF5
      add_traj_observable(wepy_h5, 'lig-3_TS-warp+featdist-0.07_pc-0', pc0)
      add_traj_observable(wepy_h5, 'lig-3_TS-warp+featdist-0.07_pc-1', pc1)
      add_traj_observable(wepy_h5, 'lig-3_TS-warp+featdist-0.07_pc-2', pc2)

#+END_SRC


***** FE Profiles

****** Spans

******* Interactive

#+begin_src python :tangle src/seh_pathway_hopping/execution/fe_profile_spans_show.py

  import click

  @click.command()
  @click.argument('gexp')
  @click.argument('observable')
  def main(gexp, observable):

      from seh_pathway_hopping._tasks import (
          gexp_show_plot_spans_fe_obs,
      )

      gexp_show_plot_spans_fe_obs(gexp, observable)

  if __name__ == '__main__':
      main()
#+end_src

******* Save Plots

#+begin_src python :tangle src/seh_pathway_hopping/execution/fe_profile_spans_save.py

  import click

  @click.command()
  @click.argument('gexp')
  @click.argument('observable')
  def main(gexp, observable):

      from seh_pathway_hopping._tasks import (
          gexp_save_plot_spans_fe_obs,
      )

      gexp_save_plot_spans_fe_obs(gexp, observable)

  if __name__ == '__main__':
      main()
#+end_src


****** Spans Convergence

******* Interactive

#+begin_src python :tangle src/seh_pathway_hopping/execution/fe_profile_spans_convergence_show.py

  import click

  @click.command()
  @click.argument('gexp')
  @click.argument('observable')
  def main(gexp, observable):

      from seh_pathway_hopping._tasks import (
          gexp_show_plot_spans_convergence_fe_obs,
      )

      gexp_show_plot_spans_convergence_fe_obs(gexp, observable)

  if __name__ == '__main__':
      main()
#+end_src

******* Save Plots

#+begin_src python :tangle src/seh_pathway_hopping/execution/fe_profile_spans_convergence_save.py

  import click

  @click.command()
  @click.argument('gexp')
  @click.argument('observable')
  def main(gexp, observable):

      from seh_pathway_hopping._tasks import (
          gexp_save_plot_spans_convergence_fe_obs,
      )

      gexp_save_plot_spans_convergence_fe_obs(gexp, observable)

  if __name__ == '__main__':
      main()
#+end_src


****** All GEXPs Aggregate

#+begin_src python :tangle src/seh_pathway_hopping/execution/fe_profile_all_gexps_agg.py

  import click

  @click.command()
  @click.option('--save', is_flag=True)
  @click.option('--show', is_flag=True)
  @click.argument('observable')
  def main(save, show, observable):

      from seh_pathway_hopping._tasks import (
          all_render_plot_agg_fe_obs,
      )

      all_render_plot_agg_fe_obs(
          observable,
          save=save,
          show=show,
      )

  if __name__ == '__main__':
      main()
#+end_src




***** Clustering

#+begin_src python :tangle src/seh_pathway_hopping/execution/do_gexp_classification.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          do_gexp_classification,
      )

      kwargs = parse_kwargs(specs)

      do_gexp_classification(
          kwargs['gexp'],
          kwargs['observable'],
          kwargs['clf_id'],
      )

  if __name__ == "__main__":

      cli()
#+end_src

***** TODO Markov State Models Hyperparameter Optimization

This is still a WIP, since I'm not really using it.

#+begin_src python :tangle :tangle src/seh_pathway_hopping/execution/msm_hyperparameter_opt.py

  import click
  @click.command()
  @click.argument('gexp')
  @click.argument('observable')
  def main(gexp, observable):

        from seh_pathway_hopping._tasks import (
            score_msm_gmrq_crossval_classifier,
        )

      space = [
          sp.Integer(100, 110, name="n_clusters"),
      ]



      @use_named_args(space)
      def objective(**params):

          # the classifier to use
          clf = MiniBatchKMeans(
              n_clusters=params['n_clusters'],
              batch_size=100,
          )

          # the cross-validation scheme as a splitter object
          splitter = KFold(n_splits=n_splits)

          # do the cross-validation score for the classifier
          stats = score_msm_gmrq_crossval_classifier(
              gexp,
              observable,
              clf,
              splitter,
              lag_time=lag_time,
          )


          # the final score is the negative of the mean
          return -stats['mean']


      ## There are different ways to control optimization with skopt

      method = 'highlevel'

      # the high level function and the sklearn compatible wrapper both
      # use callbacks to get info. Otherwise you can control the loop
      # with the optimizer.

      # callbacks for reporting
      def print_result_cb(res):

          msg = f"""
  Current Results
  ---------------

  - last objective function score :: {res['fun']}
  - solution :: {res['x']}
  """
          print(msg)

      verbose_cb = skopt.callbacks.VerboseCallback(1)

      DEADLINE = 108000 # seconds
      deadline_cb = skopt.callbacks.DeadlineStopper(DEADLINE)

      # use the high level function
      if method == 'highlevel':

          res_gp = skopt.gp_minimize(
              objective,
              space,
              n_calls=10,
              callback=[
                  print_result_cb,
                  verbose_cb,
                  deadline_cb,
              ],
          )

      elif method == 'sklearn':

          pass

      elif method == 'asktell':

          optimizer = skopt.Optimizer(
              space,
          )

          for step in range(3):

              # get the next sample of hyperparameters from the optimizer
              next_sample = optimizer.ask()

              print(next_sample)

              # compute the objective function
              obj_val = objective(next_sample)

              print(f"Objective Function Score: {obj_val}")

              # Report the score for the sample to the optimizer

              optimizer.tell(
                  next_sample,
                  obj_val,
              )



  if __name__ == "__main__":

      main()

      import itertools as it

      import numpy as np

      import skopt
      import skopt.space as sp
      from skopt.utils import use_named_args

      from sklearn.cluster import MiniBatchKMeans
      from sklearn.model_selection import (
          ShuffleSplit,
          KFold,
      )

      from wepy.analysis.network import MacroStateNetwork

      from seh_pathway_hopping._tasks import *

      ## perform the clustering on the observable using only the
      ## training runs

      # space = [
      #     sp.Integer(2, 1000, name="n_clusters"),
      #     sp.Integer(100, 10000, name="batch_size"),
      # ]

#+end_src



***** State Networks

****** Generate & Save Basic Model

#+begin_src python :tangle src/seh_pathway_hopping/execution/save_basic_msm.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          make_basic_csn,
          save_all_csn_stuff,
      )

      kwargs = parse_kwargs(specs)

      gexp = kwargs['gexp']
      csn_id = kwargs['csn_id']

      if 'tag' in kwargs:
          tag = kwargs['tag']
      else:
          tag = None

      if 'layout_id' in kwargs:
          layout_id = kwargs['layout_id']
      else:
          layout_id = None


      # make a basic CSN from the specs
      msn = make_basic_csn(
          gexp,
          csn_id,
      )

      # save it as the current set of network files that are used for
      # analysis and visualization etc.
      save_all_csn_stuff(
          csn_id,
          gexp,
          msn,
          tag=tag,
          layout_id=layout_id,
          overwrite=True,
      )

  if __name__ == "__main__":

      cli()

#+end_src

****** Reload the Network

#+begin_src python :tangle src/seh_pathway_hopping/execution/update_from_gexf.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          load_gephi_graph,
          update_network_layout_from_gephi,
          save_all_csn_stuff,
          get_msn,
      )

      kwargs = parse_kwargs(specs)

      csn_id = kwargs['csn_id']
      gexp = kwargs['gexp']
      layout_id = kwargs['layout_id']

      if 'tag' in kwargs:
          tag = kwargs['tag']
      else:
          tag = None

      ## Basically we load the gexf file and then dump the results to
      ## the rest of the un-updated files

      # get the gephi gexf networkx graph
      gephi_nx = load_gephi_graph(
          csn_id,
          gexp,
          tag=tag,
          layout_id=layout_id,
      )

      # load the msn to update
      msn = get_msn(
          csn_id,
          gexp,
          tag=tag,
      )

      # update the msn with the gexf data
      update_msn = update_network_layout_from_gephi(
          msn,
          gephi_nx,
          layout_name=layout_id,
      )

      # save it as the current set of network files that are used for
      # analysis and visualization etc.
      save_all_csn_stuff(
          csn_id,
          gexp,
          update_msn,
          tag=tag,
          layout_id=layout_id,
          overwrite=True,
      )

  if __name__ == "__main__":

      cli()

#+end_src

****** Calculate Observable Stats

#+begin_src python :tangle src/seh_pathway_hopping/execution/calc_msn_obs_node_stats.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          save_all_csn_stuff,
          get_h5_msn,
          calc_msn_obs_stats,
      )

      kwargs = parse_kwargs(specs)

      csn_id = kwargs['csn_id']
      gexp = kwargs['gexp']
      layout_id = kwargs['layout_id']

      observable_name = kwargs['obs_name']

      if 'tag' in kwargs:
          tag = kwargs['tag']
      else:
          tag = None

      ### load the MSN with the ContigTree backing
      msn = get_h5_msn(
          csn_id,
          gexp,
          tag=tag,
      )

      # then compute the stats for the observable adding it to the
      # network
      _ = calc_msn_obs_stats(
          msn,
          observable_name,
          save=True,
      )

      # we want to only save the base MSN so make sure to only do that part

      # save it as the current set of network files that are used for
      # analysis and visualization etc.
      save_all_csn_stuff(
          csn_id,
          gexp,
          msn.base_network,
          tag=tag,
          layout_id=layout_id,
          overwrite=True,
      )

  if __name__ == "__main__":

      cli()

#+end_src


****** Find Native State

#+begin_src  python :tangle src/seh_pathway_hopping/execution/find_native_state.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          get_msn,
          save_all_csn_stuff,
          MSM_SPECS,
          classify_native_state_cluster,
      )

      kwargs = parse_kwargs(specs)

      ## Load
      msn = get_msn(
          kwargs['msm_id'],
          kwargs['gexp'],
          csn_id=kwargs['csn_id'],
      )

      clf_id = MSM_SPECS[kwargs['msm_id']]['clf_id']

      ## Transformations
      native_node_id = classify_native_state_cluster(
          clf_id,
          kwargs['gexp'],
      )

      print(f"native node", native_node_id)

      msn.set_node_group('native_state_id', [native_node_id])

      ## Save it
      save_all_csn_stuff(
          kwargs['msm_id'],
          kwargs['gexp'],
          msn,
          csn_id=kwargs['csn_id'],
          layout_id=kwargs['layout_id'],
          overwrite=True,
      )

  if __name__ == "__main__":

      cli()
#+end_src

****** Save Cluster Centers

#+begin_src  python :tangle src/seh_pathway_hopping/execution/write_cluster_center_dcd.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          save_cluster_centers,
          get_cluster_center_traj,
      )


      kwargs = parse_kwargs(specs)

      msm_id = kwargs['msm_id']
      gexp = kwargs['gexp']

      centers_traj = get_cluster_center_traj(
          gexp,
          csn_id,
      )

      save_cluster_centers(
          msm_id,
          gexp,
          centers_traj,
      )

  if __name__ == "__main__":

      cli()
#+end_src

****** Markov State Modelling

#+begin_src  python :tangle src/seh_pathway_hopping/execution/msn_markov_model.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          MSM_SPECS,
          get_msn,
          save_all_csn_stuff,
          save_msm,
          make_pyemma_msm,
          add_msm_to_msn,
      )


      kwargs = parse_kwargs(specs)

      msm_id = kwargs['msm_id']
      gexp = kwargs['gexp']
      layout_id = kwargs['layout_id']

      if 'tag' in kwargs:
          tag = kwargs['tag']
      else:
          tag = None

      msm_spec = MSM_SPECS[msm_id]
      csn_id = msm_spec['csn_id']

      print("MSM Spec:", msm_spec)

      trim_method = msm_spec['trim_method']
      trim_kwargs = msm_spec['trim_kwargs']

      transprob_method = msm_spec['transition_prob_method']
      transprob_kwargs = msm_spec['transition_prob_kwargs']

      print("trim method: ", trim_method)
      print("trim kwargs: ", trim_kwargs)

      print("transprob method: ", transprob_method)
      print("transprob kwargs: ", transprob_kwargs)

      print("Loading the MSN")
      ## Load
      msn = get_msn(
          csn_id,
          gexp,
          tag=tag,
      )

      print("Making the MSM")
      ## Transformation
      pyemma_msm, trimming_mapping = make_pyemma_msm(
          msn,
          trim_method=trim_method,
          trim_kwargs=trim_kwargs,
          transprob_method=transprob_method,
          transprob_kwargs=transprob_kwargs,
      )

      print(f"Trimmed network has {len(trimming_mapping)} nodes")

      msm_msn = add_msm_to_msn(
          msn,
          msm_id,
          pyemma_msm,
          trimming_mapping,
      )

      ## Save the network and MSM

      print("Saving the MSM")
      save_msm(
          msm_id,
          gexp,
          pyemma_msm,
          trimming_mapping,
      )

      print("Saving the CSN stuff")
      save_all_csn_stuff(
          csn_id,
          gexp,
          msm_msn,
          tag=tag,
          layout_id=layout_id,
          overwrite=True,
      )

  if __name__ == "__main__":

      cli()
#+end_src


***** Committors and TS Prediction

****** Calculate the Basins

#+begin_src  python :tangle src/seh_pathway_hopping/execution/choose_committor_basins.py

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          get_msn,
          save_all_csn_stuff,
          MSM_SPECS,
          BASIN_SPECS,
          compute_csn_bound_basin,
          compute_csn_unbound_basin,
          basins_to_msn,
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      gexp = kwargs['gexp']
      csn_id = kwargs['csn_id']
      layout_id = kwargs['layout_id']

      # if there was a basin_id only compute for that otherwise do all
      # of the basin specs
      if 'basin_id' in kwargs:
          basin_ids = [kwargs['basin_id']]

          print("Getting the basins for basin_id:")
          print(basin_ids[0])

      for basin_id in basin_ids:

          msm_id = BASIN_SPECS[basin_id]['msm_id']

          ## Load
          msn = get_msn(
              csn_id,
              gexp,
          )

          ## Transformations

          # get the bound basin according to the 'basin_id' specs
          bound_basin_idxs = compute_csn_bound_basin(
              basin_id,
              gexp,
              csn_id,
          )

          unbound_basin_idxs = compute_csn_unbound_basin(
              basin_id,
              gexp,
              csn_id,
          )

          print(f"Basins for basin_id: {basin_id}")
          print(f"Found {len(bound_basin_idxs)} nodes for bound basin")
          print(" ".join([str(i) for i in bound_basin_idxs]))
          print(f"Found {len(unbound_basin_idxs)} nodes for unbound basin")
          print(" ".join([str(i) for i in unbound_basin_idxs]))

          msn = basins_to_msn(
              basin_id,
              gexp,
              msn,
              bound_basin_idxs,
              unbound_basin_idxs,
          )


          ## Save it
          save_all_csn_stuff(
              csn_id,
              gexp,
              msn,
              layout_id=layout_id,
              overwrite=True,
          )

  if __name__ == "__main__":

      cli()
#+end_src

****** Evaluate the Basin Quality

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/evaluate_basins.py
  import click
  from seh_pathway_hopping.execution import parse_kwargs

  import matplotlib.pyplot as plt
  import itertools as it

  ## specs

  AXES = [
      'unbound',
      'bound',
      'trim',
  ]

  CASES_SPECS = {
      'unbound' : {
          'label' : "Unbound Cutoff (nm)",
          'cases' : [
              '0.7',
              '0.6',
              '0.5',
          ],
      },

      'bound' : {
          'label' : "Bound Cutoff (nm)",
          'cases' : [
              '0.1',
              '0.2',
              '0.3',
          ],
      },

      'trim' : {
          'label' : "Trim Cutoff (num. transitions)",
          'cases' : [
              '10',
              '5',
              '2',
              '1'
          ],
      },

  }

  CHOOSE_SPECS = {
      'unbound' : '0.6',
      'bound' : '0.3',
      'trim' : '1',
  }

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          get_msn,
          load_msm,
          MSM_SPECS,
          BASIN_SPECS,
          TS_SPECS,
          calc_msm_committors,
          save_gexp_basin_eval_plots,
      )

      kwargs = parse_kwargs(specs)

      # get the show and save args
      if 'save' in kwargs:
          save = bool(kwargs['save'])
      else:
          save = False

      if 'show' in kwargs:
          show = bool(kwargs['show'])
      else:
          show = False


      # This is specified in the basin id since it is dependent on it for trimming
      gexp = kwargs['gexp']
      csn_id = kwargs['csn_id']

      case_0_key = kwargs['case_0']
      case_1_key = kwargs['case_1']

      case_0_label = CASES_SPECS[case_0_key]['label']
      case_1_label = CASES_SPECS[case_1_key]['label']

      # Load the msn
      msn = get_msn(
          csn_id,
          gexp,
      )

      cases_0 = CASES_SPECS[case_0_key]['cases']
      cases_1 = CASES_SPECS[case_1_key]['cases']

      # find which axis is not being used
      no_case_key = [axis for axis in AXES
                      if axis not in (case_0_key, case_1_key)]

      assert len(no_case_key) == 1
      no_case_key = no_case_key[0]

      # get the pinned value for the unused parameter
      no_case_val = CHOOSE_SPECS[no_case_key]


      # number of nodes in committors
      fig, axes = plt.subplots(
          len(cases_0),
          len(cases_1),
          constrained_layout=True,
          figsize=(10,10),
      )

      # sasas in committors
      sasa_fig, sasa_axes = plt.subplots(
          len(cases_0),
          len(cases_1),
          constrained_layout=True,
          figsize=(10,10),
      )

      def find_no_case(
              c0_case,
              c1_case,
      ):

          vals = {}

          for axis in AXES:

              # find the case that is the trim
              panel_case = list(it.filterfalse(
                      lambda x: x != axis,
                      (case_0_key, case_1_key,)
              ))

              if len(panel_case) == 0:

                  vals[axis] = no_case_val

              else:

                  if axis == case_0_key:
                      vals[axis] = c0_case

                  elif axis == case_1_key:
                      vals[axis] = c1_case

          return vals['trim'], vals['bound'], vals['unbound']

      # get the ts ids for these cases
      ts_tups = []
      for c0_idx, c0_case in enumerate(cases_0):
          for c1_idx, c1_case in enumerate(cases_1):

              trim_val, bound_val, unbound_val = find_no_case(
                  c0_case,
                  c1_case,
              )


              # use these to build query strings for matching the TS Spec key
              msm_id = f"cutoff-{trim_val}"

              basin_id = f"msm-{msm_id}_basin-bound-{bound_val}-unbound-{unbound_val}-cutoff"

              ts_id = f"basin-{basin_id}_committor-msmb"

              plot_idx = (c0_idx, c1_idx,)
              ts_tups.append((
                  ts_id,
                  trim_val,
                  unbound_val,
                  bound_val,
                  plot_idx,
              ))

      for plot_idx, ts_tup in enumerate(ts_tups):

          ts_id, trim_val, unbound_val, bound_val, plot_id = ts_tup

          ts_spec = TS_SPECS[ts_id]

          basin_id = ts_spec['basin_id']
          committor_method = ts_spec['committor_method']

          print("----------------------------------------")
          print(f"Doing Analysis for plot: {plot_id}")
          print(f"Trimming Value: {trim_val}")
          print(f"Unbound Value: {unbound_val}")
          print(f"Bound Value: {bound_val}")

          msm_id = BASIN_SPECS[basin_id]['msm_id']

          bound_basin = msn.node_groups[f'basin-{basin_id}/bound_basin']
          unbound_basin = msn.node_groups[f'basin-{basin_id}/unbound_basin']

          print("Bound Basin", bound_basin)
          print("Unbound Basin", unbound_basin)

          print("Calculating committors")
          unbound_committors, bound_committors = calc_msm_committors(
              msm_id,
              gexp,
              bound_basin,
              unbound_basin,
              method=committor_method,
          )
          print("Finished Calculating committors")



          unbound_method = BASIN_SPECS[basin_id]['unbound']['method']
          unbound_params = BASIN_SPECS[basin_id]['unbound']['method_kwargs']

          bound_method = BASIN_SPECS[basin_id]['bound']['method']
          bound_params = BASIN_SPECS[basin_id]['bound']['method_kwargs']

          title_str = f"Trim: {trim_val}; Unbound: {unbound_val}; Bound {bound_val}"


          ## Plot histogram of num nodes over the committor values

          axes[plot_id].hist(unbound_committors)
          axes[plot_id].set_xlabel("Unbound Committor (P)")
          axes[plot_id].set_ylabel("Node Counts")
          axes[plot_id].set_title(title_str)


          ## histogram of total weights over committors

          nodes_total_weights = msn.get_nodes_attribute('_observables/total_weight')

          # get them associated with the committors we have for plotting
          total_weights = []
          for node_idx, committor in enumerate(unbound_committors):

              node_id = msn.node_idx_to_id(node_idx)

              total_weights.append(nodes_total_weights[node_id])

          # plot the weights and set the range to around the TS
          sasa_axes[plot_id].hist(
              unbound_committors,
              weights=total_weights,
              range=(0.3, 1.0),
          )

          # also draw a vertical line at 0.5 committor
          sasa_axes[plot_id].axvline(x=0.5, color='k')

          sasa_axes[plot_id].set_xlabel("Unbound Committor (P)")
          sasa_axes[plot_id].set_ylabel("Total Weight")
          sasa_axes[plot_id].set_title(title_str)


          # OLD: don't do sasas now
          # ## Plot chart of SASAs over the committor probabilities

          # nodes_mean_sasa = msn.get_nodes_attribute('obs/lig-sasa_npoints-100/mean')

          # # get them associated with the committors we have for plotting
          # mean_sasas = []
          # for node_idx, committor in enumerate(unbound_committors):

          #     node_id = msn.node_idx_to_id(node_idx)

          #     mean_sasas.append(nodes_mean_sasa[node_id])

          # sasa_axes[plot_id].scatter(
          #     mean_sasas,
          #     unbound_committors,
          # )
          # sasa_axes[plot_id].set_xlabel("Ligand SASA")
          # sasa_axes[plot_id].set_ylabel("Committor Probability")
          # sasa_axes[plot_id].set_title(title_str)

      if save:

          save_gexp_basin_eval_plots(
              gexp,
              case_0_key,
              case_1_key,
              num_nodes_hist=fig,
              weights_hist=sasa_fig,
          )

      if show:
          print("SHOWING")
          plt.show()

      # save the plots


  if __name__ == "__main__":

      cli()

#+END_SRC


****** Compute Committor Probabilities & Predict TS

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/predict_ts.py

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          get_msn,
          load_msm,
          save_all_csn_stuff,
          CSN_SPECS,
          MSM_SPECS,
          BASIN_SPECS,
          TS_SPECS,
          msn_committors_ts,
          add_ts_to_msn,
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      gexp = kwargs['gexp']
      csn_id = kwargs['csn_id']
      layout_id = kwargs['layout_id']

      # if we have a single basin id requested only do that
      if 'ts_id' in kwargs:
          ts_ids = [kwargs['ts_id']]
      else:
          ts_ids = list(TS_SPECS.keys())

      clf_id = CSN_SPECS[csn_id]['clf_id']

      # load the msn
      msn = get_msn(
          csn_id,
          gexp,
      )

      # do it for each ts spec
      for ts_id in ts_ids:

          basin_id = TS_SPECS[ts_id]['basin_id']

          msm_id = BASIN_SPECS[basin_id]['msm_id']

          ## Load

          msm = load_msm(
              msm_id,
              gexp,
          )

          ## Transformations

          # calculate committors and make TS prediction
          forward_probs, backwards_probs, ts_node_ids = msn_committors_ts(
              msn,
              gexp,
              ts_id,
          )

          print("Adding data to the MSN")
          # save data to the msn
          msn = add_ts_to_msn(
              msn,
              ts_id,
              forward_probs,
              backwards_probs,
              ts_node_ids,
          )

          print("Saving All CSN stuff")
          ## Save it
          save_all_csn_stuff(
              csn_id,
              gexp,
              msn,
              layout_id=layout_id,
              overwrite=True,
          )

  if __name__ == "__main__":

      cli()

#+END_SRC


***** TS PCA Analysis

****** TS PCA Model Selection


#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/ts_pca_model_selection.py
  # size of test set for scoring the PCA model
  TEST_SIZE = 0.25

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  import matplotlib.pyplot as plt
  from tabulate import tabulate

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          ts_pca_model_selection,
          save_tspca_model,
          save_tspca_score_table,
          make_tspca_model_score_table,
          plot_tspca_model_score,
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      csn_id = kwargs['csn_id']
      ts_id = kwargs['ts_id']

      ## Analysis

      # choose which gexps have a TS model that can actually be used for
      # the model selection, i.e. if some simulations couldn't have a TS
      # predicted exclude them
      TS_PCA_MODEL_GEXPS = (
          'TPPU-legacy',
          '3',
          '10',
          '17',
      )

      # calculate the various subsamplings etc for the PCA model and
      # return them all with their scores
      model_results = ts_pca_model_selection(
          TS_PCA_MODEL_GEXPS,
          csn_id,
          ts_id,
          test_size=TEST_SIZE,
      )


      # Save the PCA model and associated data
      for tspca_id, result_d in model_results.items():

          _ = save_tspca_model(
              tspca_id,
              result_d['model'],
              result_d['model_score'],
              result_d['mode_scores'],
              result_d['test_size'],
              result_d['gexps'],
          )

      # make the table of the scores:
      model_score_df = make_tspca_model_score_table(
          model_results,
      )

      # print the table nicely
      table_str = tabulate(
          model_score_df,
          headers=model_score_df.columns,
          tablefmt='orgtbl',
      )

      print("Results of the model selection")
      print(table_str)

      # save it
      save_tspca_score_table(model_score_df)

      # plot scores of the model
      fig, axes = plot_tspca_model_score(model_results)

      # show the plots
      plt.show()

  if __name__ == "__main__":

      cli()

#+END_SRC

****** TODO Calc TSPCA Projection Observable

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/calc_tspca_projection_obs.py
  from functools import partial

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  from multiprocessing.pool import Pool as MPPool

  from wepy.hdf5 import WepyHDF5

  from seh_pathway_hopping._tasks import get_gexp_wepy_h5

  from seh_pathway_hopping._tasks import (
      get_gexp_wepy_h5,
      load_tspca_model,
      observable_traj_fields_hom_com_projections,
  )


  def obs_func(map_func, **kwargs):

      gexp = kwargs['gexp']
      tspca_id = kwargs['tspca_id']

      FIELD = f"tspca_projection/{tspca_id}"
      FIELDS = ['positions', 'box_vectors']
      RUN_IDXS = Ellipsis


      wepy_h5 = get_gexp_wepy_h5(
          gexp,
          mode='r+',
      )

      # load the tspca model
      tspca_model, _, _, _, _ = load_tspca_model(tspca_id)


      chunk_func = partial(
          compute_traj_fields_hom_com_projections,
          tspca_model,
          gexp,
      )

      with wepy_h5:

          observable = wepy_h5.compute_observable(
              func,
              FIELDS,
              (),
              map_func=map_func,
              save_to_hdf5=FIELD,
          )

      print("finished calculation")


  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.option('--n-cores', '-n', type=int, default=1)
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          load_tspca_model,
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      tspca_id = kwargs['tspca_id']
      gexp = kwargs['gexp']


      # load the contigtree for the gexp
      contigtree = get_contigtree(gexp)

      pool = MPPool(n_cores)

      CHUNKSIZE = 500
      def map_func(chunk_func, *args):

          return pool.imap(chunk_func,
                           ,*args,
                           CHUNKSIZE,
                           )

      obs_func(map_func, **kwargs)


  if __name__ == "__main__":

      cli()
#+END_SRC

****** TODO Calc TSPCA Projection Observable MSN Stats

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/calc_tspca_projection_obs_msn_stats.py

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  import matplotlib.pyplot as plt

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      csn_id = kwargs['csn_id']
      tspca_id = kwargs['tspca_id']
      gexp = kwargs['gexp']
      layout_id = kwargs['layout_id']

      ## Analysis

  if __name__ == "__main__":

      cli()
#+END_SRC

****** TODO Make COM Trajectories

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/make_tspca_com_trajs.py

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  import matplotlib.pyplot as plt

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      csn_id = kwargs['csn_id']
      tspca_id = kwargs['tspca_id']
      gexp = kwargs['gexp']

      ## Analysis

  if __name__ == "__main__":

      cli()
#+END_SRC

****** TODO Make TSPCA Projection FE Profiles

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/make_tspca_fe_profiles.py

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  import matplotlib.pyplot as plt

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      csn_id = kwargs['csn_id']
      tspca_id = kwargs['tspca_id']
      gexp = kwargs['gexp']

      ## Analysis

  if __name__ == "__main__":

      cli()
#+END_SRC

****** OLD

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/execution/ts_pc_analysis.py
  # size of test set for scoring the PCA model
  TEST_SIZE = 0.25

  import click
  from seh_pathway_hopping.execution import parse_kwargs

  @click.command(context_settings=dict(
      ignore_unknown_options=True,))
  @click.argument('specs', nargs=-1, type=click.UNPROCESSED)
  def cli(specs):

      from seh_pathway_hopping._tasks import (
          get_msn,
          save_all_csn_stuff,
          CSN_SPECS,
          MSM_SPECS,
          BASIN_SPECS,
          TS_SPECS,
          compute_gexp_ts_pca,
          compute_traj_com_projections,
          save_tspca_model,
          save_com_trajs,
      )

      kwargs = parse_kwargs(specs)

      # This is specified in the basin id since it is dependent on it for trimming
      # msm_id = kwargs['msm_id']
      gexp = kwargs['gexp']
      csn_id = kwargs['csn_id']
      layout_id = kwargs['layout_id']
      ts_id = kwargs['ts_id']

      # load the msn
      msn = get_msn(
          csn_id,
          gexp,
      )

      ## Analysis

      # Make the PCA model and get scores

      pca_model, ts_coms, mode_scores, model_score = compute_gexp_ts_pca(
          gexp,
          csn_id,
          ts_id,
          test_size=TEST_SIZE,
      )

      # make data for 3D visualization of the COMs

      # then we want to visualize the Transition State coms, make the trajectory and PC
      # projection values for each mode
      ts_com_traj, ts_com_pc_projections = compute_traj_com_projections(
          pca_model,
          ts_coms,
      )

      # TODO
      # apply the projections values for the whole network
      # compute_gexp_tspca_projections_csn(
      #     msn_h5,
      #     pca_model,
      # )


      ## Do all the saving at once so its more likely to be consistent

      # Save the PCA model and associated data
      # _ = save_tspca_model(
      #     ts_id,
      #     gexp,
      #     pca_model,
      #     ts_coms,
      #     mode_scores,
      #     model_scores,
      # )

      # # save the TS COMs as a trajectory with the projection values as the color
      # _ = save_com_trajs(
      #     ts_id,
      #     gexp,
      #     ts_com_traj,
      #     pc_projections,
      # )

      # # TODO
      # # add the data of the PC mode projections to the network




      # # Plot the 1D free energy


      # # Plot the 2D free energy


      # print("Saving All CSN stuff")
      # ## Save it
      # save_all_csn_stuff(
      #     csn_id,
      #     gexp,
      #     msn,
      #     layout_id=layout_id,
      #     overwrite=True,
      # )

  if __name__ == "__main__":

      cli()

#+END_SRC

** Troubleshooting

*** TODO Prototype hyperparameter optimization process

**** classes

#+begin_src python :tangle troubleshoot/gmrq_training.py
  import numpy as np

  import skopt
  import skopt.space as sp

  from sklearn.cluster import MiniBatchKMeans
  from sklearn.model_selection import ShuffleSplit

  from seh_pathway_hopping._tasks import *

  class MSMError(Exception):
      """Error for propagating bad GMRQ calculations."""

      pass

  class NoSplit():
      """Splitter implementing API, but doesn't split at all. All features
      go to training."""

      def __init__(self, **kwargs):

          pass

      def split(self, X, **kwargs):

          train_idxs = range(len(X))
          test_idxs = []

          # yield here to satisfy the splitter API
          yield train_idxs, test_idxs

  class Split():
      """Splitter implementing API, but doesn't split at all. All features
      go to training."""

      def __init__(self, **kwargs):

          pass

      def split(self, X, **kwargs):

          train_idxs = range(len(X))
          test_idxs = []

          # yield here to satisfy the splitter API
          yield train_idxs, test_idxs


  def get_clf():

      N_CLUSTERS = 1000
      BATCH_SIZE = 1000

      # make the clusterer with the seed for reproducibility
      clusterer = MiniBatchKMeans(n_clusters=N_CLUSTERS,
                                  batch_size=BATCH_SIZE,
                                  random_state=1)

      return clusterer

  ### Transition Probability Matrix Calculations

  # there are a few ways to do this, copying or using code from
  # different libraries


  ## Transition Probabilities Method 1: Maximum Likelihood (MLE) PYEMMA
  ## version
  def transprob_mle_pyemma(counts_mat):

      from msmtools.estimation \
          import transition_matrix \
          as pyemma_transmat_mle

      # this uses the MLE method
      transprob_mat, stationary_distribution = \
          pyemma_transmat_mle(
              counts_mat,
              # this should be calculated by this method, so we set to None
              # here
              mu=None,

              # this is which algorithms to use based on the sparsity of the
              # matrix, which we just set to auto since we should get the
              # same answer either way and let it figure it out.
              method='auto',

              # this has a bunch of options that go with it, but
              # this makes the matrix reversible, which is what we
              # want, rest of the options assume this is True
              reversible=True,

              # return the stationary distribution as well
              return_statdist=True,

          )

      return transprob_mat, stationary_distribution

  ## Transition Probabilities Method 2: Maximum Likelihood (MLE)
  ## MSMBuilder version
  def transprob_mle_msmb(counts_mat):
      # MSMBuilder way

      from msmbuilder.msm._markovstatemodel \
          import _transmat_mle_prinz \
          as msmb_transmat_mle

      # no options here except the tolerance
      transprob_mat, stationary_distribution = \
          msmb_transmat_mle(counts_mat)


      return transprob_mat, stationary_distribution

  ## Transition Probabilities Method 3: Transpose method
  def transprob_transpose(counts_mat):

      rev_counts = 0.5 * (counts + counts.T)

      populations = rev_counts.sum(axis=0)
      populations /= populations.sum(dtype=float)
      transmat = rev_counts.astype(float) / rev_counts.sum(axis=1)[:, None]

      return transmat, populations

  def subset_trim(countsmat, subset_idxs):

      # K is the input states, N is the output states
      in_n_states = countsmat.shape[0]
      out_n_states = len(subset_idxs)

      # allocate trimmed matrix
      trim_countsmat = np.zeros(
          (out_n_states, out_n_states,),
          dtype=countsmat.dtype,
      )

      # copy values for the subset
      for out_idx, in_idx in enumerate(subset_idxs):

          trim_countsmat[out_idx, :] = countsmat[in_idx, subset_idxs]
          trim_countsmat[:, out_idx] = countsmat[subset_idxs, in_idx]

      # make the mapping
      mapping = {in_idx : out_idx
                 for out_idx, in_idx in enumerate(subset_idxs)}

      return trim_countsmat, mapping

  def ergodic_trim(
          countsmat,
          ergodic_cutoff=1,
          method='csnanalysis',
  ):
      """Trim a counts matrix and get the largest connected component.

      Parameters
      ----------

      countsmat : arraylike (K_states, K_states)

      ergodic_cutoff : int or float

      Returns
      -------



      """

      from msmtools.estimation import \
          connected_sets as pyemma_connected_set

      from csnanalysis.csn import CSN

      from msmbuilder.msm.core import \
          _strongly_connected_subgraph as msmb_strongly_connected_subgraph

      # The CSNAnalysis way
      if method == "csnanalysis":

          csn = CSN(countsmat)
          csn.trim(min_count=ergodic_cutoff)

          chosen_subset = csn.trim_indices

      # the MSMBuilder way
      elif method == "msmbuilder":

          # NOTE: ergodic_cutoff here should be a float and should be
          # very small, i.e. it is not a count like in the CSNAnalysis
          # way.
          countsmat, mapping, _ = \
                  msmb_strongly_connected_subgraph(
                      countsmat,
                      weight=ergodic_cutoff,
                      verbose=False,
                      )

          chosen_subset = None


      # ALERT: the pyemma way, this wasn't working for me
      elif method == "pyemma":

          subsets = pyemma_connected_sets(countsmat)

          # get the index of largest one and the set itself
          subset_sizes = [len(subset) for subset in subsets]

          largest_subset_idx = np.argmax(subset_sizes)

          # choose the one to use
          chosen_subset = subsets[largest_subset_idx]



      # get the submatrix of this along with the mapping for the
      # countsmat trimmings
      trim_countsmat, trimming_mapping = subset_trim(countsmat, chosen_subset)

      return trim_countsmat, trimming_mapping

  def make_pyemma_msm(msn):

      from pyemma.msm import MSM as MarkovStateModel

      # get the transition probability matrix of the strongly ergodic
      # component

      # Get the connected subset we are interested in
      trim_countsmat, trimming_mapping = ergodic_trim(
          msn.countsmat,
          ergodic_cutoff=1,
          method="csnanalysis",
      )

      tprob_mat, populations = transprob_mle_pyemma(trim_countsmat)

      msm = MarkovStateModel(
          tprob_mat,
      )

      ## compose the state mapping for the MSN with the trimming mapping
      ## so we know that the idxs in the trimmed countsmat match the
      ## root state idxs
      dict_match = lambda dict1, dict2 : \
          {k: dict2.get(v) for k, v in dict1.items() if v in dict2}

      mapping = dict_match(
          msn.node_id_to_idx_dict(),
          trimming_mapping,
      )

      return msm, mapping

  def make_msm(msn):

      return make_pyemma_msm(msn)

  ### Calculating GMRQ for training and test sets

  # I've chopped it up into a number of different functions for clarity
  # of understanding rather than performance. This really is fast
  # compared to the clustering process so speed is not an issue here at
  # all.

  def map_eigenvectors(source_mapping,
                       source_eigvecs,
                       target_mapping,
  ):
      """Given a mapping of K states that has two sub-mappings N_s and N_t
      (N_s >= N_t) each of which has a set of eigenvectors we want to
      get the eigenvectors from N_s that are equivalent to the
      eigenvectors in N_t.

      Parameters
      ----------

      source_mapping : dict of int to int

      source_eigvecs : arraylike

      target_mapping : dict of int to int

      Returns
      -------

      mapped_eigvecs : arraylike

      """

      # def dict_compose
      dict_match = lambda dict1, dict2 : \
          {k: dict2.get(v) for k, v in dict1.items() if v in dict2}

      # make a mapping between the source states to the target states:
      # N_s : K match K : N_t --> N_s : N_t
      transform_mapping = dict_match(
          # reverse the source mapping so its N_s : K
          {v: k for k, v in source_mapping.items()},
          # this is K : N_t
          target_mapping
      )

      # get the source and value indices as two lists
      source_indices, dest_indices = zip(*transform_mapping.items())

      # make a new eigenvector array that is the size of the target
      mapped_eigvecs = np.zeros((
          len(target_mapping),
          source_eigvecs.shape[1]
      ))

      # if there are any eigenvectors in the source not in the target
      # than they will just be zero here

      # copy the source eigenvectors to the mapped eigenvectors
      mapped_eigvecs[dest_indices, :] = np.take(
          source_eigvecs,
          source_indices,
          axis=0,
      )

      return mapped_eigvecs


  def gmrq_overlap_matrix(
          stationary_distribution,
  ):
      """Compute the overlap matrix (sometimes abbreviated as S) from the
      stationary probability distribution (typically abbreviated as pi)
      for a transition probability matrix.

      Parameters
      ----------

      stationary_distribution : numpy.array of shape (N_s,)
         The stationary probability distribution (pi) of the transition
         probability matrix (T) where N_s is the number of states in the
         state model.

      Returns
      -------

      overlap_matrix : np.array of shape (N_s, N_s)
         The overlap matrix (S) of the stationary probability distribution.

      """

      # S is the typical single letter abbreviation
      return np.diag(stationary_distribution)


  def gmrq_correlation_matrix(
          overlap_matrix,
          transition_matrix,
  ):
      """Compute the correlation matrix for a transition matrix and the
      associated overlap matrix.

      N_s is the number of states in the MSM.

      Parameters
      ----------

      overlap_matrix : np.array of shape (N_s, N_s)
         The overlap matrix (S) of the stationary probability distribution.

      transition_matrix : np.array of shape (N_s, N_s)
         The transition probability matrix (T) of the MSM.

      Returns
      -------

      correlation_matrix : np.array of shape (N_s, N_s)
         The correlation matrix (C) of the stationary probability distribution.

      """

      # C is the common abbreviation here
      return overlap_matrix.dot(transition_matrix)


  def gmrq_score_train_msm(train_msm):
      """Score the MSM based on the eigenvalues of its transition matrix."""

      return np.sum(train_msm.eigenvalues())


  def gmrq_score_test_msm(
          train_msm,
          train_mapping,
          test_msm,
          test_mapping,
  ):
      """Score a test MSM given the training MSM eigenvectors.

      The test_msm must have the same state definitions as the train_msm.

      Parameters
      ----------

      train_msm : pyemma.MarkovStateModel

      train_mapping : dict of int to int

      test_msm : pyemma.MarkovStateModel

      test_mapping : dict of int to int

      Returns
      -------

      test_gmrq : float
          GMRQ score for the test set using the training model.

      """

      # calculate the eigenvectors for the training MSM which will be
      # used to score their performance on the testing MSM. This is A in
      # the McGibbon paper
      A = train_msm.eigenvectors_right()

      # we need to make sure we are comparing the correct eigenvectors
      # between the training and test MSMs because both can have
      # different sets of states which are included in their transition
      # probability matrices due to the need to do ergodic
      # trimming. That means that the root MSN will have K states as
      # specified from clustering but the training MSM has N_train and
      # the test MSM has N_test states which are both subsets of K but
      # are potentially disjoint sets between each other. To reconcile
      # this we have the mappings of indices from K to each N. We use
      # these to match states/eigenvectors from N_train to N_test. This
      # assumes that N_train >= N_test.

      # if the two mappings are identical we can just ignore this
      # mapping step
      if train_mapping != test_mapping:

          # otherwise go ahead and map them and replace the A
          # eigenvectors with the mapped one

          # ALERT: not sure this is correct here. Is it okay if N_train < N_test ?

          # this might fail if the training set is smaller than testing
          # set
          try:
              A = map_eigenvectors(
                  train_mapping,
                  A,
                  test_mapping
              )

          # TODO: -inf or NaN here?

          # if it does fail we automatically score this as -inf, which
          # will result in an infinite score during optimization
          except MSMError:
              breakpoint()
              return np.nan

      # from the test set we compute the overlap and correlation
      # matrices which will be used to calculate the score on the
      # training eigenvectors
      S = gmrq_overlap_matrix(
          test_msm.pi
      )
      C = gmrq_correlation_matrix(
          S,
          test_msm.transition_matrix,
      )

      # compute the GMRQ score of the training eigenvectors on the
      # testing overlap and correlation matrices.

      # we handle linear algebra answers and return NaN if necessary.
      try:
          test_gmrq = np.trace(
              A.T.dot(
                  C.dot(A)
              ).dot(
                  np.linalg.inv(
                      A.T.dot(
                          S.dot(A)
                      )
                  )
              )
          )
      except np.linalg.LinAlgError:
          test_gmrq = np.nan


      return test_gmrq


  ## Training stuff

  # two layers to functions: one that applies a classfier to a
  # particular split of samples, and one that runs all of the splits.
  # another is for aggregating the split scores

  def score_classifier_split(
          clf,
          train_rep_idxs,
          test_rep_idxs,
          gexp=None,
          lag_time=None,
          all_features=None,
          span_runs=None,
  ):
      """Score a classifier given a given splitting of training and sample
      replicates.

      Parameters
      ----------

      clf : classifier following scikit learn API

      train_rep_idxs : list of int
         The simulation replicates in the training set.

      test_rep_idxs : list of int
         The simulation replicates in the testing set.


      gexp : str

      lag_time : int
          Lag time for computing transitions must be 2 or greater.

      all_features : dict of int to features
          A dictionary of all the clustering features by run. Its easier
          to read from memory once and then take from here since there
          is a lot of redundancy in the splits.

      span_runs : dict of int to list of int
          A mapping of a spanning contig (replicate) to the runs it
          contains.

      Returns
      -------

      test_gmrq : float
         The GMRQ score for the test dataset.

      """

      # make sure the "globals" are given
      assert gexp is not None
      assert lag_time is not None
      assert all_features is not None
      assert span_runs is not None

      # use the reps for each split to get the runs for each rep
      train_run_idxs = it.chain(*[span_runs[rep_idx] for rep_idx in train_rep_idxs])
      test_run_idxs = it.chain(*[span_runs[rep_idx] for rep_idx in test_rep_idxs])

      # ALERT sort them here. we need to sort and order the run idxs
      # in such a way that we can restructure them later after
      # desctructuring them here.

      # sort them to get a stable ordering
      train_run_idxs = sorted(train_run_idxs)
      test_run_idxs = sorted(test_run_idxs)

      # we grab the features for the runs here and inline
      # restructure them so we don't have to deal with the garbage
      # collector

      train_samples = unscalarize_features(
        reshape_run_to_features(
            [all_features[run_idx] for run_idx in train_run_idxs]
        )
      )

      test_samples = unscalarize_features(
        reshape_run_to_features(
            [all_features[run_idx] for run_idx in test_run_idxs]
        )
      )

      # perform the clustering on the training data
      clf.fit(train_samples)

      # get the labels from this and reshape into an assignments
      # shape. Restructure them to run_idx -> traj, cycles, 1
      train_labels = {
        run_idx : run_obs
        for run_idx, run_obs
        in zip(train_run_idxs,
               reshape_features_to_run(
                   clf.labels_,
                   gexp,
                   run_idxs=train_run_idxs,
               )
        )
      }

      # we can also get the labels for the training data
      test_labels = clf.predict(test_samples)

      # restructure them
      test_labels = {
        run_idx : run_obs
        for run_idx, run_obs
        in zip(test_run_idxs,
               reshape_features_to_run(
                   test_labels,
                   gexp,
                   run_idxs=test_run_idxs,
               )
        )
      }

      # then we use the clustering data to make a network

      # first get the contigtree which is a subset of runs we are using

      # Training

      # make a sub-contigtree of just the training runs/spans
      train_contigtree = get_contigtree(gexp,
                                      runs=train_run_idxs)

      # make a network with all the labels
      train_msn = MacroStateNetwork(train_contigtree,
                                transition_lag_time=lag_time,
                                assignments=train_labels
      )

      # Testing
      test_contigtree = get_contigtree(gexp,
                                    runs=test_run_idxs)

      test_msn = MacroStateNetwork(test_contigtree,
                                transition_lag_time=lag_time,
                                assignments=test_labels
      )

      # get the counts matrix for both the training data and the test
      # data
      train_countsmat = train_msn.countsmat

      test_countsmat = test_msn.countsmat

      ## Compute the transition probabilities and stationary
      ## distribution, there are different ways to do this, choose the
      ## appropriate one

      # the mappings are relative to their MSNs count matrix

      # make the MSM for the training data
      train_msm, train_mapping = make_msm(train_msn)

      # score the training msm for fun, but this isn't actually used
      train_gmrq = gmrq_score_train_msm(train_msm)

      test_msm, test_mapping = make_msm(test_msn)

      # then score the testing data from this model
      test_gmrq = gmrq_score_test_msm(
        train_msm,
        train_mapping,
        test_msm,
        test_mapping,
      )

  #     # Report on results
  #     msg = f"""
  # Results for split
  # -----------------

  # Train   Test
  # {train} {test}

  # Total number of states: {train_msn.num_states}

  # - Training MSM:
  #   - num states :: {train_msm.nstates}
  #   - GMRQ :: {train_gmrq}

  # - Testing MSM:
  #   - num states :: {test_msm.nstates}
  #   - GMRQ :: {test_gmrq}
  # """

      return test_gmrq


  def score_classifier(
          clf,
  ):
      """Score a classifier."""

      ## Parameters

      GEXP = '10'
      LAG_TIME = 2

      OBSERVABLE = 'lig_rmsd'

      N_SPLITS = 3

      # Specify the cross validation splits
      splitter = KFold(n_splits=N_SPLITS)

      ## Code

      # split into training and testing based on the replicates (spans
      # in the contigtree)

      all_contigtree = get_contigtree(GEXP)

      n_replicates = len(all_contigtree.span_traces)

      replicate_idxs = [i for i in range(n_replicates)]

      # split on the replicates using the splitter
      splits = list(splitter.split(replicate_idxs))

      # OPT: figure out where these should be loaded so as to reduce
      # loading from disk. Here it would would be once per
      # hyperparameter sample which is decent since it would get loaded
      # K times for each fold otherwise.

      # get all the labels for the runs and index them by the run in a
      # dict. Subsamples will draw from here
      all_features = {run_idx : run_obs
                    for run_idx, run_obs in
                    enumerate(get_observable(
                        OBSERVABLE,
                        GEXP,
                        source='fs',))
      }

      # get a mapping of replicate/span to which runs it has
      span_runs = {rep_idx :
                   set((run_idx
                        for run_idx, cycle_idx
                        in span_trace))
                   for rep_idx, span_trace
                   in all_contigtree.span_traces.items()}

      split_scores = []
      # perform the process for each splitting
      for split_idx, split in enumerate(splits):

          train_rep_idxs = split[0]
          test_rep_idxs = split[1]


          test_gmrq = score_classifier_split(
              clf,
              train_rep_idxs,
              test_rep_idxs,
              gexp=GEXP,
              lag_time=LAG_TIME,
              all_features=all_features,
              span_runs=span_runs,
          )

          split_scores.append(test_gmrq)

      # aggregate the scores for each split in cross validation

      stats = {
          'mean' : np.mean(split_scores),
          'median' : np.median(split_scores),
          'min' : np.min(split_scores),
          'max' : np.max(split_scores),
          'std' : np.std(split_scores),
          'var' : np.var(split_scores),
          'scores' : split_scores,
      }

      # this is the final score for the classifier
      return stats

#+end_src


**** Main

#+begin_src python :tangle troubleshoot/gmrq_training.py
  if __name__ == "__main__":

      import itertools as it

      import numpy as np

      import skopt
      import skopt.space as sp
      from skopt.utils import use_named_args

      from sklearn.cluster import MiniBatchKMeans
      from sklearn.model_selection import (
          ShuffleSplit,
          KFold,
      )

      from wepy.analysis.network import MacroStateNetwork

      from seh_pathway_hopping._tasks import *

      ## perform the clustering on the observable using only the
      ## training runs

      # space = [
      #     sp.Integer(2, 1000, name="n_clusters"),
      #     sp.Integer(100, 10000, name="batch_size"),
      # ]

      space = [
          sp.Integer(100, 110, name="n_clusters"),
      ]

      @use_named_args(space)
      def objective(**params):

          clf = MiniBatchKMeans(
              n_clusters=params['n_clusters'],
              batch_size=100,
          )

          stats = score_classifier(clf)


          # the final score is the negative of the mean
          return -stats['mean']


      ## There are different ways to control optimization with skopt

      method = 'highlevel'

      # the high level function and the sklearn compatible wrapper both
      # use callbacks to get info. Otherwise you can control the loop
      # with the optimizer.

      # callbacks for reporting
      def print_result_cb(res):

          msg = f"""
  Current Results
  ---------------

  - last objective function score :: {res['fun']}
  - solution :: {res['x']}
  """
          print(msg)

      verbose_cb = skopt.callbacks.VerboseCallback(1)

      DEADLINE = 108000 # seconds
      deadline_cb = skopt.callbacks.DeadlineStopper(DEADLINE)

      # use the high level function
      if method == 'highlevel':

          res_gp = skopt.gp_minimize(
              objective,
              space,
              n_calls=10,
              callback=[
                  print_result_cb,
                  verbose_cb,
                  deadline_cb,
              ],
          )

      elif method == 'sklearn':

          pass

      elif method == 'asktell':

          optimizer = skopt.Optimizer(
              space,
          )

          for step in range(3):

              # get the next sample of hyperparameters from the optimizer
              next_sample = optimizer.ask()

              print(next_sample)

              # compute the objective function
              obj_val = objective(next_sample)

              print(f"Objective Function Score: {obj_val}")

              # Report the score for the sample to the optimizer

              optimizer.tell(
                  next_sample,
                  obj_val,
              )
#+end_src

*** COMMENT Running observable computations on HPCC

The issue is having the right data since I aggregate it on my local
computer:

#+begin_src python
  from seh_pathway_hopping._tasks import *

  h5_path = gexp_wepy_h5_path('10')

  from wepy.hdf5 import WepyHDF5
#+end_src

*** COMMENT Patching Continuations to ligand 3 HDF5

#+begin_src python :tangle troubleshoot/patch_lig10_results_h5_continuations.py
  from seh_pathway_hopping._tasks import *

  GEXP = '3'

  orch = get_gexp_master_orch(GEXP)
  wepy_h5 = get_gexp_wepy_h5(GEXP)

  # # go through each run and set the continuations in the hdf5
  # for run_id in orch.run_hashes():

  #     continued_run_id = orch.run_continues(*run_id)

  #     continuation = ()
#+end_src

*** DONE COMMENT Patching Continuations to ligand 10 HDF5

#+begin_src python :tangle troubleshoot/patch_lig10_results_h5_continuations.py
  from seh_pathway_hopping._tasks import *

  GEXP = '10'

  orch = get_gexp_master_orch(GEXP)
  wepy_h5 = get_gexp_wepy_h5(GEXP)

  # # go through each run and set the continuations in the hdf5
  # for run_id in orch.run_hashes():

  #     continued_run_id = orch.run_continues(*run_id)

  #     continuation = ()
#+end_src
*** DONE COMMENT Get the WExplore Resampler PMIN

#+begin_src python

  from wepy.orchestration.orchestrator import Orchestrator

  snap_orch_path = "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/20_simulations/jobs/3546061/lig-20_contig-0-1_production/checkpoint.orch.sqlite"

  orch = Orchestrator(snap_orch_path, mode='r')

  snaphash_begin = '892e307e4a9906d3749b7cd8c0e4a098'
  begin_snap = orch.get_snapshot(snaphash_begin)
  print(begin_snap.apparatus.resampler.pmin)

  snaphash_end = '92047914ba43c1dfadd28a33289c4a48'
  end_snap = orch.get_snapshot(snaphash_end)

  # ALERT: interestingly there is no resampler attribute for this one...
  # print(end_snap.apparatus.resampler.pmin)
  print(end_snap.apparatus.filters[2].pmin)
#+end_src

*** DONE COMMENT Debug hard transitions in lig 10 linneages

#+begin_src python
  gexp = '10'

  span_id = 0
  walker_idx = 20

  cycles = (24, 25)



  from seh_pathway_hopping._tasks import *


  lig_atom_idxs = lig_selection_idxs(gexp)['main_rep/ligand']

  sel_idx = lig_atom_idxs[0]

  contig = get_gexp_span_contig(gexp, span_id)

  lineage_traces = get_warp_lineage_traces(gexp, span_id)

  def show_transition(trace):

      with contig.wepy_h5:
          fields = contig.wepy_h5.get_trace_fields(trace, ['positions'])

      return fields['positions'][cycles, sel_idx, :]

  print(show_transition(lineage_traces[0]))
#+end_src
*** COMMENT Spans DF

#+begin_src python

  from wepy.orchestration.orchestrator import Orchestrator

  COMMON_ROOT = '6a33f4a4746dc7a1e83296a727c1f8b5'


  # we manually specify the spans since we do patching on the starting
  # snapshots which screws it all up
  SPANS = {
      0 : [
          (COMMON_ROOT, '6d2c47827d6c6bfcc7974fda080f4378'),
          ('e5cdd6a932d7ef99f42af4d0fc0b13c3', '0fe3b4450344a149fdf6778b013dcd79'),
      ],

      1 : [
          (COMMON_ROOT, 'e160f24a6155bf7664b2e6a41b8b2da7'),
      ],

      2 : [
          (COMMON_ROOT, 'af45df1ec10ad6286f3c7ca37401ddef'),
          ('a727fe1b95ef3a91aa103a30f3c734c9', 'f9d613627fdd07fd714afb6a025084e8'),
      ],

      3 : [
          (COMMON_ROOT, '8d219d1788b9c8d3c5f8f649755f1221'),
          ('64c6723e262b80a4a99b4f69a2ebabb9', '61cb809ee5a2fcc8ac2ca96203999770'),
      ],

      4 : [
          (COMMON_ROOT, '1bf3e875f58e8ea7996efb2a3dbce966'),
          ('7f24b6fe68739af9536e1eebf14d870f', '6328a9234a028649ddf94bc40499e083'),
      ],

      5 : [
          ('0b7c48749264490eadac5af79987ca00', 'd6c98acf0b786c8dd94cf957469d0dcf'),
      ],
  }


  def report_spans(orch, spans):
      run_records = orch.get_run_records()
      run_lengths = {(start, end) : n_cycles for start, end, _, n_cycles in run_records}
      span_lengths = {span_id : 0 for span_id in spans.keys()}
      for span_id, span_segs in spans.items():
          for run_id in span_segs:
              span_lengths[span_id] += run_lengths[run_id]
      return span_lengths



  orch = Orchestrator("hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite",
                      mode='r')


#+end_src


*** COMMENT Testing which bound ensembles are the best

 #+BEGIN_SRC python
   import numpy as np

   import matplotlib.pyplot as plt

   from csnanalysis.csn import CSN

   from seh_pathway_hopping._tasks import *

   MODEL = 'kcenters_canberra_bs_lig_pair_dists'
   LIG_ID = 3
   LAG_TIME = 2

   net = lig_state_network(LIG_ID, MODEL, LAG_TIME)

   # load the graph with the hand selections and incorporate it
   gephi_graph = load_gephi_graph(MODEL, LIG_ID, LAG_TIME, name_pattern='main-handsel-0')
   net = update_network_from_gephi(net, gephi_graph, layout_name=None)


   # make a CSN
   csn = CSN(np.asarray(net.countsmat, dtype=int))

   # trim it to the main components

   # TODO: should we be using 1e-12 since the counts are weighted now
   csn.trim(min_count=1)

   # get the indices that were trimmed off, for their node_ids
   trimmed_group = [net.node_idx_to_id(idx) for idx in
                    set(range(csn.nnodes)).difference(csn.trim_indices)]

   # make a group on the network for this
   net.set_node_group('trimmed_nodes', trimmed_group)


   # the unbound basin
   warp_unbound_basin = compute_csn_unbound_basin_warp(MODEL, LIG_ID)

   # add this as a group to the network
   net.set_node_group('unbound_basin/warp', warp_unbound_basin)

   # get the different models for the bound basin
   bigstate_bound_basin = compute_csn_bound_basin_simple(MODEL, LIG_ID)
   natives_bound_basin = compute_csn_bound_basin_natives(MODEL, LIG_ID)

   RMSD_CUTOFFS = (0.1, 0.2, 0.3)
   rmsd_bound_basins = []
   for cutoff in RMSD_CUTOFFS:
       rmsd_bound_basins.append(compute_csn_bound_basin_rmsd(MODEL, LIG_ID, cutoff))

   FEATDIST_CUTOFFS = (0.07, 0.1, 0.2, 0.3)
   featdist_bound_basins = []
   for cutoff in FEATDIST_CUTOFFS:
       featdist_bound_basins.append(compute_csn_bound_basin_feature_dist(MODEL, LIG_ID, cutoff))


   # we also have some hand selected basins for testing out picking
   # nodes
   handsel_keys = []
   handsel_bound_basins = []
   for group_name, group in net.node_groups.items():

       if group_name.startswith('bound_basin/handsel'):
           handsel_key = '/'.join(group_name.split('/')[1:])
           handsel_keys.append(handsel_key)
           handsel_bound_basins.append(group)


   bound_basins = [
       ('bigstate', bigstate_bound_basin),
       ('natives', natives_bound_basin),
   ]

   bound_basins.extend(list(zip(handsel_keys, handsel_bound_basins)))
   bound_basins.extend([('rmsd/{}'.format(cutoff), basin)
                        for cutoff, basin in zip(RMSD_CUTOFFS, rmsd_bound_basins)])
   bound_basins.extend([('featdist/{}'.format(cutoff), basin)
                        for cutoff, basin in zip(FEATDIST_CUTOFFS, featdist_bound_basins)])

   # convert the unbound ids to idxs
   warp_unbound_basin_idxs = [net.node_id_to_idx(node_id) for node_id in warp_unbound_basin]
   for bound_name, bound_basin in bound_basins:

       # add the bound basin as a group
       net.set_node_group('bound_basin/{}'.format(bound_name), bound_basin)

       # convert the node_ids to node_idxs
       bound_basin_idxs = [net.node_id_to_idx(node_id) for node_id in bound_basin]

       # figure out the basins
       basins = [bound_basin_idxs, warp_unbound_basin_idxs]

       # calculate the commitor probabilities
       committors = csn.calc_committors(basins, labels=['bound', 'unbound'])

       bound_committors = committors[:,0]
       unbound_committors = committors[:,1]

       # get nodes which are in the transition state, and convert to ids
       ts_group = [net.node_idx_to_id(idx) for idx in
                   np.argwhere((bound_committors >= 0.4) & (bound_committors <= 0.6)).flatten()]

       # set the transition state observable
       if len(ts_group) > 0:
           net.set_node_group('TS/warp+{}'.format(bound_name), ts_group)
       else:
           print("No transition state nodes found for {}".format('warp+{}'.format(bound_name)))

       bound_values = {net.node_idx_to_id(node_idx) : val
                       for node_idx, val in enumerate(bound_committors)}
       unbound_values = {net.node_idx_to_id(node_idx) : val
                         for node_idx, val in enumerate(unbound_committors)}

       # set these as node observables
       net.set_nodes_observable('bound_committor/warp+{}'.format(bound_name),
                                bound_values)

       net.set_nodes_observable('unbound_committor/warp+{}'.format(bound_name),
                                unbound_values)

   gephi_graph = load_gephi_graph(MODEL, LIG_ID, LAG_TIME, name_pattern='main')
   net = update_network_from_gephi(net, gephi_graph, layout_name='main')

   save_gexf(MODEL, LIG_ID, LAG_TIME, net)
 #+END_SRC

*** COMMENT figuring Bad transitions between warped clusters

 #+BEGIN_SRC python
   from seh_pathway_hopping._tasks import *

   CLUSTERING_MODEL = 'kcenters_canberra_bs_lig_pair_dists'
   LIG_ID = 3
   LAG_TIME = 2

   native_node = classify_native_state_cluster(CLUSTERING_MODEL, LIG_ID)
   unbound_basin = compute_csn_unbound_basin_warp(CLUSTERING_MODEL, LIG_ID)

   net = get_gephi_network(CLUSTERING_MODEL, LIG_ID, LAG_TIME)

   # get the trace for the assignments in the weird group
   warp_nodes_micro_traces = {}
   for node_id in unbound_basin:
       assgs = net.get_node_attribute(node_id, 'assignments')
       warp_nodes_micro_traces[node_id] = assgs

   native_trace = net.get_node_attribute(native_node, 'assignments')

   # figure out which contig this is in
   contigtree = lig_contigtree(LIG_ID)

   # get all of the raw warping records with the run idxs
   with contigtree.wepy_h5 as wepy_h5:
       warping_run_trace = []
       for run_idx in wepy_h5.run_idxs:
           recs = wepy_h5.warping_records_dataframe(run_idx).to_dict(orient='records')

           for i in range(len(recs)):
               warping_run_trace.append((run_idx, recs['walker_idx'], recs['cycle_idx']))


   # for node_id, warp_nodes_micro_trace in warp_nodes_micro_traces.items():
   #     print("working on node {} which has {} microstates".format(node_id, len(warp_nodes_micro_trace)))

   #     for microstate_idx, micro_rec in enumerate(warp_nodes_micro_trace):
   #         print("working on microstate {}".format(microstate_idx))

   #         micro_run_idx, micro_traj_idx, micro_frame_idx = micro_rec

   #         # get which span this is in
   #         matching_span = None
   #         for span_idx, span_trace in contigtree.span_traces.items():
   #             span_run_idxs = set([run_idx for run_idx, frame_idx in span_trace])

   #             if micro_run_idx in span_run_idxs:
   #                 matching_span = span_idx
   #                 break

   #         assert matching_span is not None

   #         # with the span determine if this was a warped microstate
   #         contig = contigtree.span_contig(span_idx)

   #         # get the warping records for the span
   #         with contig:
   #             warp_df = contig.warping_records_dataframe()

   #         # convert the microstate run trace to a contig trace
   #         micro_walker_idx, micro_contig_cycle_idx = contigtree.run_trace_to_contig_trace([micro_rec])[0]

   #         # then query whether this was a warp or not
   #         match_rows = warp_df[(warp_df['walker_idx'] == micro_walker_idx) &
   #                              (warp_df['cycle_idx'] == micro_contig_cycle_idx)]

   #         if match_rows.shape[0] > 0:

   #             print("Matched to a warp event")

   #             warp_microstates.append({
   #                 'span_idx' : matching_span,
   #                 'contig_rec' : (micro_walker_idx, micro_contig_cycle_idx),
   #                 'run_rec' : micro_rec,
   #                 'run_idx' : micro_run_idx,
   #                 'node_id' : node_id,
   #             })

 #+END_SRC

*** COMMENT Redoing clustering predictions and assignments

 #+BEGIN_SRC python 
   import numpy as np

   from seh_pathway_hopping._tasks import (
       get_clustering_model,
       get_observable,
   )

   feature_obs = 'bs_lig_pair_dists'
   lig_id = 3

   clf = get_clustering_model('kcenters_canberra_bs_lig_pair_dists', lig_id)

   features = np.concatenate(get_observable(feature_obs, lig_id))

   labels = clf.predict(features)

   del features
 #+END_SRC


 Now that we have the labels we can reshap them to try and get the
 right labels:

 #+BEGIN_SRC python
   from seh_pathway_hopping._tasks import *

   from wepy.analysis.network import MacroStateNetwork
   from wepy.hdf5 import WepyHDF5

   import joblib

   from csnanalysis.csn import CSN

   with open('data/tmp/feature_labels.jl.pkl', 'rb') as rf:
       labels = joblib.load(rf)

   contigtree = lig_contigtree(3)

   # then do it for the run based assignments in the HDF5
   run_assignments = reshape_features_to_obs(labels, 3)

   with WepyHDF5(gexp_wepy_h5_path(3), mode='r+') as wepy_h5:

       wepy_h5.add_observable('kcenters_canberra_bs_lig_pair_dists',
                              run_assignments)

   run_net = MacroStateNetwork(contigtree, transition_lag_time=2,
                               assignments=run_assignments)

   run_net.set_macrostate_weights()

   # write out the gexf file for visualization
   run_net.write_gexf('data/tmp/test.gexf')

   run_csn = CSN(run_net.countsmat)

 #+END_SRC


 Still looks weird so write out a cluster for visualization.

 #+BEGIN_SRC python
   node_id = 819

   from wepy.util.mdtraj import traj_fields_to_mdtraj

   from seh_pathway_hopping._tasks import recenter_superimpose_traj

   with run_net.wepy_h5:
       top = run_net.wepy_h5.get_topology()

   traj_fields = run_net.get_node_fields(node_id, ['positions', 'box_vectors'])

   traj_fields['positions'] = recenter_superimpose_traj(traj_fields, 3, 'main_rep')[0]

   traj = traj_fields_to_mdtraj(traj_fields, top)

   traj.save_dcd('data/tmp/debug_cluster.dcd')

 #+END_SRC


 This was good. SO rescue the labels.

 #+BEGIN_SRC python
   from seh_pathway_hopping._tasks import *

   import joblib

   with open('data/tmp/feature_labels.jl.pkl', 'rb') as rf:
       labels = joblib.load(rf)

   run_assignments = reshape_features_to_obs(labels, 3)

   save_observable('kcenters_canberra_bs_lig_pair_dists', 3, run_assignments)


 #+END_SRC
*** COMMENT Copy run slice testing

 #+BEGIN_SRC python :tangle hpcc/scripts/test_run_slice_copy.py
   import os.path as osp

   from wepy.hdf5 import WepyHDF5

   tmp_dir = "/mnt/gs18/scratch/users/lotzsamu/seh.pathway_hopping/tmp"

   source_path = "play.wepy.h5"

   wepy_h5 = WepyHDF5(source_path, 'r')

   test_h5_path = osp.join(tmp_dir, 'play_test.wepy-run.h5')
   with wepy_h5:

       # clone a new header for the new WepyHDF5
       new_wepy_h5 = wepy_h5.clone(test_h5_path, mode='w')

       # then copy a slice of a run into the new file at the subpath
       # specified
       new_h5 = wepy_h5.copy_run_slice(0, test_h5_path, '/runs/0',
                              run_slice=(0, 550),
                              mode='r+')
 #+END_SRC

**** COMMENT
 A quick patch to add in the units to this test file since they were
 dropped at one point in time:

 #+BEGIN_SRC python :tangle hpcc/scripts/add_units.py
   import os.path as osp

   from wepy.hdf5 import WepyHDF5

   from setup_paths import tmp_dir, lig3_all_wepy

   wepy_h5 = WepyHDF5(lig3_all_wepy, 'r')

   units = {
   'box_vectors_unit' : 'nanometer',
   'box_volume_unit' : 'nanometer',
   'forces_unit' : 'kilojoule/(nanometer*mole)',
   'kinetic_energy_unit' : 'kilojoule/mole',
   'positions_unit' : 'nanometer',
   'potential_energy_unit' : 'kilojoule/mole',
   'time_unit' : 'picosecond',
   'velocities_unit' : 'nanometer/picosecond',
   }

   test_h5_path = osp.join(tmp_dir, 'test.h5')
   with wepy_h5:
       grp = wepy_h5.h5.require_group("/units")
       for unit_name, unit in units.items():
           grp.create_dataset(unit_name, data=unit)
 #+END_SRC


*** COMMENT Restarting Simulations


 NOTE: this part may not be necessary if you directly reconcile the
 checkpoint into the orchestrator. Awaiting confirmation that this
 works. If this works then you won't actually use the recover mechanism
 and you will just continue it with a normal run.


 If your simulation dies due to a failure we need to restart it from a
 checkpoint.

 We will need to add the checkpoint to the inputs, but we want to
 organize this by the job name. Just set which ligand and the JOBID you
 want to recover from.

**** copy checkpoints to inputs

 e.g.
 #+BEGIN_SRC bash
 file-checkpoints 3 2577900 2577901
 #+END_SRC

 #+NAME: file-run-checkpoints
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   file-checkpoints() {

         lig_id=$1

         projects_dir="/mnt/home/lotzsamu/projects"
         project_dir="${projects_dir}/seh.pathway_hopping"

         sims_dir="${project_dir}/simulations"

         # to localize either in the research or the home directory
         # structure

         # home
         lig_sim_dir="${sims_dir}/${lig_id}_simulations"

         input_dir="${lig_sim_dir}/input"

         jobs_dir="${lig_sim_dir}/jobs"

         nargs=$#
         let "njobargs = $nargs - 1"
         job_ids=("${@:2:$njobargs}")

         for job_id in ${job_ids[@]}; do

             job_dir=(${jobs_dir}/${job_id})

             # uniformly they should all be called "output" from now on
             output_dir="${job_dir}/output"


             # to localize the orchestrators from the scratch dir to the
             # outputs dir we need to know: 1) the path to the orchestrator
             # we want to patch and 2) the new "work_dir" that the outputs
             # were put into.

             # to get the paths to the orchestrators in the simulation
             # results dir we need to get the name of it which should be
             # the only dir
             sim_results_dir=$(ls -d ${output_dir}/*/)

             echo "The results dir is: ${sim_results_dir}"

             # then get the basename alone for getting the new path later
             sim_results_dirname=$(basename ${sim_results_dir})

             # find the orchestrator the simulation produced (if at all)
             # this will either be a finished one or a checkpoint, and
             # either of those maybe patched or not. If a patched one exist
             # we preferentially choose that.
             checkpoint_orch=(${sim_results_dir}/checkpoint.chk)
             patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_V2)

             # the first priority is the patched checkpoint
             if [[ -e $patched_checkpoint_orch ]]; then
                 echo "Found a patched checkpoint for $job_id"
                 checkpoint_path=$patched_checkpoint_orch

             # second priority is an unpatched checkpoint
             elif [[ -e $checkpoint_orch ]]; then
                 echo "Found an unpatched checkpoint for $job_id"
                 checkpoint_path=$checkpoint_orch

             # and report if none was found just in case
             else
                 echo "No orchestrator found for JOB ${job_id}";
                 checkpoint_path="None"

             fi

             cp ${checkpoint_path} "${input_dir}/${job_id}.chk"

         done
   }
 #+END_SRC


**** get the end hashes of the contigs from the checkpoints

 It can be onerous to get all the hash information from the checkpoints
 so this is a jig to do it for us given a ligand ID and a list of jobids.

 Makes a file called "checkpoint_hashes.txt" with the info in it.

 #+NAME: checkpoint_end_hashes
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   checkpoint-end-hashes () {

       lig_id=$1

       conda activate seh_pathway_hopping

       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"

       sims_dir="${project_dir}/simulations"

       # to localize either in the research or the home directory
       # structure

       # home
       lig_sim_dir="${sims_dir}/${lig_id}_simulations"

       checkpoint_hash_log="${lig_sim_dir}/checkpoint_hashes.txt"

       jobs_dir="${lig_sim_dir}/jobs"

       nargs=$#
       let "njobargs = $nargs - 1"
       job_ids=("${@:2:$njobargs}")

       for job_id in ${job_ids[@]}; do

           job_dir=(${jobs_dir}/${job_id})

           # uniformly they should all be called "output" from now on
           output_dir="${job_dir}/output"


           # to localize the orchestrators from the scratch dir to the
           # outputs dir we need to know: 1) the path to the orchestrator
           # we want to patch and 2) the new "work_dir" that the outputs
           # were put into.

           # to get the paths to the orchestrators in the simulation
           # results dir we need to get the name of it which should be
           # the only dir
           sim_results_dir=$(ls -d ${output_dir}/*/)

           # then get the basename alone for getting the new path later
           sim_results_dirname=$(basename ${sim_results_dir})

           # find the orchestrator the simulation produced (if at all)
           # this will either be a finished one or a checkpoint, and
           # either of those maybe patched or not. If a patched one exist
           # we preferentially choose that.
           checkpoint_orch=(${sim_results_dir}/checkpoint.chk)
           patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_V2)

           # the first priority is the patched checkpoint
           if [[ -e $patched_checkpoint_orch ]]; then
               echo "Found a patched checkpoint for $job_id"
               checkpoint_path=$patched_checkpoint_orch

           # second priority is an unpatched checkpoint
           elif [[ -e $checkpoint_orch ]]; then
               echo "Found an unpatched checkpoint for $job_id"
               checkpoint_path=$checkpoint_orch

           # and report if none was found just in case
           else
               echo "No orchestrator found for JOB ${job_id}";
               checkpoint_path="None"

           fi

           # list the run hashes for this checkpoint
           echo "Job Id: ${job_id}" >> ${checkpoint_hash_log}
           wepy ls-runs ${checkpoint_path} >> ${checkpoint_hash_log}

       done
   }
 #+END_SRC

*** COMMENT special post-v3 orch reconcile


***** COMMENT Without reconciling the HDF5s only orches:
 #+NAME: v3-reconcile-run_orchs
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   v3-reconcile-run-orchs() {
       # get the ligand id and set up paths for this ligand
       lig_id="$1"
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       scripts_dir="${project_dir}/scripts"
       sim_dir="${project_dir}/simulations"
       lig_sim_dir="${sim_dir}/${lig_id}_simulations"
       #input_dir="${lig_sim_dir}/input"
       lig_results_dir="${lig_sim_dir}/results"
       mkdir -p ${lig_results_dir}
       orch_dir="${lig_sim_dir}/orchs"
       mkdir -p "$orch_dir"

       # get just the job ids as an array
       nargs=$#
       let "njobargs = $nargs - 1"
       job_ids=("${@:2:$njobargs}")

       # then loop over them to get the paths to the orchestrators from
       # them
       echo ""
       echo "Getting Paths"
       orch_paths=()
       for i in ${!job_ids[@]}; do

           job_id=${job_ids[$i]}
           echo $job_id

           # get the directory for the job
           job_dir="${lig_sim_dir}/jobs/${job_id}"
           echo $job_dir

           # we need to rename them to the original folder that the
           # orchestrator is expecting. Instead of the name of the output
           # folder my job submission system renames it to. If this was
           # already done ignore these and their messages.
           output_dir="${job_dir}/output"

           # Note we run the localize_reporter_path.py script over
           # them with the localized paths we won't need to do
           # this. Localizing is the less confusing thing in the future
           # once we take on the burden of renaming paths in the
           # orchestrator

           # if ! (mv ${output_dir} ${job_dir}/exec); then echo "${jobid} already renamed"; fi
           # output_dir="${job_dir}/exec"


           # to get the paths to the orchestrators in the simulation
           # results dir we need to get the name of it which should be
           # the only dir
           sim_results_dir=$(ls -d ${output_dir}/*/)
           # remove trailing slash
           sim_results_dir=${sim_results_dir%/}

           echo "The results dir is: ${sim_results_dir}"

           # find the orchestrator the simulation produced (if at all)
           # this will either be a finished one or a checkpoint
           # only patched ones
           # orch=(${sim_results_dir}/*.orch)

           patched_orch_2=(${sim_results_dir}/*.orch.PATCH_V2.PATCH_v3.sqlite)
           patched_orch=(${sim_results_dir}/*.orch.PATCH_v3.sqlite)
           # checkpoint_orch=(${sim_results_dir}/checkpoint.chk)
           patched_checkpoint_orch_2=(${sim_results_dir}/checkpoint.chk.PATCH_V2.PATCH_v3.sqlite)
           patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_v3.sqlite)

           # first priority is the patched run orch
           if [[ -e $patched_orch_2 ]]; then
               echo "Found a patched (2) orchestrator for $job_id"

               # if the job succeeded there will be a file generated which is
               # the orchestrator for the end of that simulation
               sim_orch_path=$patched_orch_2

           elif [[ -e $patched_orch ]]; then
               echo "Found a patched orchestrator for $job_id"

               # if the job succeeded there will be a file generated which is
               # the orchestrator for the end of that simulation
               sim_orch_path=$patched_orch

           elif [[ -e $patched_checkpoint_orch_2 ]]; then
               echo "Found a patched checkpoint (2) for $job_id"
               sim_orch_path=$patched_checkpoint_orch_2


           elif [[ -e $patched_checkpoint_orch ]]; then
               echo "Found a patched checkpoint for $job_id"
               sim_orch_path=$patched_checkpoint_orch

           # # third priority is an unpatched run orch
           # elif [[ -e $orch ]]; then
           #     echo "Found an unpatched orchestrator for $job_id"
           #     sim_orch_path=$orch

           # # final priority is an unpatched checkpoint
           # elif [[ -e $checkpoint_orch ]]; then
           #     echo "Found an unpatched checkpoint for $job_id"
           #     sim_orch_path=$checkpoint_orch

           # and report if none was found just in case
           else
               echo "No orchestrator found for JOB ${job_id}";
               sim_orch_path="None"

           fi

           # and save it in the array of the orchestrator paths
           orch_paths[i]=${sim_orch_path}
       done

       echo ""
       echo "orchestrators being reconciled"
       for orch in ${orch_paths[@]}; do
           echo "$orch"
       done

       # backup the old orchestrator and hdf5
       # cp "${input_dir}/sEH_lig-${lig_id}.orch" "${orch_dir}/sEH_lig-${lig_id}_BACKUP.orch"
       # mv "${lig_results_dir}/all_results.wepy.h5" "${lig_results_dir}/all_results_BACKUP.wepy.h5"


       # do the reconciliation
       conda activate seh_pathway_hopping
       echo "Reconciling"

       python ${scripts_dir}/reconcile_v3.py \
            $lig_id \
            "$lig_sim_dir/master-v3_sEH_lig-${lig_id}.orch.sqlite" \
            ${orch_paths[*]}

   }

 #+END_SRC

***** COMMENT reconcile orchs and hdf5s

 Doesn't do this yet. use the orch only one which has special handling
 and then we can generate the HDF5s later

 #+NAME: v3-reconcile-runs
 #+BEGIN_SRC bash :tangle hpcc/scripts/bash_funcs.sh
   v3-reconcile-runs() {
       # get the ligand id and set up paths for this ligand
       lig_id="$1"
       projects_dir="/mnt/home/lotzsamu/projects"
       project_dir="${projects_dir}/seh.pathway_hopping"
       sim_dir="${project_dir}/simulations"
       lig_sim_dir="${sim_dir}/${lig_id}_simulations"
       input_dir="${lig_sim_dir}/input"
       lig_results_dir="${lig_sim_dir}/results"
       mkdir -p ${lig_results_dir}
       orch_dir="${lig_sim_dir}/orchs"
       mkdir -p "$orch_dir"

       # get just the job ids as an array
       nargs=$#
       let "njobargs = $nargs - 1"
       job_ids=("${@:2:$njobargs}")

       # then loop over them to get the paths to the orchestrators from
       # them
       echo ""
       echo "Getting Paths"
       orch_paths=()
       for i in ${!job_ids[@]}; do

           job_id=${job_ids[$i]}
           echo $job_id

           # get the directory for the job
           job_dir="${lig_sim_dir}/jobs/${job_id}"
           echo $job_dir

           # we need to rename them to the original folder that the
           # orchestrator is expecting. Instead of the name of the output
           # folder my job submission system renames it to. If this was
           # already done ignore these and their messages.
           output_dir="${job_dir}/output"

           # Note we run the localize_reporter_path.py script over
           # them with the localized paths we won't need to do
           # this. Localizing is the less confusing thing in the future
           # once we take on the burden of renaming paths in the
           # orchestrator

           # if ! (mv ${output_dir} ${job_dir}/exec); then echo "${jobid} already renamed"; fi
           # output_dir="${job_dir}/exec"


           # to get the paths to the orchestrators in the simulation
           # results dir we need to get the name of it which should be
           # the only dir
           sim_results_dir=$(ls -d ${output_dir}/*/)
           # remove trailing slash
           sim_results_dir=${sim_results_dir%/}

           echo "The results dir is: ${sim_results_dir}"

           # find the orchestrator the simulation produced (if at all)
           # this will either be a finished one or a checkpoint, and
           # either of those maybe patched or not. If a patched one exist
           # we preferentially choose that.
           orch=(${sim_results_dir}/*.orch)
           patched_orch=(${sim_results_dir}/*.orch.PATCH_V2)
           checkpoint_orch=(${sim_results_dir}/checkpoint.chk)
           patched_checkpoint_orch=(${sim_results_dir}/checkpoint.chk.PATCH_V2)

           # first priority is the patched run orch
           if [[ -e $patched_orch ]]; then
               echo "Found a patched orchestrator for $job_id"

               # if the job succeeded there will be a file generated which is
               # the orchestrator for the end of that simulation
               sim_orch_path=$patched_orch

           # second priority is the patched checkpoint
           elif [[ -e $patched_checkpoint_orch ]]; then
               echo "Found a patched checkpoint for $job_id"
               sim_orch_path=$patched_checkpoint_orch

           # third priority is an unpatched run orch
           elif [[ -e $orch ]]; then
               echo "Found an unpatched orchestrator for $job_id"
               sim_orch_path=$orch

           # final priority is an unpatched checkpoint
           elif [[ -e $checkpoint_orch ]]; then
               echo "Found an unpatched checkpoint for $job_id"
               sim_orch_path=$checkpoint_orch

           # and report if none was found just in case
           else
               echo "No orchestrator found for JOB ${job_id}";
               sim_orch_path="None"

           fi

           # and save it in the array of the orchestrator paths
           orch_paths[i]=${sim_orch_path}
       done

       echo ""
       echo "orchestrators being reconciled"
       for orch in ${orch_paths[@]}; do
           echo "$orch"
       done

       # backup the old orchestrator and hdf5
       cp "${input_dir}/sEH_lig-${lig_id}.orch" "${orch_dir}/sEH_lig-${lig_id}_BACKUP.orch"
       mv "${lig_results_dir}/all_results.wepy.h5" "${lig_results_dir}/all_results_BACKUP.wepy.h5"


       # do the reconciliation
       conda activate seh_pathway_hopping
       echo "Reconciling"

       python ${scripts_dir}/reconcile_v3.py \
            $lig_id \
            "${input_dir}/master-v3_sEH_lig-${lig_id}.orch.sqlite" \
            ${orch_paths[*]}

   }
 #+END_SRC



*** COMMENT special reconciliation


 #+BEGIN_SRC python :tangle hpcc/scripts/reconcile_v3.py
   import os
   import os.path as osp
   import itertools as it

   lig_3_runs = [
       # 0
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', 'de390f00dac09f52f97d82ca5b8789fa'),
       ('de390f00dac09f52f97d82ca5b8789fa', '2a2888da6c4733be0328b22eb4b7af26'),
       ('2a2888da6c4733be0328b22eb4b7af26', 'd1317b41436d5d9c48b79c1b500d759f'),
       ('d1317b41436d5d9c48b79c1b500d759f', '055e58cbe616fc271fb25d3daa78b471'),
       ('055e58cbe616fc271fb25d3daa78b471', '0cf1c1dbbdb8a8e91f0bfc5c090a0b74'),
       # 1
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', 'bb621a3fa5b36b7954a3d21745f203aa'),
       ('bb621a3fa5b36b7954a3d21745f203aa', '8e97b2d28e2afd49db6b2e1c18d48608'),
       # 2
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', '12f792ca1ec034cac0de7d9ff6517c91'),
       ('12f792ca1ec034cac0de7d9ff6517c91', '1dde5be5baf0faef2233201b68832245'),
       # 3
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', '9b027c1488717712905b00fe3e8cb7a4'),
       ('9b027c1488717712905b00fe3e8cb7a4', '1ff9228c6da5cd22713a9d8ec42351ad'),
       # 4
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', 'eaab5d019e061a229dd07e387a242ffc'),
       ('eaab5d019e061a229dd07e387a242ffc', '5756b809fe2f60269f46eff3407d76db'),
       # 5
       ('ed11f6caeabd40a7cf5d687a98f5a6b2', 'ff4b43ac2bb8c4277fbc0cb3b08496c4'),
       ('ff4b43ac2bb8c4277fbc0cb3b08496c4', '18dcfb9749be00a67e22f85105ee378d'),
   ]

   lig_10_runs = [
       # 0
       ('1b1ef610fddb4fd86e514eb1acd96a3f', '5ff88e59dff891973bd459224813a539'),
       ('5ff88e59dff891973bd459224813a539', '56ed345df9cfd1bef9c6ee646bcf2b5b'),
       ('56ed345df9cfd1bef9c6ee646bcf2b5b', 'a4bd4a55dd4fb99fe1ad86002911945f'),
       # 1
       ('1b1ef610fddb4fd86e514eb1acd96a3f', '5a9ca6bae5b48dad00ee452e896aa61b'),
       ('5a9ca6bae5b48dad00ee452e896aa61b', '7125793c770e23d596bb82973bf5a26b'),
       ('7125793c770e23d596bb82973bf5a26b', '2e6c8ad8f3bc58dbb935f1c91b32c345'),
       # 2
       ('1b1ef610fddb4fd86e514eb1acd96a3f', 'b2303d633641de4b6cb12eed548f87dd'),
       ('b2303d633641de4b6cb12eed548f87dd', 'f43da7bcb5ec077e1d71a58efb59b275'),
       ('f43da7bcb5ec077e1d71a58efb59b275', '5f0b05d01960da72b3c1a5a55bf408fd'),
       # 3
       ('1b1ef610fddb4fd86e514eb1acd96a3f', '0e081557e6e3082183a14c671f76ad86'),
       # 4
       ('1b1ef610fddb4fd86e514eb1acd96a3f', '34e64c97669757a13440f19b394622c1'),
       ('34e64c97669757a13440f19b394622c1', '342f7011e584f7a5db3428acb04b32ea'),
       ('342f7011e584f7a5db3428acb04b32ea', 'fa74ecc7bd2e1d8ba50bb4d566003742'),
       # 5
       ('1b1ef610fddb4fd86e514eb1acd96a3f', '4ecde27bd33391126d5706041b28e550'),
       ('4ecde27bd33391126d5706041b28e550', 'efd25ab7fe51c42551e754d9f6a593cd'),
   ]

   lig_18_runs = [

       # 0
       ('31f88cf33b945c8c4d732dac6944e0d6', '3ce746e9adf69469f6e0582aeae269fc'),
       ('3ce746e9adf69469f6e0582aeae269fc', 'bd8e6362dc7a4e61fc98502c5972180d'),
       ('bd8e6362dc7a4e61fc98502c5972180d', 'eba2781a1f49a59133144cedcffec35b'),
       ('eba2781a1f49a59133144cedcffec35b', 'dacddf49609e3cc47e29daca52caa5d8'),
       ('dacddf49609e3cc47e29daca52caa5d8', '4bc9e83f424237c023a0cc868face740'),
       # 1
       ('31f88cf33b945c8c4d732dac6944e0d6', '91bbd7a0cc82a270123004b7f9479c9d'),
       # 2
       ('31f88cf33b945c8c4d732dac6944e0d6', 'fd61ace5e31482a8c07fd86ec84a1480'),
       ('fd61ace5e31482a8c07fd86ec84a1480', '71dfce792cff9046ebd99e7f4e10a448'),
       ('71dfce792cff9046ebd99e7f4e10a448', '96b540d815a500374bd66d20fd7d18af'),
       # 3
       ('31f88cf33b945c8c4d732dac6944e0d6', 'f99b643595dffa5d54ba730506409fff'),
       ('f99b643595dffa5d54ba730506409fff', '07614b623256fa7c1e54aa99d2b4c2d3'),
       ('07614b623256fa7c1e54aa99d2b4c2d3', '23a311b9d9cc45f5df44b929916e566a'),
       # 4
       ('31f88cf33b945c8c4d732dac6944e0d6', '0a7fa88bd1e0ef72000281d761d2559a'),
       ('0a7fa88bd1e0ef72000281d761d2559a', '7646fd603713084c822060f9adcb011c'),
       ('7646fd603713084c822060f9adcb011c', 'b94290d18cf7c9282d7518a56fe6bd66'),
       # 5
       ('31f88cf33b945c8c4d732dac6944e0d6', '64c497938e30085ef905ca9d096da265'),
       ('64c497938e30085ef905ca9d096da265', 'bd192c5c4da3973076411f37ea1a044b'),
   ]


   lig_20_runs = [

       # 0
       ('243924f8b642a1b69e9ff83ef466e2d2', '18f87753e7646610f97b3ad801adde5b'),
       ('18f87753e7646610f97b3ad801adde5b', '3ed328662a6729de4828df4b6c25d915'),
       ('3ed328662a6729de4828df4b6c25d915', '79248ae08705a5e9dc524331d5b1879a'),
       ('79248ae08705a5e9dc524331d5b1879a', '77925f2d0870263ee41aad604ecaa285'),
       # 1
       ('243924f8b642a1b69e9ff83ef466e2d2', '6571e3f1d6100215c7711b1a69f06f6f'),
       ('6571e3f1d6100215c7711b1a69f06f6f', 'db0db3d06713accc3e5e3befb6aa0ac9'),
       ('db0db3d06713accc3e5e3befb6aa0ac9', '65c6fc950375b4de5be05c1a81fbe777'),
       # 2
       ('243924f8b642a1b69e9ff83ef466e2d2', 'c5682961b7fd63b2b1d66b7601d0b49e'),
       # 3
       ('243924f8b642a1b69e9ff83ef466e2d2', 'a1563856befefce7970e322cd2d8740c'),
       ('a1563856befefce7970e322cd2d8740c', 'fce412e170a916dd34938ef682fdad82'),
       ('fce412e170a916dd34938ef682fdad82', '4c381d53f1e8b432f11c7ed3595f9d5e'),
       # 4
       ('243924f8b642a1b69e9ff83ef466e2d2', 'b5ef16134e028f176fd11177ada8ec77'),
       # 5
       ('243924f8b642a1b69e9ff83ef466e2d2', '6a058bdf469dd0bde2d121de461f28ad'),
       ('6a058bdf469dd0bde2d121de461f28ad', 'bfcac3fcea48ad651ce7aa783c10b9ad'),
       ('bfcac3fcea48ad651ce7aa783c10b9ad', 'f6aa72c339471089ebf3829a103eab56'),

   ]


   runs = {'3' : lig_3_runs,
           '10' : lig_10_runs,
           '18' : lig_18_runs,
           '20' : lig_20_runs}

   # this is a mapping of a job name (and the corresponding orchestrator
   # file and the snapshots to get from it).
   lig_3_selections = [

       # lig 3
       ('2503029', ('ed11f6caeabd40a7cf5d687a98f5a6b2', 'de390f00dac09f52f97d82ca5b8789fa')),
       ('3075070', ('2a2888da6c4733be0328b22eb4b7af26',)),
       ('3722763', ('d1317b41436d5d9c48b79c1b500d759f',)),
       ('5940559', ('055e58cbe616fc271fb25d3daa78b471',)),
       ('5940557', ('bb621a3fa5b36b7954a3d21745f203aa',)),
       ('5940766', ('12f792ca1ec034cac0de7d9ff6517c91',)),
       ('5940966', ('9b027c1488717712905b00fe3e8cb7a4',)),
       ('5940967', ('eaab5d019e061a229dd07e387a242ffc',)),
       ('5940968', ('ff4b43ac2bb8c4277fbc0cb3b08496c4',)),
       ('13327704', ('0cf1c1dbbdb8a8e91f0bfc5c090a0b74',)),
       ('13326969', ('8e97b2d28e2afd49db6b2e1c18d48608',)),
       ('13326970', ('1dde5be5baf0faef2233201b68832245',)),
       ('13326971', ('1ff9228c6da5cd22713a9d8ec42351ad',)),
       ('13326972', ('5756b809fe2f60269f46eff3407d76db',)),
       ('13326973', ('18dcfb9749be00a67e22f85105ee378d',)),
   ]
   lig_10_selections = [
       # lig 10
       ('2577899', ('5ff88e59dff891973bd459224813a539', '1b1ef610fddb4fd86e514eb1acd96a3f')),
       ('5941385', ('56ed345df9cfd1bef9c6ee646bcf2b5b',)),
       ('5942786', ('5a9ca6bae5b48dad00ee452e896aa61b',)),
       ('5942788', ('b2303d633641de4b6cb12eed548f87dd',)),
       ('5942790', ('0e081557e6e3082183a14c671f76ad86',)),
       ('5942793', ('34e64c97669757a13440f19b394622c1',)),
       ('5942795', ('4ecde27bd33391126d5706041b28e550',)),
       ('13326974', ('a4bd4a55dd4fb99fe1ad86002911945f',)),
       ('13459588', ('7125793c770e23d596bb82973bf5a26b',)),
       ('13326976', ('f43da7bcb5ec077e1d71a58efb59b275',)),
       ('13326978', ('342f7011e584f7a5db3428acb04b32ea',)),
       ('13326979', ('efd25ab7fe51c42551e754d9f6a593cd',)),
       ('14786752', ('2e6c8ad8f3bc58dbb935f1c91b32c345',)),
       ('14786753', ('5f0b05d01960da72b3c1a5a55bf408fd',)),
       ('14786754', ('fa74ecc7bd2e1d8ba50bb4d566003742',)),
   ]
   lig_18_selections = [
       # lig 18
       ('2577900', ('3ce746e9adf69469f6e0582aeae269fc', '31f88cf33b945c8c4d732dac6944e0d6')),
       ('2965088', ('bd8e6362dc7a4e61fc98502c5972180d',)),
       ('5941434', ('eba2781a1f49a59133144cedcffec35b',)),
       ('5943360', ('91bbd7a0cc82a270123004b7f9479c9d',)),
       ('5943391', ('fd61ace5e31482a8c07fd86ec84a1480',)),
       ('5943392', ('f99b643595dffa5d54ba730506409fff',)),
       ('5943394', ('0a7fa88bd1e0ef72000281d761d2559a',)),
       ('5943400', ('64c497938e30085ef905ca9d096da265',)),
       ('13327296', ('dacddf49609e3cc47e29daca52caa5d8',)),
       ('13326981', ('71dfce792cff9046ebd99e7f4e10a448',)),
       ('13326982', ('07614b623256fa7c1e54aa99d2b4c2d3',)),
       ('13326983', ('7646fd603713084c822060f9adcb011c',)),
       ('14699913', ('bd192c5c4da3973076411f37ea1a044b',)),
       ('14786755', ('4bc9e83f424237c023a0cc868face740',)),
       ('14786756', ('96b540d815a500374bd66d20fd7d18af',)),
       ('14786757', ('23a311b9d9cc45f5df44b929916e566a',)),
       ('14786758', ('b94290d18cf7c9282d7518a56fe6bd66',)),
   ]

   lig_20_selections = [
       # lig 20
       ('2577901', ('18f87753e7646610f97b3ad801adde5b', '243924f8b642a1b69e9ff83ef466e2d2')),
       ('5942751', ('3ed328662a6729de4828df4b6c25d915',)),
       ('7630942', ('6571e3f1d6100215c7711b1a69f06f6f',)),
       ('5943436', ('c5682961b7fd63b2b1d66b7601d0b49e',)),
       ('5943537', ('a1563856befefce7970e322cd2d8740c',)),
       ('5943613', ('b5ef16134e028f176fd11177ada8ec77',)),
       ('5943737', ('6a058bdf469dd0bde2d121de461f28ad',)),
       ('13327455', ('79248ae08705a5e9dc524331d5b1879a',)),
       ('13326985', ('db0db3d06713accc3e5e3befb6aa0ac9',)),
       ('13326987', ('fce412e170a916dd34938ef682fdad82',)),
       ('13326988', ('17e8193ab5ecb9db4949a53389df5303',)),
       ('13326989', ('bfcac3fcea48ad651ce7aa783c10b9ad',)),
       ('14786759', ('77925f2d0870263ee41aad604ecaa285',)),
       ('14786760', ('65c6fc950375b4de5be05c1a81fbe777',)),
       ('14786761', ('4c381d53f1e8b432f11c7ed3595f9d5e',)),
       ('14786763', ('f6aa72c339471089ebf3829a103eab56',)),

   ]

   orch_snap_selections = {'3' : lig_3_selections,
                           '10' : lig_10_selections,
                           '18' : lig_18_selections,
                           '20' : lig_20_selections}



   if __name__ == '__main__':
       import sys
       from collections import defaultdict
       from wepy.orchestration.orchestrator import Orchestrator

       lig_id = sys.argv[1]
       output = sys.argv[2]
       orchestrators = sys.argv[3:]

       new_orch = Orchestrator(output, mode='w')

       # go through each orchestrator and use the data from above to get
       # what we need out of it and add it to the new orchestrator
       run_configs = defaultdict(list)
       run_cycles = {}
       for orch_path in orchestrators:
           print('reconciling orch {}'.format(orch_path))
           orch = Orchestrator(orch_path, mode='r')

           selections = dict(orch_snap_selections[lig_id])

           # we also need to know which job id this was. We know that it
           # is the name of the directory which is 3 above
           job_id = orch_path.split('/')[-4]

           # these are the hashes of the snapshots we want to get from it
           snapshot_hashes = selections[job_id]

           # add them to the new orchestrator
           for snaphash in snapshot_hashes:
                 new_orch.add_serial_snapshot(orch.snapshot_kv[snaphash])

           run_hashes = orch.run_hashes()

           # then we want to see if the desired run is in here and get
           # it's configuration

           # check if any of the end hashes in this orchs runs match the
           # specified runs and get the run data for them so we can
           # register runs later
           for start_hash, end_hash in run_hashes:

               # only match the end hash since that is the identifier for
               # the orch that produced a given run and will have the
               # correct config for it
               if end_hash in [end_hash for start_hash, end_hash in runs[lig_id]]:

                   # get the run_id from the specification
                   run_id_spec = [(s, e)
                                  for s, e in runs[lig_id]
                                  if e == end_hash][0]

                   # get the configuration
                   config = orch.run_configuration(start_hash, end_hash)

                   # save the configuration and last cycle idx for the
                   # specified run_id, not the run_id that is used to
                   # retrive it
                   run_configs[run_id_spec].append(config)
                   run_cycles[run_id_spec] = orch.run_last_cycle_idx(start_hash, end_hash)

           orch.close()

       # now we should have all of the snapshots that we need so register
       # the runs

       for run_id, cycle_idx, in run_cycles.items():

           # just add the first configuration we have
           config_hash = new_orch.add_configuration(run_configs[run_id][0])

           # then register the run
           new_orch.register_run(run_id[0], run_id[1], config_hash, cycle_idx)

       new_orch.close()
 #+END_SRC

*** COMMENT sqlite prototypes

 Prototyping the SQLite implementation of the orchestrator.

 #+BEGIN_SRC python :tangle hpcc/scripts/test_new_orch.py
   import os.path as osp
   import pickle
   from copy import deepcopy

   from wepy.orchestration.new_orchestrator import Orchestrator as NewOrchestrator
   from wepy.orchestration.new_orchestrator import reconcile_orchestrators

   from wepy.orchestration.orchestrator import Orchestrator


   from wepy.walker import WalkerState

   from setup_paths import tmp_dir

   orch_path = osp.join(tmp_dir, 'test.orch.sqlite')
   old_orch_path = osp.join(tmp_dir, 'old.orch')

   print("opening old orch")
   with open(old_orch_path, 'rb') as rf:
       old_orch = pickle.load(rf)

   # create a new database for the orch
   print("Creating new orch database")
   orch = NewOrchestrator(orch_path, mode='w')


   # then get stuff out of the old one and feed them into the new one for
   # testing

   print("setting metadata stuff")
   default_apparatus = old_orch.default_apparatus
   default_configuration = old_orch.default_configuration
   default_init_walkers = old_orch.default_init_walkers

   # these go in the metadata, which we set with dedicated methods
   orch.set_default_sim_apparatus(default_apparatus)
   orch.set_default_configuration(default_configuration)
   orch.set_default_init_walkers(default_init_walkers)

   print("generating the default snapshot")
   # generating a default starting snapshot
   default_snaphash = orch.gen_default_snapshot()

   # a snapshot for the snapshot kv
   snapshot = list(old_orch._snapshots.values())[0]

   # we want to make this one a little bit different so we can make a
   # fake run

   # Set the first atom position to the origin
   snapshot.walkers[0].state = WalkerState(**snapshot.walkers[0].state.dict())
   snapshot.walkers[0].state['positions'][0] = (0., 0., 0.)


   print("adding a snapshot")
   # adding snapshot
   snaphash = orch.add_snapshot(snapshot)

   # then add the run between the snapshots
   start_hash = orch.get_default_snapshot_hash()

   config_hash = orch.get_default_configuration_hash()

   print("registering a run")
   # register the run with a fake number of cycles
   orch.register_run(start_hash, snaphash, config_hash, 10)



   print("all of the runs:")
   print(orch.run_hashes())
   print(orch.run_last_cycle_idx(start_hash, snaphash))

   # then we need another database so that we can do a union between them
   other_orch_path = osp.join(tmp_dir, 'other.orch.sqlite')
   other_orch = NewOrchestrator(other_orch_path, mode='w')

   other_orch.set_default_sim_apparatus(default_apparatus)
   other_orch.set_default_configuration(default_configuration)
   other_orch.set_default_init_walkers(default_init_walkers)

   # lets make it a continuation so we will ad the last snapshot then
   # mutate it and add another one.
   o2_start_hash = other_orch.add_snapshot(snapshot)

   # Set the second atom position to the origin
   other_snapshot = deepcopy(snapshot)
   # need to get it as a state we can actually modify
   other_snapshot.walkers[0].state = WalkerState(**other_snapshot.walkers[0].state.dict())
   other_snapshot.walkers[0].state['positions'][1] = (0., 0., 0.)


   print("adding a snapshot")
   # adding snapshot
   o2_end_hash = other_orch.add_snapshot(other_snapshot)

   print("registering a run")
   # register the run with a fake number of cycles
   other_orch.register_run(o2_start_hash, o2_end_hash, config_hash, 10)

   new_orch = reconcile_orchestrators(orch_path, other_orch_path)

 #+END_SRC


 Then we want to do some stuff with these two databases but rerun all
 that.


 #+BEGIN_SRC python 

   # get a cursor to the original orchestrator
   c = orch._db.cursor()

   # run the attach command from the URI to the other orchestrator
   c.execute("ATTACH '{}' AS other_orch".format(other_orch.metadata_kv.db_uri))

   # get the rows from other orch that are not in this orch and update
   # them into the run table
   c.execute("""
   INSERT INTO runs
   SELECT * FROM
   (
   SELECT * FROM other_orch.runs
   EXCEPT
   SELECT * FROM runs
   )
   """)
 #+END_SRC


 #+BEGIN_SRC python :tangle hpcc/scripts/union_orchs.py
   import os.path as osp

   from sqlalchemy import create_engine
   import pandas as pd

   from setup_paths import tmp_dir

   from wepy.orchestration.new_orchestrator import reconcile_orchestrators
   from wepy.orchestration.new_orchestrator import Orchestrator


   orch_path = osp.join(tmp_dir, 'test.orch.sqlite')
   other_orch_path = osp.join(tmp_dir, 'other.orch.sqlite')

   # orch_sql_engine = create_engine("sqlite:///{}".format(orch_path))
   # orch_sql_conn = orch_sql_engine.connect()

   # before_df = pd.read_sql_table('runs', orch_sql_conn)

   new_orch = reconcile_orchestrators(orch_path, other_orch_path)

   # after_df = pd.read_sql_table('runs', orch_sql_conn)


   orch = Orchestrator(orch_path, mode='r')
   new_orch = Orchestrator(other_orch_path, mode='r')
 #+END_SRC

*** COMMENT individual commands
 Then some configuration for which ligand you are working on:
 #+BEGIN_SRC bash
 LIG_ID="3"
 #+END_SRC

 Get into the right environment and set some common paths.

 #+BEGIN_SRC bash
 PROJECTS_DIR="/mnt/home/lotzsamu/projects"
 PROJECT_DIR="${PROJECTS_DIR}/seh.pathway_hopping"
 SIM_DIR="${PROJECT_DIR}/simulations"
 LIG_SIM_DIR="${SIM_DIR}/${LIG_ID}_simulations"
 INPUT_DIR="${LIG_SIM_DIR}/input"
 LIG_RESULTS_DIR="${LIG_SIM_DIR}/results"
 mkdir -p ${LIG_RESULTS_DIR}
 #+END_SRC

 Now we want to choose which jobs we want to reconcile in a bash array:

 #+BEGIN_SRC bash
 JOB_IDS=(5940559 5940557)
 #+END_SRC

 #+BEGIN_SRC bash
 ORCH_PATHS=()
 for ((i=0; i<${#JOB_IDS[*]}; i++)); do
     jobid=${JOB_IDS[i]};
     job_dir="${LIG_SIM_DIR}/jobs/${LIG_ID}_simulations-${jobid}";
     #JOB_DIRS[i]=$job_dir;
     output_dir="${job_dir}/output"
     mv ${output_dir} ${job_dir}/exec
     output_dir="${job_dir}/exec"
     sim_results_dir=${output_dir}/*-${LIG_ID}
     sim_orch_path=${sim_results_dir}/*.orch
     ORCH_PATHS[i]=${sim_orch_path}
 done

 conda activate seh_pathway_hopping
 wepy reconcile --hdf5 "${LIG_RESULTS_DIR}/all_results_new.wepy.h5" \
               "${INPUT_DIR}/sEH_lig-${LIG_ID}.orch" \
               "${INPUT_DIR}/sEH_lig-${LIG_ID}.orch" ${ORCH_PATHS[*]}

 mv all_results.wepy.h5 all_results_old.wepy.h5
 mv all_results_new.wepy.h5 all_results.wepy.h5
 #+END_SRC


 Putting this all into a function:

*** COMMENT testing multi-worker single device work mapper

 I want to see if this gives you any benefit on GPUs.

 #+BEGIN_SRC bash

 #+END_SRC

*** COMMENT Simulation performance

 We want to know how increasing the nonbonded cutoffs from 8.5 to 10
 angstroms effects things.


 We wrote scripts to run each of these.

 #+BEGIN_SRC bash
 LIG_ID=3
 N_STEPS=10000
 echo "Running $N_STEPS steps with 0.85 nm cutoff"
 python nonbonded_cutoff_8-5.py "sEH_lig-${LIG_ID}_equilibrated.state.pkl" "sEH_lig-${LIG_ID}_system.top.json" "charmm36.xml" "charmm36_solvent.xml" "unl.xml" $N_STEPS
 echo "Running $N_STEPS steps with 0.10 nm cutoff"
 python nonbonded_cutoff_10.py "sEH_lig-${LIG_ID}_equilibrated.state.pkl" "sEH_lig-${LIG_ID}_system.top.json" "charmm36.xml" "charmm36_solvent.xml" "unl.xml" $N_STEPS
 #+END_SRC

 Results:

 #+BEGIN_EXAMPLE
 Using OpenCL platform
 Running 10000 steps with 0.85 nm cutoff
 Done running, took 47.83631730079651 s
 Running 10000 steps with 1.0 nm cutoff
 Done running, took 61.227272033691406 s
 #+END_EXAMPLE


 On HPCC:


 #+BEGIN_SRC bash
 LIG_ID=3
 N_STEPS=10000
 conda activate seh_pathway_hopping
 module load CUDA/9.2.88
 echo "Running $N_STEPS steps with 0.85 nm cutoff"
 python nonbonded_cutoff_8-5.py "sEH_lig-${LIG_ID}_equilibrated.state.pkl" "sEH_lig-${LIG_ID}_system.top.json" "charmm36.xml" "charmm36_solvent.xml" "unl.xml" $N_STEPS
 echo "Running $N_STEPS steps with 0.10 nm cutoff"
 python nonbonded_cutoff_10.py "sEH_lig-${LIG_ID}_equilibrated.state.pkl" "sEH_lig-${LIG_ID}_system.top.json" "charmm36.xml" "charmm36_solvent.xml" "unl.xml" $N_STEPS
 #+END_SRC


 #+BEGIN_EXAMPLE
 On Node lac-197
 Running 10000 steps with 0.85 nm cutoff
 Done running, took 31.307952165603638 s

 Running 10000 steps with 0.10 nm cutoff
 Done running, took 40.47777533531189 s
 #+END_EXAMPLE

 #+BEGIN_EXAMPLE
 on node lac-196
 Running 10000 steps with 0.85 nm cutoff
 Done running, took 31.02560544013977 s

 Running 10000 steps with 0.10 nm cutoff
 Done running, took 40.8801383972168 s
 #+END_EXAMPLE

*** COMMENT wepy orchestrator performance

 I want to see if the orchestrator stuff makes wepy run slower or if it
 is the node on HPCC or somehting.

 So I will do a speed test that can even be run on my local machine.

 First setup:

 #+BEGIN_SRC bash
 N_CYCLES=5
 N_STEPS=10000
 RESNAME="UNL"
 DATA_DIR="/home/salotz/tree/lab/projects/seh.pathway_hopping/data"
 PERF_DIR="${DATA_DIR}/perftest"
 NO_ORCH_DIR="${PERF_DIR}/no-orch"
 ORCH_DIR="${PERF_DIR}/orch"
 rm -rf ${NO_ORCH_DIR}
 rm -rf ${ORCH_DIR}
 cd ${PERF_DIR}
 conda activate seh_pathway_hopping
 #+END_SRC


 Then run the no orch test. Arguments are:
 init_state_path, json_top_path, resname, prot_ff_path, solvent_ff_path, lig_ff_path, \
 ligand_id, n_cycles, n_steps, n_workers

 #+BEGIN_SRC bash
 LIG_ID="3"
 MDSYS_DIR="${DATA_DIR}/md_systems/${LIG_ID}"
 INIT_STATE="${MDSYS_DIR}/sEH_lig-${LIG_ID}_equilibrated.state.pkl"
 JSON_TOP_PATH="${MDSYS_DIR}/sEH_lig-${LIG_ID}_system.top.json"
 PROT_FF_PATH="${MDSYS_DIR}/charmm36.xml"
 SOLVENT_FF_PATH="${MDSYS_DIR}/charmm36_solvent.xml"
 LIG_FF_PATH="${MDSYS_DIR}/unl.xml"
 N_WORKERS=1
 rm -rf ${NO_ORCH_DIR}
 mkdir -p ${NO_ORCH_DIR}
 cd ${NO_ORCH_DIR}
 python -m seh_prep.run_without_orch ${INIT_STATE} \
                                     ${JSON_TOP_PATH} \
                                     ${RESNAME} \
                                     ${PROT_FF_PATH} ${SOLVENT_FF_PATH} ${LIG_FF_PATH}\
                                     ${LIG_ID} \
                                     ${N_CYCLES} ${N_STEPS} \
                                     ${N_WORKERS}
 #+END_SRC



 On HPCC:

 #+BEGIN_SRC bash
 LIG_ID="3"
 N_CYCLES=5
 N_STEPS=10000
 RESNAME="UNL"
 INIT_STATE="sEH_lig-${LIG_ID}_equilibrated.state.pkl"
 JSON_TOP_PATH="sEH_lig-${LIG_ID}_system.top.json"
 PROT_FF_PATH="charmm36.xml"
 SOLVENT_FF_PATH="charmm36_solvent.xml"
 LIG_FF_PATH="unl.xml"
 N_WORKERS=1
 python run_without_orch.py ${INIT_STATE} \
                                     ${JSON_TOP_PATH} \
                                     ${RESNAME} \
                                     ${PROT_FF_PATH} ${SOLVENT_FF_PATH} ${LIG_FF_PATH}\
                                     ${LIG_ID} \
                                     ${N_CYCLES} ${N_STEPS} \
                                     ${N_WORKERS}
 #+END_SRC


 And with an orchestrator:


 2x1 Multi-Worker Single Device test:

 #+BEGIN_SRC bash
 N_STEPS=10000
 RUN_TIME=7200
 N_DEVICES=1
 N_WORKERS=2
 START_HASH="3a3eed6bb613a042d089547bcab8b1e6"
 TEST_RUN_NAME="MWSD-2x1_test"
 CONFIG_PATH="OpenMMGPUWorker_MWSD-2x8-workers_lig-3.config.dill.pkl"
 ORCH_PATH="/mnt/home/lotzsamu/projects/seh.pathway_hopping/root_orchs/sEH_lig-3.orch"
 rm -rf ./${TEST_RUN_NAME}/*
 mkdir -p ./${TEST_RUN_NAME}
 conda activate seh_pathway_hopping
 python -m wepy.orchestration.cli run --n-devices ${N_DEVICES} --n-workers ${N_WORKERS} \
         --job-name ${TEST_RUN_NAME} --configuration ${CONFIG_PATH} --log INFO ${ORCH_PATH} \
         $START_HASH $RUN_TIME $N_STEPS
 #+END_SRC


 1x1 Multi-Worker Single Device test:

 #+BEGIN_SRC bash
 cd /mnt/home/lotzsamu/projects/seh.pathway_hopping/test_run
 N_STEPS=10000
 RUN_TIME=7200
 N_DEVICES=1
 N_WORKERS=1
 START_HASH="3a3eed6bb613a042d089547bcab8b1e6"
 TEST_RUN_NAME="MWSD-1x1_test"
 CONFIG_PATH="OpenMMGPUWorker_MWSD-2x8-workers_lig-3.config.dill.pkl"
 ORCH_PATH="/mnt/home/lotzsamu/projects/seh.pathway_hopping/root_orchs/sEH_lig-3.orch"
 rm -rf ./${TEST_RUN_NAME}/*
 mkdir -p ./${TEST_RUN_NAME}
 conda activate seh_pathway_hopping
 python -m wepy.orchestration.cli run --n-devices ${N_DEVICES} --n-workers ${N_WORKERS} \
         --job-name ${TEST_RUN_NAME} --configuration ${CONFIG_PATH} --log INFO ${ORCH_PATH} \
         $START_HASH $RUN_TIME $N_STEPS
 #+END_SRC

*** COMMENT Testing the n_workers issue

 #+BEGIN_SRC bash
 N_STEPS=10000
 RUN_TIME=7200
 START_HASH="3a3eed6bb613a042d089547bcab8b1e6"
 TEST_RUN_NAME="pilot-run"
 rm -rf ./${TEST_RUN_NAME}/*
 mkdir -p ./${TEST_RUN_NAME}
 conda activate seh_pathway_hopping
 wepy run --job-name ${TEST_RUN_NAME} --n-workers 8 --log INFO root.orch $START_HASH $RUN_TIME $N_STEPS
 #+END_SRC


*** COMMENT PSF stuff
 Load a PSF in openmm for investiagation

 #+BEGIN_SRC python :tangle hpcc/scripts/test_omm_top.py
   import sys
   from copy import copy, deepcopy

   import pandas as pd

   import parmed as pmd
   from parmed.charmm import CharmmParameterSet, CharmmPsfFile

   import simtk.openmm.app as omma
   import simtk.openmm as omm
   import simtk.unit as unit

   pdb_path = sys.argv[1]
   psf_path = sys.argv[2]
   pdb_output_path = sys.argv[3]
   psf_output_path = sys.argv[4]
   csv_path = sys.argv[5]

   prot_rtf = "top_all36_prot.rtf"
   prot_prm = "par_all36m_prot.prm"

   cgenff_rtf = "top_all36_cgenff.rtf"
   cgenff_prm = "par_all36_cgenff.prm"

   solvent_str = "toppar_water_ions.str"

   lig_rtf = "unl.rtf"
   lig_prm = "unl.prm"

   # load the parameter set
   params = omma.CharmmParameterSet(prot_rtf, prot_prm,
                                    cgenff_rtf, cgenff_prm,
                                    solvent_str,
                                    lig_rtf, lig_prm)

   # load the PSF
   # load the charmm file for the topology
   psf = omma.CharmmPsfFile(psf_path)

 #+END_SRC


 For writing out psfs

 #+BEGIN_SRC 
 # This stuff was for generating PSFs from the rearranged data

 # # now that we have a table that accurately describes our atoms, we
 # # generate a structure from them
 # struct = pmd.Structure()

 # # fill it with atoms
 # residue_idx = 0
 # last_row = None
 # for row_idx, row in grouped_df.iterrows():

 #     # figure out the residue idx

 #     # we look at the last row and compare it to this one

 #     # if its None then this is the first row.
 #     if last_row is None:
 #         pass

 #     # otherwise we determine if we are into a new residue now
 #     else:

 #         # if there is a difference in the resname then we have a
 #         # different residue for sure
 #         if row['resname'] != last_row['resname']:
 #             residue_idx += 1

 #         # a different resname does not always mean they aren't
 #         # different residues since you could have the same name twice
 #         # in a row, so if the resid is different then we have the next
 #         # residue
 #         elif row['resid'] != last_row['resid']:
 #             residue_idx += 1

 #         # otherwise these two atoms are in the same residue
 #         else:
 #             pass

 #     # now we create the atom
 #     atom = pmd.Atom(number=row_idx,
 #                     atomic_number=row['atomic_number'],
 #                     name=row['name'],
 #                     type=row['type'],
 #                     charge=row['charge'],
 #                     mass=row['mass'],
 #     )

 #     # add the coordinates
 #     atom.xx = row['xx']
 #     atom.xy = row['xy']
 #     atom.xz = row['xz']

 #     # now we add it to the structure, always set the chain to nothing
 #     # so we can have more than 10000 residues
 #     struct.add_atom(atom, row['resname'], residue_idx,
 #                     chain='',
 #                     segid=row['segid']
 #     )


 #     last_row = row

 # struct.symmetry = None


 # # write a new PDB and PSF for this structure
 # struct.save(psf_output_path, overwrite=True)

 # struct.save(pdb_output_path, charmm=True,
 #             overwrite=True)



 # # for the PDB parmed writes HETATM records for the HSD
 # # residues... lets do a quick change of all HETATM records to ATOM
 # # records
 # with open(pdb_output_path, 'r') as pdb_f:
 #     pdb_str = pdb_f.read()
 #     pdb_str = pdb_str.replace('HETATM', 'ATOM  ')

 # with open(pdb_output_path, 'w') as pdb_f:
 #     pdb_f.write(pdb_str)

 #+END_SRC
**** Generate openmm system from system CSVs

 #+BEGIN_SRC python :tangle hpcc/scripts/gen_sys_from_csvs.py

   import sys

   import simtk.openmm.app as omma
   import simtk.openmm as omm

   atom_csv_path = sys.argv[1]
   bond_csv_path = sys.argv[2]
   angle_csv_path = sys.argv[3]
   dihedral_csv_path = sys.argv[4]
   improper_csv_path = sys.argv[5]
   cmap_csv_path = sys.argv[6]

   # now make an OpenMM system manually

   system = omm.System()

   # add all the particles
   particle_idxs = []
   for row_idx, row in atom_df.iterrows():

       particle_idx = system.addParticle(row['mass'])
       particle_idxs.append(particle_idx)
 #+END_SRC

 making parameter tables
 #+BEGIN_SRC python
 angle_table = []
 angle_columns = ['atom_1', 'atom_2', 'atom_3',
                  'force_constant', 'equilibrium_distance']
 for angle in omm_psf.angle_list:
     row = (angle.atom1.idx, angle.atom2.idx, angle.atom3.idx,
            angle.angle_type.k, angle.angle_type.theteq)
     angle_table.append(row)

 angle_df = pd.DataFrame(angle_table, columns=angle_columns)

 # urey-bradley terms are not present but this is a reminder to check
 # if you are copying this script
 # UB_table = []
 # for UB in omm_psf.UreyBradley:
 #     atom_idxs = (UB.atom1.idx, UB.atom2.idx, UB.atom3.idx)
 #     UB_table.append(atom_idxs)

 dihedral_table = []
 dihedral_columns = ['atom_1', 'atom_2', 'atom_3', 'atom_4',
                     'force_constant', 'periodicity', 'phase']
 for dihedral in omm_psf.dihedral_parameter_list:
     row = (dihedral.atom1.idx, dihedral.atom2.idx,
                  dihedral.atom3.idx, dihedral.atom4.idx,
                  dihedral.dihedral_type.phi_k,
                  dihedral.dihedral_type.per,
                  dihedral.dihedral_type.phase)

     dihedral_table.append(row)

 dihedral_df = pd.DataFrame(dihedral_table, columns=dihedral_columns)


 improper_table = []
 improper_columns = ['atom_1', 'atom_2', 'atom_3', 'atom_4',
                     'force_constant', 'equilibrium_torsion_angle']
 for improper in omm_psf.improper_list:
     row = (improper.atom1.idx, improper.atom2.idx,
                  improper.atom3.idx, improper.atom4.idx,
                  improper.improper_type.k,
                  improper.improper_type.phieq)
     improper_table.append(row)

 improper_df = pd.DataFrame(improper_table, columns=improper_columns)


 # cmaps are a little different...  they contain a single resolution
 # parameter (we will call R) plus a grid of values that is R x R so we
 # make a separate column for each term (i,j) 0 >= i,j >= R
 cmap_table = []
 # so get the resolution and generate the columns for it
 resolution = omm_psf.cmap_list[0].cmap_type.resolution
 grid_term_idxs = list(it.product(range(resolution), range(resolution)))
 grid_term_idxs.sort()
 # then stringify them
 grid_term_names = ["{}-{}".format(i,j) for i,j in grid_term_idxs]
 # make the columns
 cmap_columns = ['atom_1', 'atom_2', 'atom_3', 'atom_4', 'atom_5',
                 'resolution'] + grid_term_names
 for cmap in omm_psf.cmap_list:
     grid_terms = [cmap.cmap_type.grid[i,j] for i,j in grid_term_idxs]
     row = [cmap.atom1.idx, cmap.atom2.idx,
            cmap.atom3.idx, cmap.atom4.idx,
            cmap.atom5.idx,
            cmap.cmap_type.resolution] + grid_terms

     cmap_table.append(row)

 cmap_df = pd.DataFrame(cmap_table, columns=cmap_columns)

 #+END_SRC


*** COMMENT Debugging warping records in contigs

 I have found that when you make contigs from the contig tree of runs
 that have good warping events they come out wrong.

 Using data from ligand 3 we can show this.

 After running this:


 #+BEGIN_SRC python :tangle hpcc/scripts/spanning_contigs.py
   from load_results import *

   from pprint import pprint

   lig_spans_data = {}
   for lig_id, lig_contigtree in ((3, lig3_contigtree), (10, lig10_contigtree),
                          (18, lig18_contigtree), (20, lig20_contigtree)):
       lig_span_traces = lig_contigtree.spanning_contig_traces()


       spans_data = {}
       num_cycles = []
       for span_idx, span_trace in enumerate(lig_span_traces):

           span_data = {'warping_df' : None, 'parent_table' : None,
                        'n_cycles' : None}

           span_data['n_cycles'] = len(span_trace)
           num_cycles.append(len(span_trace))
           print("Ligand {} span {}, num cycles: {}".format(lig_id, span_idx, len(span_trace)))

           # generate the contig
           contig = lig_contigtree.make_contig(span_trace)

           # get the warping records
           span_data['warping_df'] = contig.warping_records_dataframe()

           # the parent table
           span_data['parent_table'] = contig.parent_table()

           spans_data[span_idx] = span_data

           lig_spans_data[lig_id] = spans_data

       print("TOTAL Ligand {} num cycles: {}".format(lig_id,
                                                     sum(num_cycles)))



 #+END_SRC

 #+RESULTS:


 We note that span 2 has the only warping records of which there are
 ten:

 #+BEGIN_SRC python
 spans_data[2]['warping_df']
 #+END_SRC


 To prove to ourselves that it is the contigs and not the runs data
 itself we will display the runs data.

 #+BEGIN_SRC python
 lig3_wepy_h5.warping_records_dataframe([0,1,2,3,4,5,6,7,8])
 #+END_SRC

 #+BEGIN_EXAMPLE
     cycle_idx  walker_idx  target_idx        weight
 0         288          22           0  6.945904e-10
 1         293          23           0  5.904018e-10
 2         294          47           0  2.952009e-10
 3         295          22           0  1.476005e-10
 4         296          23           0  1.736476e-10
 5         305          23           0  7.380023e-11
 6         311          32           0  2.952009e-10
 7         312           6           0  8.682379e-11
 8         313           6           0  4.341190e-11
 9         316          32           0  4.341190e-11
 10        330           6           0  1.476005e-10
 11        333           4           0  8.682379e-11
 12        333           6           0  8.682379e-11
 13        338           4           0  3.690011e-11
 14        351          23           0  3.690011e-11
 15        381           6           0  9.225028e-12
 16        395          12           0  7.380023e-11
 17        421           4           0  9.225028e-12
 18        429          41           0  1.153129e-12
 19        440          41           0  1.180804e-09
 20        442          12           0  4.612514e-12
 21        442          21           0  1.153129e-12
 22        445          41           0  2.306257e-12
 23        452          23           0  1.153129e-12
 24        470          40           0  1.153129e-12
 25        501          30           0  1.302357e-10
 26        539          31           0  3.704482e-10
 27        541          12           0  7.408964e-10
 28        545          18           0  3.704482e-10
 29        611          32           0  1.476005e-10
 30        637          16           0  1.356622e-12
 31        663          20           0  1.356622e-12
 32       1742           9           0  1.634631e-12
 33       1831          27           0  1.418872e-12
 34       1845          30           0  1.418872e-12
 35       1885          35           0  1.735282e-12
 36       1908           7           0  1.481680e-12
 37       1998          35           0  1.407681e-12
 38       2790          46           0  1.147732e-12
 39       2810          40           0  1.183598e-11
 40       3583          26           0  6.640784e-09
 41       3793          15           0  1.556434e-10
 42       3800          42           0  4.150490e-10
 43       3840          23           0  2.075245e-10
 44       3846          42           0  2.075245e-10
 45       3883          45           0  4.150490e-10
 46       3965          33           0  1.407681e-12
 47       4196          18           0  1.407681e-12
 48       4224          19           0  1.087736e-12
 49       4396          24           0  1.543805e-12
 50       4575          23           0  1.549029e-12
 51       5255          28           0  1.700333e-12
 52       5288           4           0  1.700333e-12
 #+END_EXAMPLE


 So now where is it going wrong?

 Lets circumvent the contigtree and make a contig directly from the
 runs which are supposed to go together.

 #+BEGIN_SRC python
 lig3_wepy_h5.continuations
 #+END_SRC

 #+BEGIN_EXAMPLE
 array([[2, 5],
        [5, 4],
        [7, 2]])
 #+END_EXAMPLE

 We know that only one contig is joined by multiple runs and they are:
 7, 2, 5, 4

 which corresponds to the contig which we had multiple runs for:

 - root :: 3a3eed6bb613a042d089547bcab8b1e6
   - 0-0 :: 1b0a5c2dc8d36ab234935a7c6b3cfc44
     - 0-1 :: 608107bdef0980187daa7c66c125e854
     - 0-2 :: 8cd97e8583f682845106ff1456916318
     - 0-3 :: b6119f14a8656e9b7099865a6bf443bd

 So really the contig tree shouldn't really have any problem with the
 other runs since they are separate subtrees completely anyhow...
 0, 1, 3, 5, 8

 Lets focus on run 0 since it has exit points.

 #+BEGIN_SRC python
 lig3_wepy_h5.warping_records_dataframe([0])
 #+END_SRC

 #+BEGIN_EXAMPLE
     cycle_idx  walker_idx  target_idx        weight
 0         288          22           0  6.945904e-10
 1         293          23           0  5.904018e-10
 2         294          47           0  2.952009e-10
 3         295          22           0  1.476005e-10
 4         296          23           0  1.736476e-10
 5         305          23           0  7.380023e-11
 6         311          32           0  2.952009e-10
 7         312           6           0  8.682379e-11
 8         313           6           0  4.341190e-11
 9         316          32           0  4.341190e-11
 10        330           6           0  1.476005e-10
 11        333           4           0  8.682379e-11
 12        333           6           0  8.682379e-11
 13        338           4           0  3.690011e-11
 14        351          23           0  3.690011e-11
 15        381           6           0  9.225028e-12
 16        395          12           0  7.380023e-11
 17        421           4           0  9.225028e-12
 18        429          41           0  1.153129e-12
 19        440          41           0  1.180804e-09
 20        442          12           0  4.612514e-12
 21        442          21           0  1.153129e-12
 22        445          41           0  2.306257e-12
 23        452          23           0  1.153129e-12
 24        470          40           0  1.153129e-12
 25        501          30           0  1.302357e-10
 26        539          31           0  3.704482e-10
 27        541          12           0  7.408964e-10
 28        545          18           0  3.704482e-10
 29        611          32           0  1.476005e-10
 30        637          16           0  1.356622e-12
 31        663          20           0  1.356622e-12
 #+END_EXAMPLE

 Maybe the problem is with the traces.

 I looked at the traces and they have the right number of cycles in
 them:

 #+BEGIN_EXAMPLE
 Ligand 3 span 0, num cycles: 688
 Ligand 3 span 1, num cycles: 739
 Ligand 3 span 2, num cycles: 1758
 Ligand 3 span 3, num cycles: 714
 Ligand 3 span 4, num cycles: 693
 Ligand 3 span 5, num cycles: 725
 TOTAL Ligand 3 num cycles: 5317
 Ligand 10 span 0, num cycles: 957
 Ligand 10 span 1, num cycles: 668
 Ligand 10 span 2, num cycles: 699
 Ligand 10 span 3, num cycles: 645
 Ligand 10 span 4, num cycles: 725
 Ligand 10 span 5, num cycles: 727
 TOTAL Ligand 10 num cycles: 4421
 Ligand 18 span 0, num cycles: 295
 Ligand 18 span 1, num cycles: 727
 Ligand 18 span 2, num cycles: 404
 Ligand 18 span 3, num cycles: 733
 Ligand 18 span 4, num cycles: 738
 Ligand 18 span 5, num cycles: 11
 TOTAL Ligand 18 num cycles: 2908
 Ligand 20 span 0, num cycles: 747
 Ligand 20 span 1, num cycles: 743
 Ligand 20 span 2, num cycles: 710
 Ligand 20 span 3, num cycles: 978
 Ligand 20 span 4, num cycles: 724
 TOTAL Ligand 20 num cycles: 3902
 #+END_EXAMPLE

 Okay so it is not that. Lets just look at the code and see what it
 happening.


 #+BEGIN_SRC python :tangle hpcc/scripts/debug_contig_warps.py
   from load_results import *
   import sys
   sys.setrecursionlimit(5000)
   from pprint import pprint

   span_trace = lig3_contigtree.spanning_contig_traces()[0]

   contig = lig3_contigtree.make_contig(span_trace)
 #+END_SRC

*** COMMENT Testing ligand RMSD observable function

 #+BEGIN_SRC python :tangle hpcc/scripts/test_ligrmsd_obs_func.py
   import numpy as np

   from load_results import lig3_wepy_h5

   from observable_ligand_rmsd import lig_bs_rmsd_observable

   from selection_idxs import lig_selection_idxs
   from load_ref_states import ref_states

   with lig3_wepy_h5:

       ligand_idxs = lig_selection_idxs[3]['main_rep/ligand']
       receptor_idxs = lig_selection_idxs[3]['main_rep/protein']
       bs_idxs = lig_selection_idxs[3]['main_rep/binding_site']

       main_rep_idxs = np.concatenate((ligand_idxs, receptor_idxs))

       ref_state = ref_states[3].dict()
       ref_state['positions'] = ref_state['positions'][main_rep_idxs]
       lig_rmsds = lig3_wepy_h5.compute_observable(lig_bs_rmsd_observable,
                                                   ['positions', 'box_vectors'],
                                                   (ligand_idxs, receptor_idxs, bs_idxs, ref_state['positions']),
                                                   traj_sel=[(0,0)])
 #+END_SRC


*** COMMENT Testing recentering and superimpose

 #+BEGIN_SRC python :tangle hpcc/scripts/test_center_superimpose.py
   import os.path as osp

   from copy import deepcopy

   import numpy as np

   from wepy.util.mdtraj import traj_fields_to_mdtraj
   from wepy.util.util import traj_box_vectors_to_lengths_angles, box_vectors_to_lengths_angles

   from geomm.superimpose import superimpose
   from geomm.grouping import group_pair
   from geomm.centering import center_around

   from setup_paths import tmp_dir
   from load_results import lig3_contigtree
   from selection_idxs import lig_selection_idxs
   from load_ref_states import ref_states

   wep = lig3_contigtree.wepy_h5

   # write a trajectory out from the first 10 frames
   traj_fields = {'positions' : wep.traj(6,0)['positions'][0:10],
                  'box_vectors' : wep.traj(6,0)['box_vectors'][0:10]}

   json_top = wep.get_topology()

   traj = traj_fields_to_mdtraj(traj_fields, json_top)

   test_path = osp.join(tmp_dir, "traj_06_00.dcd")

   traj.save_dcd(test_path)


   # then regroup the complex

   lig_id = 3
   rep_key = "main_rep"

   sel_idxs = lig_selection_idxs[lig_id]

   all_atom_rep_idxs = sel_idxs['all_atoms/{}'.format(rep_key)]

   lig_idxs = sel_idxs['{}/ligand'.format(rep_key)]
   prot_idxs = sel_idxs['{}/protein'.format(rep_key)]
   bs_idxs = sel_idxs['{}/binding_site'.format(rep_key)]

   # the correct reference state for this ligand
   ref_state = ref_states[lig_id]

   ref_box_lengths, _ = box_vectors_to_lengths_angles(ref_state['box_vectors'])

   # get the reference positions as a slice of the full reference
   # state positions
   ref_positions = ref_state['positions'][all_atom_rep_idxs]

   grouped_ref_positions = group_pair(ref_positions, ref_box_lengths,
                                      prot_idxs, lig_idxs)
   centered_ref_positions = center_around(grouped_ref_positions, bs_idxs)

   # write out the grouped and centered ref positions
   ref_traj_fields = {'positions' : np.array([centered_ref_positions]),
                      'box_vectors' : np.array([ref_state['box_vectors']])}
   ref_traj = traj_fields_to_mdtraj(ref_traj_fields, json_top)
   ref_path = osp.join(tmp_dir, "main_rep_ref_GROUPED_CENTERED.pdb")
   ref_traj.save_pdb(ref_path)


   box_lengths, _ = traj_box_vectors_to_lengths_angles(traj_fields['box_vectors'])


   # grouped positions
   grouped_positions = [group_pair(positions, box_lengths[idx],
                                       prot_idxs, lig_idxs)
                 for idx, positions in enumerate(traj_fields['positions'])]

   traj_fields['grouped_positions'] = np.array(grouped_positions)
   grouped_traj = traj_fields_to_mdtraj(traj_fields, json_top, rep_key="grouped_positions")

   grouped_path = osp.join(tmp_dir, "traj_06_00_GROUPED.dcd")

   grouped_traj.save_dcd(grouped_path)


   # then recentered positions
   centered_positions = [center_around(positions, bs_idxs)
                 for idx, positions in enumerate(traj_fields['grouped_positions'])]

   traj_fields['centered_positions'] = np.array(centered_positions)
   centered_traj = traj_fields_to_mdtraj(traj_fields, json_top, rep_key='centered_positions')

   centered_path = osp.join(tmp_dir, "traj_06_00_CENTERED.dcd")

   centered_traj.save_dcd(centered_path)


   # superimposed positions
   sup_positions = [superimpose(centered_ref_positions, positions, idxs=bs_idxs)[0]
                    for idx, positions in enumerate(traj_fields['centered_positions'])]

   traj_fields['sup_positions'] = np.array(sup_positions)
   sup_traj = traj_fields_to_mdtraj(traj_fields, json_top, rep_key='sup_positions')

   sup_path = osp.join(tmp_dir, "traj_06_00_SUP.dcd")

   sup_traj.save_dcd(sup_path)
 #+END_SRC

*** COMMENT Testing lig rmsd fe profiles

 #+BEGIN_SRC python :tangle hpcc/scripts/test_ligrmsd_fe_profiles.py
   import numpy as np
   import matplotlib.pyplot as plt

   from load_results import lig3_contigtree
   from func_fe_convergence import cumulative_partitions, free_energy_profile

   span_trace = lig3_contigtree.spanning_contig_traces()[0]
   contig = lig3_contigtree.make_contig(span_trace)

   weights = contig.contig_fields(['weights'])['weights']
   weights = weights.reshape((weights.shape[0], weights.shape[1]))
   lig_rmsds = contig.contig_fields(['observables/lig_rmsd'])['observables/lig_rmsd']

   print("Num frames: {}".format(weights.shape[0]))

   # determine what the binning should be ahead of time based on all of
   # the data so they will be comparable. This should be parametrized
   # based upon a desired number of bins and then the largest values from
   # all data you want to compare
   num_bins = 100

   # for all of the data we would pool it all here
   bin_edges = np.histogram_bin_edges(lig_rmsds, bins=100)

   # get a generator of the cumulative partitions of the data
   weights_partition_gen = cumulative_partitions(weights, time_tranche=250)
   lig_rmsds_partition_gen = cumulative_partitions(lig_rmsds, time_tranche=250)

   fe_profiles = []
   for weights_part in weights_partition_gen:
       lig_rmsds_part = next(lig_rmsds_partition_gen)

       print("weights", weights_part.shape)
       print("ligand RMSDs", lig_rmsds_part.shape)

       fe_profile, _ = free_energy_profile(weights_part, lig_rmsds_part,
                                                   bins=bin_edges)

       fe_profiles.append(fe_profile)

   # then make the plots

   fig, ax = plt.subplots(1)
   for curve_idx, fe_profile in enumerate(fe_profiles):

       bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                               for i in range(bin_edges.shape[0] - 1)])

       # if this is the last curve make the label show the true number of
       # cycles as the upper limit

       # TODO get the right chunk widths here
       CHUNK_WIDTH = 250
       label = "Cycles: {}-{}".format(0, (curve_idx * CHUNK_WIDTH) +  CHUNK_WIDTH)

       plt.plot(bin_centers, fe_profile,
                    label=label)
       plt.xlabel("ligand RMSD ($\AA$)")
       plt.ylabel("Free Energy ($-ln(p)$)")

   plt.legend()

   plt.show()

 #+END_SRC
*** COMMENT testing lig rmsd fe profiles wepy API


 Try out different binning methods.

 #+BEGIN_SRC python :tangle hpcc/scripts/test_fe_bin_methods.py
   from load_results import lig3_contigtree

   import matplotlib.pyplot as plt
   import numpy as np

   from wepy.analysis.profiles import ContigTreeProfiler

   profiler = ContigTreeProfiler(lig3_contigtree)

   bin_methods = ('auto', 'fd', 'doane', 'scott', 'stone', 'rice', 'sturges', 'sqrt')
   field_key = 'observables/lig_rmsd'
   span_idx = 0
   lig_id = 3

   for bin_method in bin_methods:
       # get the bin edges for the whole contig
       bin_edges = profiler.bin_edges(bin_method, field_key)

       # then make a fe profile with those bins for the first span
       fe_profile = profiler.fe_profile(0, field_key, bins=bin_edges)

       # get the maximum (non infinite) value from the profile
       max_fe = np.ma.masked_invalid(fe_profile).max()

       fig, ax = plt.subplots(1)
       bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                               for i in range(bin_edges.shape[0] - 1)])

       # if this is the last curve make the label show the true number of
       # cycles as the upper limit

       # TODO get the right chunk widths here
       #label = "Cycles: {}-{}".format(0, (curve_idx * time_tranche) +  time_tranche)

       plt.plot(bin_centers, fe_profile)
       plt.xlim(left=bin_edges[0], right=bin_edges[-1])
       plt.ylim(bottom=0.0, top=max_fe+5)
       plt.title("FE Profile Observable for Ligand {} Span {}, bin method {}".format(
           lig_id, span_idx, bin_method))
       plt.xlabel("ligand RMSDs ($\AA$)")
       plt.ylabel("Free Energy ($-ln(p)$)")

       plt.legend()

       plt.show()
 #+END_SRC

 Then test the methods using only a single binning method.

 #+BEGIN_SRC python :tangle hpcc/scripts/test_fe_profile_api.py
   from load_results import lig3_contigtree

   import matplotlib.pyplot as plt
   import numpy as np

   from wepy.analysis.profiles import ContigTreeProfiler

   profiler = ContigTreeProfiler(lig3_contigtree)

   bin_methods = ('auto', 'fd', 'doane', 'scott', 'stone', 'rice', 'sturges', 'sqrt')
   bin_method = 'sqrt'
   field_key = 'observables/lig_rmsd'
   span_idx = 0
   lig_id = 3


   # get the bin edges for the whole contig
   bin_edges = profiler.bin_edges(bin_method, field_key)

   # then make a fe profile with those bins for the first span
   fe_profile = profiler.fe_profile(0, field_key, bins=bin_edges)

   # get the maximum (non infinite) value from the profile
   max_fe = np.ma.masked_invalid(fe_profile).max()

   fig, ax = plt.subplots(1)
   bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                           for i in range(bin_edges.shape[0] - 1)])

   # if this is the last curve make the label show the true number of
   # cycles as the upper limit

   # TODO get the right chunk widths here
   #label = "Cycles: {}-{}".format(0, (curve_idx * time_tranche) +  time_tranche)

   plt.plot(bin_centers, fe_profile)
   plt.xlim(left=bin_edges[0], right=bin_edges[-1])
   plt.ylim(bottom=0.0, top=max_fe+5)
   plt.title("FE Profile Observable for Ligand {} Span {}, bin method {}".format(
       lig_id, span_idx, bin_method))
   plt.xlabel("ligand RMSDs ($\AA$)")
   plt.ylabel("Free Energy ($-ln(p)$)")

   plt.legend()

   plt.show()


   # then a convergence analysis

   # time_tranche = 250
   num_partitions = 5

   # then make a fe profile with those bins for the first span
   fe_profiles = profiler.fe_cumulative_profiles(0, field_key, bins=bin_edges,
                                                num_partitions=5)

   # get the maximum (non infinite) value from the profile
   max_fe = np.ma.masked_invalid(np.concatenate(fe_profiles)).max()

   fig, ax = plt.subplots(1)
   bin_centers = np.array([(bin_edges[i] + (bin_edges[i + 1] - bin_edges[i]))
                           for i in range(bin_edges.shape[0] - 1)])

   # if this is the last curve make the label show the true number of
   # cycles as the upper limit
   for curve_idx, fe_profile in enumerate(fe_profiles):

       n_cycles = len(fe_profile)

       label = "Cycles: {}-{}".format(0, (curve_idx * n_cycles) +  n_cycles)

       plt.plot(bin_centers, fe_profile,
                    label=label)


   plt.xlim(left=bin_edges[0], right=bin_edges[-1])
   plt.ylim(bottom=0.0, top=max_fe+5)
   plt.title("Convergence of Observable for Ligand {} Span {}".format(lig_id, span_idx))
   plt.xlabel("ligand RMSDs ($\AA$)")
   plt.ylabel("Free Energy ($-ln(p)$)")

   plt.legend()

   plt.show()
 #+END_SRC


 Testing the new way of doing things which returns the plots instead of
 just showing them.


 #+BEGIN_SRC python :tangle hpcc/scripts/tmp_new_plots_testing.py
   import importlib

   import numpy as np

   import matplotlib.pyplot as plt
   import matplotlib as mpl

   mpl.use('Qt5Agg')
   #mpl.use('GTK3Agg')

   from wepy.analysis.profiles import ContigTreeProfiler

   from load_results import lig3_contigtree
   from plotting_functions import bin_centers_from_edges
   from func_plot_fe_profile import plot_fe_profile, plot_fe_profiles,\
                         plot_contigtrees_observable_convergence

   bin_method = 'auto'
   field_key = 'observables/lig_rmsd'
   lig_id = 3

   profiler = ContigTreeProfiler(lig3_contigtree)

   # get the bin edges for the whole contig
   bin_edges = profiler.bin_edges(bin_method, field_key)
   bin_centers = bin_centers_from_edges(bin_edges)


   # profile the finale FE profile spans for the ligand
   span_profiles = []
   for span_idx in profiler.contigtree.span_traces.keys():
       # then make a fe profile with those bins for the first span
       fe_profile = profiler.fe_profile(span_idx, field_key, bins=bin_edges)

       span_profiles.append(fe_profile)

   fig, ax = plot_fe_profiles(span_profiles, bin_edges,
                              title="Lig 3 span FE profiles",
                              observable_label="Ligand RMSD $\AA$")


   # plot the convergences of each span FE profile
   plot_contigtrees_observable_convergence([lig3_contigtree], 'lig_rmsd',
                                           bin_method=bin_method)


   plt.show()
 #+END_SRC

*** COMMENT Debugging RMSDs (again)

 This time there are a few values that are disconnected from the others
 out past 3.5 nanometers.

 We looked in the exit point trajectories and found a few of
 these. Lets find them, get the trace for them and view them.

 View them both in the original form and then each step of the
 pipeline.

 #+BEGIN_SRC python :tangle hpcc/scripts/debug_rmsd_outliers.py
   import os.path as osp
   import itertools as it

   from copy import deepcopy

   import numpy as np

   from wepy.util.mdtraj import traj_fields_to_mdtraj
   from wepy.util.util import traj_box_vectors_to_lengths_angles, box_vectors_to_lengths_angles
   from wepy.analysis.parents import ParentForest, ancestors

   from geomm.superimpose import superimpose
   from geomm.grouping import group_pair
   from geomm.centering import center_around

   from setup_paths import tmp_dir
   from load_results import lig3_contigtree
   from selection_idxs import lig_selection_idxs
   from load_ref_states import ref_states


   from load_results import lig3_contigtree


   CUTOFF = 3.5 # nm

   wep = lig3_contigtree.wepy_h5


   # Get the traj fields you want to look at


   # trace of the outliers (run_idx, traj_idx, cycle_idx)
   outliers = []
   for trace_idx, traj_fields in wep.iter_trajs_fields(['observables/lig_rmsd'],
                                                                           idxs=True):

       rmsds = traj_fields['observables/lig_rmsd']

       outlier_cycle_idxs = np.where(rmsds > CUTOFF)[0]

       outliers.extend([(*trace_idx, cycle_idx) for cycle_idx in outlier_cycle_idxs])


   # then get the fields for that trace of outliers
   outliers_fields = wep.get_trace_fields(outliers, ['positions', 'box_vectors'])


   ## then prodce the frames for this as DCD

   json_top = wep.get_topology()

   traj = traj_fields_to_mdtraj(outliers_fields, json_top)

   test_path = osp.join(tmp_dir, "outliers.dcd")

   traj.save_dcd(test_path)


   # then regroup the complex

   lig_id = 3
   rep_key = "main_rep"

   sel_idxs = lig_selection_idxs[lig_id]

   all_atom_rep_idxs = sel_idxs['all_atoms/{}'.format(rep_key)]

   lig_idxs = sel_idxs['{}/ligand'.format(rep_key)]
   prot_idxs = sel_idxs['{}/protein'.format(rep_key)]
   bs_idxs = sel_idxs['{}/binding_site'.format(rep_key)]

   # the correct reference state for this ligand
   ref_state = ref_states[lig_id]

   ref_box_lengths, _ = box_vectors_to_lengths_angles(ref_state['box_vectors'])

   # get the reference positions as a slice of the full reference
   # state positions
   ref_positions = ref_state['positions'][all_atom_rep_idxs]

   grouped_ref_positions = group_pair(ref_positions, ref_box_lengths,
                                      bs_idxs, lig_idxs)
   centered_ref_positions = center_around(grouped_ref_positions, bs_idxs)

   # write out the grouped and centered ref positions
   ref_traj_fields = {'positions' : np.array([centered_ref_positions]),
                      'box_vectors' : np.array([ref_state['box_vectors']])}
   ref_traj = traj_fields_to_mdtraj(ref_traj_fields, json_top)
   ref_path = osp.join(tmp_dir, "main_rep_ref_GROUPED_CENTERED.pdb")
   ref_traj.save_pdb(ref_path)


   box_lengths, _ = traj_box_vectors_to_lengths_angles(outliers_fields['box_vectors'])


   # grouped positions
   grouped_positions = [group_pair(positions, box_lengths[idx],
                                       bs_idxs, lig_idxs)
                 for idx, positions in enumerate(outliers_fields['positions'])]

   outliers_fields['grouped_positions'] = np.array(grouped_positions)
   grouped_traj = traj_fields_to_mdtraj(outliers_fields, json_top, rep_key="grouped_positions")

   grouped_path = osp.join(tmp_dir, "outliers_GROUPED.dcd")

   grouped_traj.save_dcd(grouped_path)


   # then recentered positions
   centered_positions = [center_around(positions, bs_idxs)
                 for idx, positions in enumerate(outliers_fields['grouped_positions'])]

   outliers_fields['centered_positions'] = np.array(centered_positions)
   centered_traj = traj_fields_to_mdtraj(outliers_fields, json_top, rep_key='centered_positions')

   centered_path = osp.join(tmp_dir, "outliers_CENTERED.dcd")

   centered_traj.save_dcd(centered_path)


   # superimposed positions
   sup_positions = [superimpose(centered_ref_positions, positions, idxs=bs_idxs)[0]
                    for idx, positions in enumerate(outliers_fields['centered_positions'])]

   outliers_fields['sup_positions'] = np.array(sup_positions)
   sup_traj = traj_fields_to_mdtraj(outliers_fields, json_top, rep_key='sup_positions')

   sup_path = osp.join(tmp_dir, "outliers_SUP.dcd")

   sup_traj.save_dcd(sup_path)




   ## for all of these outliers we are going to generate trajectories
   ## from the initial position to see how they got there

   ancestor_traces = []
   for run_idx, walker_idx, frame_idx in outliers:
       contig_trace = lig3_contigtree.get_branch_trace(run_idx, frame_idx)
       contig = lig3_contigtree.make_contig(contig_trace)
       ptable = contig.parent_table()
       contig_ancestor_trace = ancestors(ptable, frame_idx, walker_idx)
       ancestor_trace = contig.walker_trace_to_run_trace(contig_ancestor_trace)
       ancestor_traces.append(ancestor_trace)

   # write them all as one big trajectory
   ancestors_fields = wep.get_trace_fields(it.chain(*ancestor_traces),
                                           ['positions', 'box_vectors'])

   anc_box_lengths, _ = traj_box_vectors_to_lengths_angles(ancestors_fields['box_vectors'])


   grouped_positions = [group_pair(positions, anc_box_lengths[idx],
                                       bs_idxs, lig_idxs)
                 for idx, positions in enumerate(ancestors_fields['positions'])]

   centered_positions = [center_around(positions, bs_idxs)
                 for idx, positions in enumerate(grouped_positions)]

   sup_positions = [superimpose(centered_ref_positions, positions, idxs=bs_idxs)[0]
                    for idx, positions in enumerate(centered_positions)]

   ancestors_fields['sup_positions'] = sup_positions


   ancestors_traj = traj_fields_to_mdtraj(ancestors_fields, json_top, rep_key='sup_positions')

   ancestors_path = osp.join(tmp_dir, "outliers_ancestries.dcd")

   ancestors_traj.save_dcd(ancestors_path)
 #+END_SRC

*** COMMENT Testing new compute_observable

 #+BEGIN_SRC python :tangle hpcc/scripts/tmp_test_observable.py
   import numpy as np

   import scoop

   from load_results import gexps_contigtree

   print("done importing")

   count = 0

   def stupid_observable(traj_fields):

       global count
       print("running stupid, {}".format(count))
       count += 1
       return np.array([np.array([0.0]) for _ in range(traj_fields['positions'].shape[0])])


   if __name__ == "__main__":

       print('starting')

       count = 0

       for lig_id, contigtree in gexps_contigtree:

           print("LIgand {}".format(lig_id))


           # compute the observable and save to the HDF5
           lig_sasas = contigtree.wepy_h5.compute_observable(stupid_observable,
                                                             ['positions', 'box_vectors'],
                                                             (),
                                                             map_func=map,
                                                             return_results=True)

           print("Done, writing to disk")

           # open in read/write mode to save the observables outside of
           # the mapping processes
           contigtree.wepy_h5.close()
           contigtree.wepy_h5.open(mode='r+')
           contigtree.wepy_h5.swmr_mode = True

           contigtree.wepy_h5.add_traj_observable('stupid', lig_sasas)
           contigtree.wepy_h5.close()

           print("done")
 #+END_SRC

*** COMMENT Debugging RMSDs for lig 10 and 18


 #+BEGIN_SRC python :tangle hpcc/scripts/debug_weird_rmsds.py
   import os.path as osp
   import itertools as it

   from copy import deepcopy

   import numpy as np

   from wepy.util.mdtraj import traj_fields_to_mdtraj
   from wepy.util.util import traj_box_vectors_to_lengths_angles, box_vectors_to_lengths_angles

   from setup_paths import tmp_dir
   from load_results import gexps_contigtree, lig10_contigtree, lig18_contigtree
   from func_recenter_superimpose_bs import recenter_superimpose_traj

   # the trajectories with problems in ligand 10, which were all in run 0
   lig_10_problem_traj_tups = [ (0, traj_idx) for traj_idx in [1, 7, 16, 21, 36, 41, 47]]

   # lig_10_good_traj_tups = [(0,0)]

   # for lig_id, contigtree in gexps_contigtree:

   #     # make a reference from a trajectory that didn't look bad

   #     for idxs, traj_fields in contigtree.wepy_h5.iter_trajs_fields(['positions', 'box_vectors'],
   #                                                                   idxs=True,
   #                                                                   traj_sel=[(0,0)]):

   #         print("Writing out ligand {} traj {}".format(lig_id, idxs))
   #         # write the traj out raw

   #         mdj_traj = contigtree.wepy_h5.traj_fields_to_mdtraj(traj_fields)
   #         traj_path = osp.join(tmp_dir, "lig_{}_{:0>2}_{:0>2}.dcd".format(lig_id, idxs[0], idxs[1]))
   #         mdj_traj.save_dcd(traj_path)

   #         # then recenter and write out again
   #         sup_positions, centered_ref = recenter_superimpose_traj(traj_fields, lig_id, 'main_rep')

   #         traj_fields['positions'] = sup_positions

   #         mdj_traj = contigtree.wepy_h5.traj_fields_to_mdtraj(traj_fields)
   #         traj_path = osp.join(tmp_dir, "lig_{}_{:0>2}_{:0>2}_rece.dcd".format(lig_id, idxs[0], idxs[1]))
   #         mdj_traj.save_dcd(traj_path)



   # then do it for the bad trajectories

   for idxs, traj_fields in lig10_contigtree.wepy_h5.iter_trajs_fields(['positions', 'box_vectors'],
                                                                 idxs=True,
                                                                 traj_sel=lig_10_problem_traj_tups[0:1]):

       print("Writing out bads ligand 10 {}".format(idxs))
       # write the traj out raw

       mdj_traj = lig10_contigtree.wepy_h5.traj_fields_to_mdtraj(traj_fields)
       traj_path = osp.join(tmp_dir, "lig_{}_{:0>2}_{:0>2}.dcd".format(10, idxs[0], idxs[1]))
       mdj_traj.save_dcd(traj_path)

       # then recenter and write out again
       sup_positions, centered_ref = recenter_superimpose_traj(traj_fields, 10, 'main_rep')
       traj_fields['positions'] = sup_positions

       mdj_traj = lig10_contigtree.wepy_h5.traj_fields_to_mdtraj(traj_fields)
       traj_path = osp.join(tmp_dir, "lig_{}_{:0>2}_{:0>2}_rece.dcd".format(10, idxs[0], idxs[1]))
       mdj_traj.save_dcd(traj_path)

 #+END_SRC

*** COMMENT Verifying missing atoms integrity

 Wanted to make sure the patched reporters are saving the right data
 and that I can recollect it into a full picture.

 Check that we have:
 - [X] whole structure from missing HSDs
 - [ ] run hashes (only when you combine)
 - [X] initial structures

 #+BEGIN_SRC python :tangle hpcc/scripts/verify_patches_integrity.py
   import os.path as osp
   import itertools as it

   from copy import deepcopy

   import numpy as np

   from wepy.hdf5 import WepyHDF5
   from wepy.util.mdtraj import traj_fields_to_mdtraj
   from wepy.util.util import traj_box_vectors_to_lengths_angles, box_vectors_to_lengths_angles

   from selection_idxs import lig_selection_idxs, lig_selection_tops
   from setup_paths import tmp_dir, test_runs_dir

   test_run_dir = osp.join(test_runs_dir, '3/UPDATED-config-run')

   h5_path = osp.join(test_run_dir, 'UPDATED-config-run.wepy.h5')

   wepy_h5 = WepyHDF5(h5_path, mode='r')
   wepy_h5.open()

   # get the topology for the correct rep
   correct_rep_top = lig_selection_tops[3]['correct_rep']

   # the indices of the main and correct rep to the all_atoms rep, this
   # is how we will map between the two
   # main_rep_idxs = lig_selection_idxs[3]['all_atoms/main_rep']
   correct_idxs = lig_selection_idxs[3]['all_atoms/correct_rep']

   # the indices of the missing atoms in the correct rep
   correct_missing_idxs = lig_selection_idxs[3]['correct_rep/missing']

   correct_main_rep_idxs = lig_selection_idxs[3]['correct_rep/main_rep']

   # get the indices of the missing indices over the all_atoms
   # missing_idxs = np.where(np.in1d(correct_idxs, correct_missing_idxs))[0]


   correct_rep_num_atoms = len(correct_idxs)

   # get the fields for both the main positions and the missing alt_reps
   traj_fields = wepy_h5.get_run_trace_fields(0, [(0,0)], ['positions', 'alt_reps/missing', 'box_vectors'])

   # combine the missing and the main positions, first make an empty
   # array, then fill it from each rep
   correct_rep_positions = np.zeros((1, correct_rep_num_atoms, 3))

   # then set the main rep positions
   correct_rep_positions[:, correct_main_rep_idxs, :] = traj_fields['positions'][...]
   correct_rep_positions[:, correct_missing_idxs, :] = traj_fields['alt_reps/missing'][...]

   traj_fields['alt_reps/correct'] = correct_rep_positions


   traj = traj_fields_to_mdtraj(traj_fields, correct_rep_top, rep_key='alt_reps/correct')
   test_traj_path = osp.join(tmp_dir, 'test_traj.pdb')
   traj.save_pdb(test_traj_path)


   # the initial walkers as well to test them
   initial_walker_fields = wepy_h5.initial_walker_fields(0,
                                                         ['positions', 'alt_reps/missing', 'box_vectors'])
 #+END_SRC


* Invocations

** Generating new configurations


*** <2019-11-20 Wed 12:18>

Want to add new configurations with different work mappers:

#+begin_src bash
  for work_mapper_type in 'Mapper' 'WorkerMapper' 'TaskMapper'; do
      for platform in 'CPU' 'CUDA'; do
          echo $work_mapper_type $platform 1 '10'
          bash -i prep/gen_configuration.sh $work_mapper_type $platform 1 '10'
      done
  done
#+end_src

*** <2019-12-02 Mon 16:08>

Want to add new configurations with different work mappers:

#+begin_src bash
  for work_mapper_type in 'Mapper' 'WorkerMapper' 'TaskMapper'; do
      for platform in 'CPU' 'CUDA'; do
          echo $work_mapper_type $platform 1 '10'
          bash -i prep/gen_configuration.sh $work_mapper_type $platform 1 '10'
      done
  done
#+end_src

Send them to HPCC:

#+BEGIN_SRC bash
  for lig_id in 10; do
      rsync -a -v -hh --stats -i   -z  \
           "/home/salotz/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations/" \
            "lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations"
  done
#+END_SRC

On HPCC copy them to the inputs:

#+begin_src bash
cp \
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/configurations/* \
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/input/
#+end_src


*** <2019-11-26 Tue 16:32>

#+begin_src bash
  for work_mapper_type in 'Mapper' 'WorkerMapper' 'TaskMapper'; do
      for platform in 'CPU' 'CUDA'; do
          echo $work_mapper_type $platform 1 '10'
          bash -i prep/gen_configuration.sh $work_mapper_type $platform 1 '10'
      done
  done
#+end_src


Clean up HPCC:

#+begin_src bash
rm -f hpcc/simulations/10_simulations/configurations/*
rm -f hpcc/simulations/10_simulations/input/*
#+end_src

Send them to HPCC
#+BEGIN_SRC bash
  for lig_id in 10; do
      rsync -a -v -hh --stats -i   -z  \
           "/home/salotz/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations/" \
            "lotzsamu@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/${lig_id}_simulations/configurations"
  done
#+END_SRC

Once they are there we need to copy them to all the right places

#+begin_src bash
cp hpcc/simulations/10_simulations/configurations/* hpcc/simulations/10_simulations/input/
#+end_src


** Submitting jobs

*** <2019-11-15 Fri 11:54>

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-1'
#+end_src

*** <2019-11-22 Fri 16:19>

First make sure you create the submissions:

#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_src

#+begin_example
(common) lotzsamu@dev-intel16-k80 :$28: [/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping]{Fri Nov 2204:42:18} 0: 
--> ./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_example


*** <2019-11-25 Mon 10:52>

#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_src

#+begin_example

#+end_example


*** <2019-11-25 Mon 16:10>

#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_src

#+begin_example

#+end_example



*** <2019-11-26 Tue 11:34>

#+begin_src bash
  inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_src

#+begin_example

#+end_example


*** <2019-11-26 Tue 16:52>


#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-2' '1-1' '2-2' '3-2' '4-2' '5-1'
#+end_src

#+begin_example

#+end_example
*** <2019-11-27 Wed 12:43>

*** <2019-12-02 Mon 15:27>

Add the run specs you want.

- [X] '1-2' '2-3' '3-3' '4-3' '5-2'

#+begin_src bash
inv custom.push-scripts
#+end_src

#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '1-2' '2-3' '3-3' '4-3' '5-2'
#+end_src


*** <2019-12-03 Tue 14:23>

Add the run specs you want.

- [X] '0-3' '3-3'

On local:
#+begin_src bash
inv custom.push-scripts
#+end_src

On HPCC:
#+begin_src bash
inv custom.hpcc-gen-submissions
#+end_src

#+begin_src bash
./hpcc/scripts/submit_contig_runs.sh 10 '0-3' '3-3'
#+end_src


** Post-Simulation

*** <2019-11-22 Fri 14:37>

Gather the list of the jobs you want to deal with:

49459812 49415681 49415682 49415683 49459810


**** The end hashes

#+begin_src bash
source hpcc/scripts/bash_funcs.sh 
print-gathered-checkpoint-snaphashes 10 49459812 49415681 49415682 49415683 49459810
#+end_src

#+begin_example
--> print-gathered-checkpoint-snaphashes 10 49459812 49415681 49415682 49415683 49459810

Getting Paths
49459812
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812/lig-10_contig-0-1_production/checkpoint.orch.sqlite
49415681
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681/lig-10_contig-2-1_production/checkpoint.orch.sqlite
49415682
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682/lig-10_contig-3-1_production/checkpoint.orch.sqlite
49415683
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683/lig-10_contig-4-1_production/checkpoint.orch.sqlite
49459810
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810/lig-10_contig-5-0_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812/lig-10_contig-0-1_production/checkpoint.orch.sqlite snapsots:
0fe3b4450344a149fdf6778b013dcd79 e5cdd6a932d7ef99f42af4d0fc0b13c3
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681/lig-10_contig-2-1_production/checkpoint.orch.sqlite snapsots:
a727fe1b95ef3a91aa103a30f3c734c9 f9d613627fdd07fd714afb6a025084e8
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682/lig-10_contig-3-1_production/checkpoint.orch.sqlite snapsots:
61cb809ee5a2fcc8ac2ca96203999770 64c6723e262b80a4a99b4f69a2ebabb9
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683/lig-10_contig-4-1_production/checkpoint.orch.sqlite snapsots:
6328a9234a028649ddf94bc40499e083 7f24b6fe68739af9536e1eebf14d870f
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810/lig-10_contig-5-0_production/checkpoint.orch.sqlite snapsots:
0b7c48749264490eadac5af79987ca00 d6c98acf0b786c8dd94cf957469d0dcf
#+end_example



**** Reconcile

Get an interactive job since this takes some time:

#+BEGIN_SRC bash
srun -N 1 -c 8 --mem=25G --time=72:00:00 --pty /bin/bash
#+END_SRC

#+begin_src bash
./hpcc/scripts/reconcile_runs_orch.sh 10 49459812 49415681 49415682 49415683 49459810
#+end_src


#+begin_example
--> ./hpcc/scripts/reconcile_runs_orch.sh 10 49459812 49415681 49415682 49415683 49459810

...

Getting Paths
49459812
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812/lig-10_contig-0-1_production/checkpoint.orch.sqlite
49415681
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681/lig-10_contig-2-1_production/checkpoint.orch.sqlite
49415682
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682/lig-10_contig-3-1_production/checkpoint.orch.sqlite
49415683
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683/lig-10_contig-4-1_production/checkpoint.orch.sqlite
49459810
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810/lig-10_contig-5-0_production/checkpoint.orch.sqlite

orchestrators being reconciled
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459812/lig-10_contig-0-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415681/lig-10_contig-2-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415682/lig-10_contig-3-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49415683/lig-10_contig-4-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/49459810/lig-10_contig-5-0_production/checkpoint.orch.sqlite
Reconciling
Reconciling snapshots to: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite
#+end_example


*** <2019-12-02 Mon 15:15>

We are cancelling some jobs which are running way too slow on the work
mapper we need them for.

So first we do that and then reconcile them

50642728 50642729 50642730 50642731 50642732

#+begin_src bash
scancel 50642728 50642729 50642730 50642731 50642732
#+end_src

**** The end hashes

#+begin_src bash
source hpcc/scripts/bash_funcs.sh 
print-gathered-checkpoint-snaphashes 10 50642728 50642729 50642730 50642731 50642732
#+end_src

#+begin_example
--> print-gathered-checkpoint-snaphashes 10 50642728 50642729 50642730 50642731 50642732

Getting Paths
50642728
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728/lig-10_contig-1-1_production/checkpoint.orch.sqlite
50642729
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729/lig-10_contig-2-2_production/checkpoint.orch.sqlite
50642730
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730/lig-10_contig-3-2_production/checkpoint.orch.sqlite
50642731
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731/lig-10_contig-4-2_production/checkpoint.orch.sqlite
50642732
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732/lig-10_contig-5-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728/lig-10_contig-1-1_production/checkpoint.orch.sqlite snapsots:
d567d7e73589156a5942f3635ea370e9 d6026a71b66f31053665c0ccbba16f84
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729/lig-10_contig-2-2_production/checkpoint.orch.sqlite snapsots:
9b1483a3fe8776431bec6e4863661fc0 e4c77e375e5bc2bc8e9d7357d3c16ce7
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730/lig-10_contig-3-2_production/checkpoint.orch.sqlite snapsots:
6522772c2320c24a5e5aac0e17ef0bb5 dc74d28358ef2361690698ff966b96db
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731/lig-10_contig-4-2_production/checkpoint.orch.sqlite snapsots:
ee5515f439567f3787c87bd5ac3d63ea f54d00819eb8ff9db67336ac198853a2
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732/lig-10_contig-5-1_production/checkpoint.orch.sqlite snapsots:
978f37c41a8d880a45e8a35b91ab5be3 cca05a1850609c6c488b295a36604317
#+end_example

- [X] put them into the management

**** Reconcile

Get an interactive job since this takes some time:

#+BEGIN_SRC bash
srun -N 1 -c 8 --mem=25G --time=72:00:00 --pty /bin/bash
#+END_SRC

#+begin_src bash
./hpcc/scripts/reconcile_runs_orch.sh 10 50642728 50642729 50642730 50642731 50642732
#+end_src


#+begin_example
lotzsamu@lac-028 :$0: [/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping]{Mon Dec 0203:19:17} 0: 
--> ./hpcc/scripts/reconcile_runs_orch.sh 10 50642728 50642729 50642730 50642731 50642732

Getting Paths
50642728
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728/lig-10_contig-1-1_production/checkpoint.orch.sqlite
50642729
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729/lig-10_contig-2-2_production/checkpoint.orch.sqlite
50642730
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730/lig-10_contig-3-2_production/checkpoint.orch.sqlite
50642731
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731/lig-10_contig-4-2_production/checkpoint.orch.sqlite
50642732
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732/lig-10_contig-5-1_production/checkpoint.orch.sqlite

orchestrators being reconciled
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642728/lig-10_contig-1-1_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642729/lig-10_contig-2-2_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642730/lig-10_contig-3-2_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642731/lig-10_contig-4-2_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642732/lig-10_contig-5-1_production/checkpoint.orch.sqlite
Reconciling
Reconciling snapshots to: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite

#+end_example



*** <2019-12-03 Tue 14:19>

50642727

**** The end hashes

#+begin_src bash
source hpcc/scripts/bash_funcs.sh 
print-gathered-checkpoint-snaphashes 10 50642727
#+end_src

#+begin_example
(seh.pathway_hopping.common) lotzsamu@dev-intel16-k80 :$12: [/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping]{Tue Dec 0302:20:31} 0: 
--> print-gathered-checkpoint-snaphashes 10 50642727

Getting Paths
50642727
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727/lig-10_contig-0-2_production/checkpoint.orch.sqlite
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727/lig-10_contig-0-2_production/checkpoint.orch.sqlite snapsots:
7680a05e926708603aa88f30841756e2 978f37c41a8d880a45e8a35b91ab5be3

#+end_example

- [ ] put them into the management

**** Reconcile

Get an interactive job since this takes some time:

#+begin_src bash
./hpcc/scripts/reconcile_runs_orch.sh 10 50642727
#+end_src


#+begin_example
(seh.pathway_hopping.common) lotzsamu@dev-intel16-k80 :$12: [/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping]{Tue Dec 0302:20:52} 0: 
--> ./hpcc/scripts/reconcile_runs_orch.sh 10 50642727

Getting Paths
50642727
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727
Found checkpoint: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727/lig-10_contig-0-2_production/checkpoint.orch.sqlite

orchestrators being reconciled
/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/jobs/50642727/lig-10_contig-0-2_production/checkpoint.orch.sqlite
Reconciling
Reconciling snapshots to: /mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/hpcc/simulations/10_simulations/orchs/master_sEH_lig-10.orch.sqlite
#+end_example



** Backing up simulations

Periodically we will back up the simulations to the storage on volta

*** <2019-12-03 Tue 14:41>

Get a job to do this with

#+BEGIN_SRC bash
srun -N 1 -c 8 --mem=25G --time=72:00:00 --pty /bin/bash
#+END_SRC

- [X] Clean out the bad runs

- [X]
#+BEGIN_SRC bash
rsync -ravvhhiz $SCRATCH/tree/lab/ \
                   salotz@volta.bch.msu.edu:~/tree/lab/
#+END_SRC




* Management

** Ligand Simulations

THis is to track the tree of simulations and to keep track of which
ones are good or not.

Reconciled is to track whether or not the HDF5 and orchestrators have
been merged.

Or if the job FAILED or was CANCELLED whether the checkpoint has been
put back into the inputs.

*** Template

**** Experiment Title

***** tree

- root :: xxx (patched: yyy)
  - 0
    - 0-0 :: 
  - 1
    - 1-0 :: 
  - 2
    - 2-0 :: 
  - 3
    - 3-0 :: 
  - 4
    - 4-0 :: 
  - 5
    - 5-0 :: 


***** table


| end_hash | contig_ID | job_ID | node | state | tag | cycles | exits | keep | success | reconciled | hdf5-join | backedup | start_hash | patched_start_hash |
|----------+-----------+--------+------+-------+-----+--------+-------+------+---------+------------+-----------+----------+------------+--------------------|
|          |       0-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|          |       1-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|          |       2-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|          |       3-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|          |       4-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|          |       5-0 |        |      |       |     |        |       |      |         |            |           |          |            |                    |
|----------+-----------+--------+------+-------+-----+--------+-------+------+---------+------------+-----------+----------+------------+--------------------|


***** Notes



*** 3

**** Current

***** span tree

This new root snapshot is suitable for things like test runs from the
root orches. To add runs to the started datasets use the original root hash

- new new root snapshot hash :: 71b4a7d5049fbd53b90b493f1a511336
- new root snapshot hash :: 8e158c28c8c426f064d032823bb9aaf9

This is the tree from the simulations which were started orginally, if
you are to run simulations within this collective dataset use the
snapshots defined here. The root snapshot similar to the new root
snapshot, but somehow is a different hash from when we rebuilt things.

- root :: ed11f6caeabd40a7cf5d687a98f5a6b2
  - 0
    - 0-0 :: de390f00dac09f52f97d82ca5b8789fa
    - 0-1 :: 2a2888da6c4733be0328b22eb4b7af26
    - 0-2 :: d1317b41436d5d9c48b79c1b500d759f
    - 0-3 :: 055e58cbe616fc271fb25d3daa78b471
    - 0-4 :: 0cf1c1dbbdb8a8e91f0bfc5c090a0b74
    - 0-5 :: 7153db16782171f2534656644c98ffea
  - 1
    - 1-0 :: bb621a3fa5b36b7954a3d21745f203aa
    - 1-1 :: 8e97b2d28e2afd49db6b2e1c18d48608
    - 1-2 :: ec513e3d59a5b0d921bbe93a9b17857e
  - 2
    - 2-0 :: 12f792ca1ec034cac0de7d9ff6517c91
    - 2-1 :: 1dde5be5baf0faef2233201b68832245
    - 2-2 :: f5835aba9352e5d8c4bd73e22c0b74f5
  - 3
    - 3-0 :: 9b027c1488717712905b00fe3e8cb7a4
    - 3-1 :: 1ff9228c6da5cd22713a9d8ec42351ad
    - 3-2 :: 844000198b1d1c40a6a2a23bf726887b
  - 4
    - 4-0 :: eaab5d019e061a229dd07e387a242ffc
    - 4-1 :: 5756b809fe2f60269f46eff3407d76db
    - 4-2 :: 09706bed29a4b433302be2566848d861
  - 5
    - 5-0 :: ff4b43ac2bb8c4277fbc0cb3b08496c4
    - 5-1 :: 18dcfb9749be00a67e22f85105ee378d
    - 5-2 :: afbc501c4a89356fc68164cb15fc3ea8


***** Old hashes

- root :: 3a3eed6bb613a042d089547bcab8b1e6
  - 0 :: 1b0a5c2dc8d36ab234935a7c6b3cfc44
    - 0-0 :: 1b0a5c2dc8d36ab234935a7c6b3cfc44 
    - 0-1 :: 608107bdef0980187daa7c66c125e854
    - 0-2 :: 8cd97e8583f682845106ff1456916318
    - 0-3 :: b6119f14a8656e9b7099865a6bf443bd
    - 0-4 :: 2ade05295a691b7eb907128deba57198
  - 1 :: d005a8ff3725cd638d8d5f64d0106f39
    - 1-0 :: d005a8ff3725cd638d8d5f64d0106f39
    - 1-1 :: 2e37121d537d99fdcf6ad36283def254
  - 2 :: 81ec62755ca3eb30c38390eab5c33b2b
    - 2-0 :: 81ec62755ca3eb30c38390eab5c33b2b
    - 2-1 :: d9d4ac20026372f2b57a37bf99f2dc39
  - 3 :: 25b4dd9712ae9d113514d31bc4a69ade
    - 3-0 :: 25b4dd9712ae9d113514d31bc4a69ade
    - 3-1 :: 2847ddf56740caab77a9edfc55f03c71
  - 4 :: 8725e396deb0cad3904b05f64a73fe8f
    - 4-0 :: 8725e396deb0cad3904b05f64a73fe8f
    - 4-1 :: 530a1088efb3186496ef2fea6be4aab4
  - 5 :: b43cc17d8c17d88be1167be2515bd8ba
    - 5-0 :: b43cc17d8c17d88be1167be2515bd8ba
    - 5-1 :: d399d6b123c54eec7ea1e755329739a9


***** mapping old to new script

Here is a python datastructure with that relationship

NOTE WARNING: This doesn't produce the right hashes to match up with
the job produced orchs for runs. I.e. the table above was not produced
entirely with this.

#+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
  import itertools as it
  from collections import defaultdict

  snaphash_records = []

  lig_3_orch_hash_maps = [

      ('13326969',
      (('d005a8ff3725cd638d8d5f64d0106f39', 'bb621a3fa5b36b7954a3d21745f203aa'),
       ('2e37121d537d99fdcf6ad36283def254', '8e97b2d28e2afd49db6b2e1c18d48608'))),

      ('13326970',
       (('81ec62755ca3eb30c38390eab5c33b2b', '12f792ca1ec034cac0de7d9ff6517c91'),
        ('d9d4ac20026372f2b57a37bf99f2dc39', '1dde5be5baf0faef2233201b68832245'))),

      ('13326971',
       (('25b4dd9712ae9d113514d31bc4a69ade', '9b027c1488717712905b00fe3e8cb7a4'),
        ('2847ddf56740caab77a9edfc55f03c71', '1ff9228c6da5cd22713a9d8ec42351ad'))),

      ('13326972',
       (('8725e396deb0cad3904b05f64a73fe8f', 'eaab5d019e061a229dd07e387a242ffc'),
        ('530a1088efb3186496ef2fea6be4aab4', '5756b809fe2f60269f46eff3407d76db'))),

      ('13326973',
       (('b43cc17d8c17d88be1167be2515bd8ba', 'ff4b43ac2bb8c4277fbc0cb3b08496c4'),
        ('d399d6b123c54eec7ea1e755329739a9', '18dcfb9749be00a67e22f85105ee378d'))),

      ('13327704',
       (('b6119f14a8656e9b7099865a6bf443bd', '055e58cbe616fc271fb25d3daa78b471'),
        ('2ade05295a691b7eb907128deba57198', '0cf1c1dbbdb8a8e91f0bfc5c090a0b74'))),

      ('2503029',
       (('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'))),

      ('3075070',
       (('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'))),

      ('3722763',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'))),

      ('5940557',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('d005a8ff3725cd638d8d5f64d0106f39', 'cd3be07f43d753b6f66f329912239d32'))),

      ('5940559',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('b6119f14a8656e9b7099865a6bf443bd', '885203f0f586faf9e484089185082e12'))),

      ('5940766',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('81ec62755ca3eb30c38390eab5c33b2b', '429601ec8d38abbd21a060e1f82bb701'))),

      ('5940966',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('25b4dd9712ae9d113514d31bc4a69ade', 'e0b4eb8f2c51ab4c9545be850a5aecb3'))),

      ('5940967',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('8725e396deb0cad3904b05f64a73fe8f', '7d31bef63622aae7e364ff369d78e706'))),

      ('5940968',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('b43cc17d8c17d88be1167be2515bd8ba', '96de1865ea30c868afe82340088ea4c2'))),
  ]


  # just ligand 3

  # combine them into one set of records with the job ids
  lig_3_records = list(it.chain.from_iterable(
        [[(orch_hash_map[0], old, new) for old, new in orch_hash_map[1]]
                for orch_hash_map in lig_3_orch_hash_maps]))

  # find the old hashes with multiple new ones, so we can choose one to
  # use
  lig_3_old_to_new = defaultdict(set)

  for job_id, old, new in lig_3_records:
      lig_3_old_to_new[old].add(new)

  lig_3_old_to_new_assgs = []
  for old_hash, new_hashes in lig_3_old_to_new.items():
      # just pop one off at random to use
      lig_3_old_to_new_assgs.append((old_hash, new_hashes.pop()))

  # print it out in an org-table like way
  print("\n")
  print("Ligand 3 all mappings table")
  print("| job id | old hash | new hash |")
  print(str(lig_3_records).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  print("Ligand 3 assigned mappings table")
  print("| old hash | new hash |")
  print(str(lig_3_old_to_new_assgs).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")


  snaphash_records.append(lig_3_records)
#+END_SRC

***** all mappings table

|   job id | old hash                         | new hash                         |
|----------+----------------------------------+----------------------------------|
| 13326969 | d005a8ff3725cd638d8d5f64d0106f39 | bb621a3fa5b36b7954a3d21745f203aa |
| 13326969 | 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 |
| 13326970 | 81ec62755ca3eb30c38390eab5c33b2b | 12f792ca1ec034cac0de7d9ff6517c91 |
| 13326970 | d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 |
| 13326971 | 25b4dd9712ae9d113514d31bc4a69ade | 9b027c1488717712905b00fe3e8cb7a4 |
| 13326971 | 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad |
| 13326972 | 8725e396deb0cad3904b05f64a73fe8f | eaab5d019e061a229dd07e387a242ffc |
| 13326972 | 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db |
| 13326973 | b43cc17d8c17d88be1167be2515bd8ba | ff4b43ac2bb8c4277fbc0cb3b08496c4 |
| 13326973 | d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d |
| 13327704 | b6119f14a8656e9b7099865a6bf443bd | 055e58cbe616fc271fb25d3daa78b471 |
| 13327704 | 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 |
|  2503029 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  2503029 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3075070 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3075070 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  3722763 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  3722763 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  3722763 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3722763 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  3722763 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940557 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940557 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940557 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940557 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940557 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940557 | d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 |
|  5940559 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940559 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940559 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940559 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940559 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940559 | b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 |
|  5940766 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940766 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940766 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940766 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940766 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940766 | 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 |
|  5940966 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940966 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940966 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940966 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940966 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940966 | 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 |
|  5940967 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940967 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940967 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940967 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940967 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940967 | 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 |
|  5940968 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940968 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940968 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940968 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940968 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940968 | b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 |


***** assigned mappings

| old hash                         | new hash                         | redo new hash                    |
|----------------------------------+----------------------------------+----------------------------------|
| d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 | bb621a3fa5b36b7954a3d21745f203aa |
| 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 |                                  |
| 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 | 12f792ca1ec034cac0de7d9ff6517c91 |
| d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 |                                  |
| 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 | 9b027c1488717712905b00fe3e8cb7a4 |
| 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad |                                  |
| 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 | eaab5d019e061a229dd07e387a242ffc |
| 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db |                                  |
| b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 | ff4b43ac2bb8c4277fbc0cb3b08496c4 |
| d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d |                                  |
| b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 | 055e58cbe616fc271fb25d3daa78b471 |
| 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 |                                  |
| 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f | ed11f6caeabd40a7cf5d687a98f5a6b2 |
| 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 | de390f00dac09f52f97d82ca5b8789fa |
| 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b | 2a2888da6c4733be0328b22eb4b7af26 |
| 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 | d1317b41436d5d9c48b79c1b500d759f |




***** table
     :PROPERTIES:
     :TABLE_EXPORT_FILE: data/sim_management/lig_3.csv
     :TABLE_EXPORT_FORMAT: orgtbl-to-csv
     :END:

|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| end_hash                         | run_idx | contig_ID |   job_ID | state     | start_hash | patched_start_hash | exits |            exit_weight | success | keep | copied | reconciled | backedup |                   rate |   restime | old_end_hash                     | v2_end_hash                      | node    | cycles | avg. runner time | avg. BC time | avg. res time | avg. cycle time | avg seg time |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| de390f00dac09f52f97d82ca5b8789fa |       0 |       0-0 |  2503029 | FINISHED  |            |                    |     0 |                      0 | yes     | yes  | yes    | yes        | yes      |                      0 |       1/0 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 | lac-027 |    241 |                  |              |               |                 |              |
| 2a2888da6c4733be0328b22eb4b7af26 |       1 |       0-1 |  3075070 | RECOVERED |            |                    |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                      0 |       1/0 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b | lac-027 |    155 |                  |              |               |                 |              |
| d1317b41436d5d9c48b79c1b500d759f |       2 |       0-2 |  3722763 | FINISHED  |            |                    |     6 |  9.097017821291077e-12 | yes     | yes  | yes    | yes        | yes      | 1.3997134757033284e-05 | 71443.193 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 | lac-025 |    677 |                  |              |               |                 |              |
| 055e58cbe616fc271fb25d3daa78b471 |       3 |       0-3 |  5940559 | FINISHED  |            |                    |     5 |  6.995932729903008e-12 | yes     | yes  | yes    | yes        | yes      | 1.0638583835010646e-05 | 93997.473 | b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 | lac-027 |    685 |                  |              |               |                 |              |
| 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 |       9 |       0-4 | 13327704 | CANCELLED |            |                    |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                        |           | 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 | lac-288 |    152 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| bb621a3fa5b36b7954a3d21745f203aa |       4 |       1-0 |  5940557 | FINISHED  |            |                    |     2 | 1.2983715742088229e-11 | yes     | yes  | yes    | yes        | yes      | 1.8942162322140815e-05 | 52792.283 | d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 | lac-141 |    714 |                  |              |               |                 |              |
| 8e97b2d28e2afd49db6b2e1c18d48608 |      10 |       1-1 | 13326969 | CANCELLED |            |                    |     8 |  5.309500795900743e-11 | yes     | yes  | yes    | yes        | yes      |                        |           | 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 | lac-028 |    557 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| 12f792ca1ec034cac0de7d9ff6517c91 |       5 |       2-0 |  5940766 | FINISHED  |            |                    |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                      0 |       1/0 | 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 | lac-028 |    738 |                  |              |               |                 |              |
| 1dde5be5baf0faef2233201b68832245 |      11 |       2-1 | 13326970 | CANCELLED |            |                    |     1 | 1.4833536742875784e-12 | yes     | yes  | yes    | yes        | yes      |                        |           | d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 | lac-027 |    555 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| 9b027c1488717712905b00fe3e8cb7a4 |       6 |       3-0 |  5940966 | FINISHED  |            |                    |     6 |  8.041574068210327e-09 | yes     | yes  | yes    | yes        | yes      |   0.012087503108781742 | 82.730072 | 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 | lac-028 |    693 |                  |              |               |                 |              |
| 1ff9228c6da5cd22713a9d8ec42351ad |      12 |       3-1 | 13326971 | CANCELLED |            |                    |    29 | 3.4406083367984306e-08 | yes     | yes  | yes    | yes        | yes      |                        |           | 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad | nvl-003 |   1055 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| eaab5d019e061a229dd07e387a242ffc |       7 |       4-0 |  5940967 | FINISHED  |            |                    |    32 |  5.886065409321491e-09 | yes     | yes  | yes    | yes        | yes      |   0.008911799614403896 | 112.21078 | 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 | lac-195 |    688 |                  |              |               |                 |              |
| 5756b809fe2f60269f46eff3407d76db |      13 |       4-1 | 13326972 | CANCELLED |            |                    |    33 | 3.7826321754656087e-10 | yes     | yes  | yes    | yes        | yes      |                        |           | 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db | lac-140 |    553 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| ff4b43ac2bb8c4277fbc0cb3b08496c4 |       8 |       5-0 |  5940968 | FINISHED  |            |                    |     2 |  3.400666086019177e-12 | yes     | yes  | yes    | yes        | yes      |  4.886014491406851e-06 | 204665.79 | b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 | lac-025 |    725 |                  |              |               |                 |              |
| 18dcfb9749be00a67e22f85105ee378d |      14 |       5-1 | 13326973 | CANCELLED |            |                    |     7 |  3.440377372541073e-09 | yes     | yes  | yes    | yes        | yes      |                        |           | d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d | nvl-005 |    992 |                  |              |               |                 |              |
|----------------------------------+---------+-----------+----------+-----------+------------+--------------------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| 7153db16782171f2534656644c98ffea |         |       0-5 | 22160883 | FINISHED  |            |                    |    31 | 3.4210872876361725e-09 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-028 |    856 |              632 |           33 |             2 |             667 |           68 |
| ec513e3d59a5b0d921bbe93a9b17857e |         |       1-2 | 22073108 | FINISHED  |            |                    |   110 |  8.398273190754693e-06 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-002 |   1771 |              264 |           33 |             2 |             301 |            7 |
| f5835aba9352e5d8c4bd73e22c0b74f5 |         |       2-2 | 22073109 | FINISHED  |            |                    |    49 |  7.692635141264896e-10 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-003 |   1791 |              260 |           33 |             2 |             296 |            7 |
| 844000198b1d1c40a6a2a23bf726887b |         |       3-2 | 22073110 | FINISHED  |            |                    |    75 |  8.755167184270571e-08 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-004 |   1800 |              260 |           33 |             2 |             295 |            7 |
| 09706bed29a4b433302be2566848d861 |         |       4-2 | 22073111 | FINISHED  |            |                    |    64 |  3.409734339522123e-10 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-195 |    848 |              631 |           37 |             2 |             671 |           70 |
| afbc501c4a89356fc68164cb15fc3ea8 |         |       5-2 | 22073112 | FINISHED  |            |                    |    33 |  6.917655884624962e-09 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-141 |    757 |              715 |           38 |             2 |             757 |           75 |
#+TBLFM: $16=1/$rate



**** COMMENT Old

These are some scripts I did for makign tables and such.


***** calculating contig props

#+begin_src python :tangle hpcc/scripts/calc_contig_props.py
  import numpy as np
  import pandas as pd

  import simtk.unit as unit

  from seh_prep.parameters import STEP_TIME, N_CYCLE_STEPS, N_WALKERS, CYCLE_TIME

  run_cols = ['contig_idx', 'inter_contig_idx', 'n_cycles', 'n_exits', 'exit_weight']
  run_recs = [
      # contig 0
      (0, 0, 241, 0, 0.),
      (0, 1, 155, 0, 0.),
      (0, 2, 677, 6, 9.097017821291077e-12),
      (0, 3, 685, 5, 6.995932729903008e-12),
      (0, 4, 152, 0, 0.),
      (0, 5, 856, 31, 3.4210872876361725e-09),

      # contig 1
      (1, 0, 714, 2, 1.2983715742088229e-11),
      (1, 1, 557, 8, 5.309500795900743e-11),
      (1, 2, 1771, 110, 8.398273190754693e-06),

      # contig 2
      (2, 0, 738, 0, 0.),
      (2, 1, 555, 1, 1.4833536742875784e-12),
      (2, 2, 1791, 49, 7.692635141264896e-10),

      # contig 3
      (3, 0, 693, 6, 8.041574068210327e-09),
      (3, 1, 1055, 29, 3.4406083367984306e-08),
      (3, 2, 1800, 75, 8.755167184270571e-08),

      # contig 4
      (4, 0, 688, 32,  5.886065409321491e-09),
      (4, 1, 553, 33, 3.7826321754656087e-10),
      (4, 2, 848, 64, 3.409734339522123e-10),

      # contig 5
      (5, 0, 725, 2, 3.400666086019177e-12),
      (5, 1, 992, 7, 3.440377372541073e-09),
      (5, 2, 757, 33, 6.917655884624962e-09),
  ]

  run_df = pd.DataFrame(run_recs, columns=run_cols)


  run_agg_funcs = {
      'inter_contig_idx' : len,
      'n_cycles' : np.sum,
      'n_exits' : np.sum,
      'exit_weight' : np.sum,
  }

  contig_df = run_df.groupby('contig_idx')\
                    .agg(run_agg_funcs)\
                    .rename(columns={'inter_contig_idx' : 'n_runs'})

  CYCLE_SAMPLING_TIME = CYCLE_TIME * N_WALKERS

  # amount of sampling for each indivual walker, the longest possible
  # contiguous simulation
  contig_df['walker_sampling_time (ms)'] = [(n_cycles * CYCLE_TIME).value_in_unit(unit.microsecond)
                                            for n_cycles in contig_df['n_cycles']]

  # total aggregate ensemble simiulation time
  contig_df['ensemble_sampling_time (ms)'] = [(n_cycles * CYCLE_SAMPLING_TIME).value_in_unit(unit.microsecond)
                                           for n_cycles in contig_df['n_cycles']]

#+end_src

***** calculating performance

#+begin_src python :tangle hpcc/scripts/calc_run_perfs.py
  from copy import copy

  import numpy as np
  import pandas as pd

  import simtk.unit as unit

  from seh_prep.parameters import STEP_TIME, N_CYCLE_STEPS, N_WALKERS, CYCLE_TIME

  perf_rec_cols = [
      'job_id',
      'node_id',
      'n_cycles',
      'avg_runner_time_s',
      'avg_bc_time_s',
      'avg_resampling_time_s',
      'avg_cycle_time_s',
      'avg_segment_time_s',
  ]

  # TODO add total actual runtime



  run_recs = [

  ( '22160883' , 'lac-028' ,  856 , 632 , 33 , 2 , 667 , 68 ),
  ( '22073108' , 'nvl-002' , 1771 , 264 , 33 , 2 , 301 ,  7 ),
  ( '22073109' , 'nvl-003' , 1791 , 260 , 33 , 2 , 296 ,  7 ),
  ( '22073110' , 'nvl-004' , 1800 , 260 , 33 , 2 , 295 ,  7 ),
  ( '22073111' , 'lac-195' ,  848 , 631 , 37 , 2 , 671 , 70 ),
  ( '22073112' , 'lac-141' ,  757 , 715 , 38 , 2 , 757 , 75 ),

   ]

  perf_df = pd.DataFrame(run_recs, columns=perf_rec_cols)

  # then compute some things from this that are of interest

  # split up the node id into node types
  node_types_col = [node_id.split('-')[0] for node_id in perf_df['node_id']]

  perf_df['node_type'] = node_types_col

  # compute the avg sampling time from the cycles
  perf_df['avg_sampling_time_ns'] = [(n_cycles * CYCLE_TIME).value_in_unit(unit.nanosecond)
                                     for n_cycles in perf_df['n_cycles']]


  # group by the node type and average over all of the interesting columns

  run_agg_funcs = {
      'job_id' : len,
      'n_cycles' : np.mean,
      'avg_runner_time_s' : np.mean,
      'avg_bc_time_s' : np.mean,
      'avg_resampling_time_s' : np.mean,
      'avg_cycle_time_s' : np.mean,
      'avg_segment_time_s' : np.mean,
      'avg_sampling_time_ns' : np.mean,
  }

  node_perf_df = perf_df.drop(columns=['node_id'])\
                    .groupby('node_type')\
                    .agg(run_agg_funcs)\
                    .rename(columns={'job_id' : 'n_runs'})


  # calculate the speedups for each time column
  time_cols = (

      'avg_runner_time_s',
      'avg_bc_time_s',
      'avg_resampling_time_s',
      'avg_cycle_time_s',
      'avg_segment_time_s',
  )

  sampling_cols = (
      'avg_sampling_time_ns',
      'n_cycles',
  )

  speedup_rec = {}
  for col in node_perf_df.columns:
      if col in sampling_cols:
          speedup_rec[col] = node_perf_df.loc['nvl'][col] / node_perf_df.loc['lac'][col]
      elif col in time_cols:
          speedup_rec[col] = node_perf_df.loc['lac'][col] / node_perf_df.loc['nvl'][col]
      else:
          speedup_rec[col] = np.nan

  speedup_df = copy(node_perf_df)
  speedup_df.loc['speedup'] = speedup_rec


  #  calculate the nanoseconds per hour, per day and per week
  week_hours = 168
  week_days = 7
  num_gpus = 8

  node_perf_df['ns_per_node_week'] = node_perf_df['avg_sampling_time_ns']
  node_perf_df['ns_per_node_day'] = node_perf_df['avg_sampling_time_ns'] / week_days
  node_perf_df['ns_per_node_hour'] = node_perf_df['avg_sampling_time_ns'] / week_hours

  node_perf_df['ns_per_gpu_week'] = node_perf_df['avg_sampling_time_ns'] / num_gpus
  node_perf_df['ns_per_gpu_day'] = node_perf_df['avg_sampling_time_ns'] / week_days / num_gpus
  node_perf_df['ns_per_gpu_hour'] = node_perf_df['avg_sampling_time_ns'] / week_hours / num_gpus


  throughput_cols = [
      'ns_per_node_week',
      'ns_per_node_day',
      'ns_per_node_hour',
      'ns_per_gpu_week',
      'ns_per_gpu_day',
      'ns_per_gpu_hour',
  ]

  throughput_df = node_perf_df[throughput_cols]


  # computing overheads of regular MD with OpenMM
  node_perf_df['avg_total_segment_gpu_time_s'] = node_perf_df['avg_segment_time_s'] * N_WALKERS

  node_perf_df['avg_total_segment_time_s'] = node_perf_df['avg_total_segment_gpu_time_s'] / num_gpus

  # compute Runner overhead
  node_perf_df['runner_overhead_s'] = node_perf_df['avg_runner_time_s'] - \
                                    node_perf_df['avg_total_segment_time_s']


  # compute Wepy total overhead
  node_perf_df['cycle_overhead_s'] = node_perf_df['avg_cycle_time_s'] - \
                                    node_perf_df['avg_total_segment_time_s']


  # include the end time where we are waiting for the simulation to
  # end. I don't have the actual rn times available anyways. But the
  # idea would be to minimize this as well since we run on node time. To
  # report wepy as a library overhead use the cycle_overhead
  total_time = week_hours * 60 * 60
  node_perf_df['node_time_overhead_s'] = total_time - \
                               (node_perf_df['avg_total_segment_time_s'] *
                                node_perf_df['n_cycles'])


  # get the residual overhead (not accounted by the components timings)
  node_perf_df['node_time_residual_overhead_s'] = node_perf_df['node_time_overhead_s'] - \
                                                  (node_perf_df['cycle_overhead_s'] *
                                                   node_perf_df['n_cycles'])

  node_perf_df['node_time_overhead_h'] = node_perf_df['node_time_overhead_s'] / (60**2)
  node_perf_df['node_time_residual_overhead_h'] = node_perf_df['node_time_residual_overhead_s'] / (60**2)

  overhead_cols = [
      'avg_total_segment_time_s',
      'runner_overhead_s',
      'cycle_overhead_s',
      'node_time_overhead_s',
      'node_time_residual_overhead_s',
      'node_time_overhead_h',
      'node_time_residual_overhead_h',
  ]


  # calculate the percentage of total time spent not doing MD
  node_perf_df['md_fraction'] = node_perf_df['avg_total_segment_time_s'] /\
                                node_perf_df['avg_cycle_time_s']

  node_perf_df['wepy_fraction'] = node_perf_df['cycle_overhead_s'] /\
                                  node_perf_df['avg_cycle_time_s']

  node_perf_df['runner_fraction'] = node_perf_df['runner_overhead_s'] /\
                                    node_perf_df['avg_cycle_time_s']

  node_perf_df['resampling_fraction'] = node_perf_df['avg_resampling_time_s'] /\
                                        node_perf_df['avg_cycle_time_s']

  node_perf_df['bc_fraction'] = node_perf_df['avg_bc_time_s'] /\
                                    node_perf_df['avg_cycle_time_s']


  fractions_cols = [
      'md_fraction',
      'wepy_fraction',
      'runner_fraction',
      'resampling_fraction',
      'bc_fraction'
  ]

  # calculate the total slowdown
  node_perf_df['slowdown'] = 1 / node_perf_df['md_fraction']

#+end_src

***** contig table
| contig | n_runs | n_cycles | n_exits |  exit weight | walker time (us) | sampling time (us) |
|--------+--------+----------+---------+--------------+------------------+--------------------|
|      0 |      6 |     2766 |      42 | 3.437180e-09 |          0.05532 |            2.65536 |
|      1 |      3 |     3042 |     120 | 8.398339e-06 |          0.06084 |            2.92032 |
|      2 |      3 |     3084 |      50 | 7.707469e-10 |          0.06168 |            2.96064 |
|      3 |      3 |     3548 |     110 | 1.299993e-07 |          0.07096 |            3.40608 |
|      4 |      3 |     2089 |     129 | 6.605302e-09 |          0.04178 |            2.00544 |
|      5 |      3 |     2474 |      42 | 1.036143e-08 |          0.04948 |            2.37504 |

***** performace table

Performance times:

| node_type | n_runs |    n_cycles | avg_runner_time_s | avg_bc_time_s | avg_resampling_time_s | avg_cycle_time_s | avg_segment_time_s | avg_sampling_time_ns |
|-----------+--------+-------------+-------------------+---------------+-----------------------+------------------+--------------------+----------------------|
| lac       |    3.0 |  820.333333 |        659.333333 |     36.000000 |                   2.0 |       698.333333 |          71.000000 |            16.406667 |
| nvl       |    3.0 | 1787.333333 |        261.333333 |     33.000000 |                   2.0 |       297.333333 |           7.000000 |            35.746667 |
|-----------+--------+-------------+-------------------+---------------+-----------------------+------------------+--------------------+----------------------|
| speedup   |        |    2.178789 |          2.522959 |      1.090909 |                   1.0 |         2.348655 |          10.142857 |             2.178789 |




Throughput:

| node_type | ns_per_node_week | ns_per_node_day | ns_per_node_hour | ns_per_gpu_week | ns_per_gpu_day | ns_per_gpu_hour |
|-----------+------------------+-----------------+------------------+-----------------+----------------+-----------------|
| lac       |        16.406667 |        2.343810 |         0.097659 |        2.050833 |       0.292976 |        0.012207 |
| nvl       |        35.746667 |        5.106667 |         0.212778 |        4.468333 |       0.638333 |        0.026597 |



Slowdowns:

| node_type | md_fraction | wepy_fraction | runner_fraction | resampling_fraction | bc_fraction |
|-----------+-------------+---------------+-----------------+---------------------+-------------|
| lac       |    0.610024 |      0.389976 |        0.334129 |            0.002864 |    0.051551 |
| nvl       |    0.141256 |      0.858744 |        0.737668 |            0.006726 |    0.110987 |

 cycle time fraction &       MD & wepy total &   runner & resampling &       bc \\
 lac                 & 0.610024 &   0.389976 & 0.334129 &   0.002864 & 0.051551 \\
 nvl                 & 0.141256 &   0.858744 & 0.737668 &   0.006726 & 0.110987 \\



*** 10
**** tree

start, end

- root :: d0cb2e6fbcc8c2d66d67c845120c7f6b (patched: d0cb2e6fbcc8c2d66d67c845120c7f6b)
  - 0
    - 0-0 :: b4b96580ae57f133d5f3b6ce25affa6d
    - 0-1 :: 6184f17f627cab5741553985f3575eec
    - 0-2 :: a638f63fe080f144ad8e32d67159b6ee
  - 1
    - 1-0 :: 12b9c5a180a98408fa2c234ffe59eebd
    - 1-1 :: 79dd6ed530ede77d15fcb34de5882bb2
    - 1-2 :: 2bf725651a9ef70d702b780889683c44
  - 2
    - 2-0 :: 0bfd845cc0320fd13bec1643f3bcbae3
    - 2-1 :: e857df19793ec1a64e1aba7a7ad823e4
    - 2-2 :: 7fc189e9007530fac4fd43d2634cf91a
  - 3
    - 3-0 :: 1b2c3187aff093ae84f7d3028840c20b
    - 3-1 :: b15381dbf86aa763db7d80bbc3b4cfb7
    - 3-2 :: 1b031163f3b0ab0b2046199da4c63749
  - 4
    - 4-0 :: 2ad5ec161f863dbed3c553805dfc2f17
    - 4-1 :: 7973d300b135003c6c22c838101e7eb7
    - 4-2 :: 67878e0214eaae19ef3a799d47d30bd1
  - 5
    - 5-0 :: 6002c812ed001eaaa022aa429290193e
    - 5-1 :: 5809528296b02b840f6c3376a5b7f431
    - 5-2 :: 182847f90f2ace68fafea3c6514feebe

**** table
     :PROPERTIES:
     :TABLE_EXPORT_FILE: data/sim_management/lig_10.csv
     :TABLE_EXPORT_FORMAT: orgtbl-to-csv
     :END:


| end_hash                         | contig_ID |   job_ID | node    | state    | tag | cycles | exits | keep | success | reconciled | hdf5-join | backedup | start_hash                       | patched_start_hash               |
|----------------------------------+-----------+----------+---------+----------+-----+--------+-------+------+---------+------------+-----------+----------+----------------------------------+----------------------------------|
| b4b96580ae57f133d5f3b6ce25affa6d |       0-0 | 64809231 | nvl-002 | FINISHED |     |    977 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
| 12b9c5a180a98408fa2c234ffe59eebd |       1-0 | 64809232 | nvl-003 | FINISHED |     |    968 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
| 0bfd845cc0320fd13bec1643f3bcbae3 |       2-0 | 64809233 | lac-025 | FINISHED |     |    724 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
| 1b2c3187aff093ae84f7d3028840c20b |       3-0 | 64809234 | lac-026 | FINISHED |     |    732 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
| 2ad5ec161f863dbed3c553805dfc2f17 |       4-0 | 64809235 | lac-028 | FINISHED |     |    714 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
| 6002c812ed001eaaa022aa429290193e |       5-0 | 64809236 | lac-029 | FINISHED |     |    736 |     0 | yes  | yes     | yes        |           | yes      | d0cb2e6fbcc8c2d66d67c845120c7f6b | d0cb2e6fbcc8c2d66d67c845120c7f6b |
|----------------------------------+-----------+----------+---------+----------+-----+--------+-------+------+---------+------------+-----------+----------+----------------------------------+----------------------------------|
| 6184f17f627cab5741553985f3575eec |       0-1 | 65848495 | nvl-003 | FINISHED |     |    954 |    38 | yes  | yes     | yes        |           | yes      | b4b96580ae57f133d5f3b6ce25affa6d | a7ba8ac66aec76176ba312e1d8fc66ed |
| 79dd6ed530ede77d15fcb34de5882bb2 |       1-1 | 65848496 | nvl-002 | FINISHED |     |   1040 |     0 | yes  | yes     | yes        |           | yes      | 12b9c5a180a98408fa2c234ffe59eebd | 7d9421d16205d8ade168d1887a726e33 |
| e857df19793ec1a64e1aba7a7ad823e4 |       2-1 | 65848497 | nvl-004 | FINISHED |     |    983 |     0 | yes  | yes     | yes        |           | yes      | 0bfd845cc0320fd13bec1643f3bcbae3 | 879fb3f6dbdab3ddfd12f4418fb0a0d1 |
| b15381dbf86aa763db7d80bbc3b4cfb7 |       3-1 | 65848499 | lac-029 | FINISHED |     |    752 |     0 | yes  | yes     | yes        |           | yes      | 1b2c3187aff093ae84f7d3028840c20b | daa9bf61e8c7aff8c683451ddfea13bf |
| 7973d300b135003c6c22c838101e7eb7 |       4-1 | 65848500 | lac-026 | FINISHED |     |    758 |     0 | yes  | yes     | yes        |           | yes      | 2ad5ec161f863dbed3c553805dfc2f17 | edc7f123a95c66883d3460fd3d1c7abb |
| 5809528296b02b840f6c3376a5b7f431 |       5-1 | 65848501 | lac-193 | FINISHED |     |    752 |     0 | yes  | yes     | yes        |           | yes      | 6002c812ed001eaaa022aa429290193e | 8babb3297ab90bee7ee862ce30115fdb |
|----------------------------------+-----------+----------+---------+----------+-----+--------+-------+------+---------+------------+-----------+----------+----------------------------------+----------------------------------|
| a638f63fe080f144ad8e32d67159b6ee |       0-2 | 66808172 | nvl-002 | FINISHED |     |   1058 |    27 | yes  | yes     |            |           |          | 6184f17f627cab5741553985f3575eec | db5a33b04125e427474749f7ce4ad847 |
| 2bf725651a9ef70d702b780889683c44 |       1-2 | 66808173 | lac-028 | FINISHED |     |    746 |     0 | yes  | yes     |            |           |          | 79dd6ed530ede77d15fcb34de5882bb2 | 0f3c1522591cbdcc2592ddba5bada3dc |
| 7fc189e9007530fac4fd43d2634cf91a |       2-2 | 66808174 | lac-027 | FINISHED |     |    749 |     0 | yes  | yes     |            |           |          | e857df19793ec1a64e1aba7a7ad823e4 | b9f8239b2849e3a1480789f0aef5d17b |
| 1b031163f3b0ab0b2046199da4c63749 |       3-2 | 66808175 | lac-029 | FINISHED |     |    743 |     0 | yes  | yes     |            |           |          | b15381dbf86aa763db7d80bbc3b4cfb7 | 3d19f81871585d9e6d454dfdfd71d375 |
| 67878e0214eaae19ef3a799d47d30bd1 |       4-2 | 66808176 | nvl-003 | FINISHED |     |    981 |     0 | yes  | yes     |            |           |          | 7973d300b135003c6c22c838101e7eb7 | 302cde79e6d9a37969910c8526343b5c |
| 182847f90f2ace68fafea3c6514feebe |       5-2 | 66808177 | nvl-004 | FINISHED |     |    985 |     0 | yes  | yes     |            |           |          | 5809528296b02b840f6c3376a5b7f431 | d6516716fd9744c85b581feb7d1a3c7e |
|----------------------------------+-----------+----------+---------+----------+-----+--------+-------+------+---------+------------+-----------+----------+----------------------------------+----------------------------------|



**** Notes

***** 64379712

No logs were made after the beginning (including the dashboard) but it
looks like the HDF5 was made. IDK I hate HPCC...

The size indicates that its much smaller though so IDK. I didn't check stdout log though.

***** Patch hashes
1484a6020285851e621e46f8da38ecca, is shown as start hash in 0-0, 1-0, 2-0, 4-0

From a stdout log after patching:

#+begin_example
Reading and deserializing snapshot
Original starting snapshot hashed as: 6a33f4a4746dc7a1e83296a727c1f8b5
Applying patch WepySimApparatus
Applying patch OpenMMRunner.platform_kwargs
Applying patch OpenMMRunner.getState_kwargs
Applying patch UnbindingBC._mdj_top
Applying patch UnbindingBC._periodic
Finished patching, serializing snapshot...
Patched starting snapshot hash is: 1484a6020285851e621e46f8da38ecca
6a33f4a4746dc7a1e83296a727c1f8b5 --> 1484a6020285851e621e46f8da38ecca
Finished. Serializing and writing output snapshot to: 6a33f4a4746dc7a1e83296a727c1f8b5.snap.dill.pkl
Finished with patch script
#+end_example

So yes that is the right hash


**** Old Notes
***** 60958966

Nothing interesting in the log. The walkers just get shut down...

***** 60959711

OOM message

this is in the logs
#+begin_example
slurmstepd: error: Detected 1 oom-kill event(s) in step 60959711.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: nvl-003: task 0: Out Of Memory
slurmstepd: error: Detected 1 oom-kill event(s) in step 60959711.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
#+end_example

***** 60959712

OOM message

in the error log.

#+begin_example
slurmstepd: error: Detected 1 oom-kill event(s) in step 60959712.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: nvl-004: task 0: Out Of Memory
slurmstepd: error: Detected 1 oom-kill event(s) in step 60959712.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
#+end_example

***** 2 fs
***** 64187173

#+begin_example
CRITICAL:root:OpenMM_GPU_Walker_Task-41: Exception 'Exception(The periodic box size has decreased to less than twice the nonbonded cutoff.)' caught in a task.
Traceback:
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/task_mapper.py", line 413, in _run_task
    result = self.run_task(task)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 1345, in run_task
    platform_kwargs=platform_options,
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/mapper.py", line 229, in __call__
    return self.func(*self.args, **self.kwargs, **worker_kwargs)
  File "<boltons.funcutils.FunctionBuilder-13>", line 2, in run_segment
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/eliot/_action.py", line 924, in logging_wrapper
    result = wrapped_function(*args, **kwargs)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 445, in run_segment
    simulation.step(segment_length)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 132, in step
    self._simulate(endStep=self.currentStep+steps)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 197, in _simulate
    self.integrator.step(10) # Only take 10 steps at a time, to give Python more chances to respond to a control-c.
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/openmm.py", line 19017, in step
    return _openmm.LangevinIntegrator_step(self, steps)
Exception: The periodic box size has decreased to less than twice the nonbonded cutoff.
#+end_example

***** 63844030

Error:

#+begin_example
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/task_mapper.py", line 413, in _run_task
    result = self.run_task(task)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 1345, in run_task
    platform_kwargs=platform_options,
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/mapper.py", line 229, in __call__
    return self.func(*self.args, **self.kwargs, **worker_kwargs)
  File "<boltons.funcutils.FunctionBuilder-13>", line 2, in run_segment
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/eliot/_action.py", line 924, in logging_wrapper
    result = wrapped_function(*args, **kwargs)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 445, in run_segment
    simulation.step(segment_length)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 132, in step
    self._simulate(endStep=self.currentStep+steps)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 197, in _simulate
    self.integrator.step(10) # Only take 10 steps at a time, to give Python more chances to respond to a control-c.
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/openmm.py", line 19017, in step
    return _openmm.LangevinIntegrator_step(self, steps)
Exception: The periodic box size has decreased to less than twice the nonbonded cutoff.

--------------------------------------------------------------------------------
#+end_example

***** 63844032

Error:

#+begin_example
CRITICAL:root:OpenMM_GPU_Walker_Task-17: Exception 'Exception(The periodic box size has decreased to less than twice the nonbonded cutoff.)' caught in a task.
Traceback:
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/task_mapper.py", line 413, in _run_task
    result = self.run_task(task)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 1345, in run_task
    platform_kwargs=platform_options,
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/mapper.py", line 229, in __call__
    return self.func(*self.args, **self.kwargs, **worker_kwargs)
  File "<boltons.funcutils.FunctionBuilder-13>", line 2, in run_segment
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/eliot/_action.py", line 924, in logging_wrapper
    result = wrapped_function(*args, **kwargs)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 445, in run_segment
    simulation.step(segment_length)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 132, in step
    self._simulate(endStep=self.currentStep+steps)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 197, in _simulate
    self.integrator.step(10) # Only take 10 steps at a time, to give Python more chances to respond to a control-c.
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/openmm.py", line 19017, in step
    return _openmm.LangevinIntegrator_step(self, steps)
Exception: The periodic box size has decreased to less than twice the nonbonded cutoff.

--------------------------------------------------------------------------------
#+end_example




***** 63844028

#+begin_example
CRITICAL:root:OpenMM_GPU_Walker_Task-23: Exception 'Exception(First periodic box vector must be parallel to x.)' caught in a task.
Traceback:
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/task_mapper.py", line 413, in _run_task
    result = self.run_task(task)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 1345, in run_task
    platform_kwargs=platform_options,
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/work_mapper/mapper.py", line 229, in __call__
    return self.func(*self.args, **self.kwargs, **worker_kwargs)
  File "<boltons.funcutils.FunctionBuilder-13>", line 2, in run_segment
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/eliot/_action.py", line 924, in logging_wrapper
    result = wrapped_function(*args, **kwargs)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/devel/wepy/src/wepy/runners/openmm.py", line 445, in run_segment
    simulation.step(segment_length)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 132, in step
    self._simulate(endStep=self.currentStep+steps)
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/app/simulation.py", line 197, in _simulate
    self.integrator.step(10) # Only take 10 steps at a time, to give Python more chances to respond to a control-c.
  File "/mnt/gs18/scratch/users/lotzsamu/tree/lab/projects/seh.pathway_hopping/_conda_envs/sims_dev/lib/python3.7/site-packages/simtk/openmm/openmm.py", line 19017, in step
    return _openmm.LangevinIntegrator_step(self, steps)
Exception: First periodic box vector must be parallel to x.

--------------------------------------------------------------------------------
#+end_example

**** Old tables

***** COMMENT 1 fs, 5 picosecond cycle, batch

starting hash: d0cb2e6fbcc8c2d66d67c845120c7f6b

| end_hash | contig_ID |   job_ID | node    | state    | tag | cycles | keep | success | reconciled | backedup | exits | start_hash                       | patched_start_hash               |
|----------+-----------+----------+---------+----------+-----+--------+------+---------+------------+----------+-------+----------------------------------+----------------------------------|
|          |       0-0 | 64379711 | nvl-003 | FINISHED |     |        |      |         |            |          |       | 6a33f4a4746dc7a1e83296a727c1f8b5 | 1484a6020285851e621e46f8da38ecca |
|          |       1-0 | 64379712 | lac-027 | FINISHED |     |        |      |         |            |          |       | 7a33f4a4746dc7a1e83296a727c1f8b5 | 1485a6020285851e621e46f8da38ecca |
|          |       2-0 | 64379713 | lac-026 | FINISHED |     |        |      |         |            |          |       | 8a33f4a4746dc7a1e83296a727c1f8b5 | 1486a6020285851e621e46f8da38ecca |
|          |       4-0 | 64379714 | lac-029 | FINISHED |     |        |      |         |            |          |       | 9a33f4a4746dc7a1e83296a727c1f8b5 | 1487a6020285851e621e46f8da38ecca |
|          |       3-0 | 64379715 | lac-025 | FINISHED |     |        |      |         |            |          |       |                                  |                                  |
|          |       5-0 | 64379716 | nvl-002 | FINISHED |     |        |      |         |            |          |       |                                  |                                  |


***** COMMENT 2 fs batch

Starting hash: 6a33f4a4746dc7a1e83296a727c1f8b5

| end_hash                         | contig_ID |   job_ID | node    | state     | tag                          | cycles | keep | success | reconciled | backedup | exits | start_hash                       | patched_start_hash               |
|----------------------------------+-----------+----------+---------+-----------+------------------------------+--------+------+---------+------------+----------+-------+----------------------------------+----------------------------------|
| c9bf9f9d31df180f6fc36b4750198a30 |       0-0 | 63839926 | lac-025 | COMPLETED | lig-10_contig-0-0_production |        |      |         |            |          |       | 6a33f4a4746dc7a1e83296a727c1f8b5 | 1484a6020285851e621e46f8da38ecca |
| 1609a496b74d4d84efaf6fed95bac175 |       1-0 | 64030030 | nvl-002 | COMPLETED | lig-10_contig-1-0_production |        |      |         |            |          |       | 7a33f4a4746dc7a1e83296a727c1f8b5 | 1485a6020285851e621e46f8da38ecca |
| 05e50d7731ff33a4d2406dfd1396db56 |       2-0 | 63844029 | nvl-003 | COMPLETED | lig-10_contig-2-0_production |        |      |         |            |          |       | 8a33f4a4746dc7a1e83296a727c1f8b5 | 1486a6020285851e621e46f8da38ecca |
| 25fa2241255e90e2a78c334dd591a183 |       4-0 | 63844031 | lac-026 | FAILED    | lig-10_contig-4-0_production |        |      |         |            |          |       | 9a33f4a4746dc7a1e83296a727c1f8b5 | 1487a6020285851e621e46f8da38ecca |
|----------------------------------+-----------+----------+---------+-----------+------------------------------+--------+------+---------+------------+----------+-------+----------------------------------+----------------------------------|
|                                  |       3-0 | 64187172 | lac-027 | RUNNING   | lig-10_contig-3-0_production |        |      |         |            |          |       |                                  |                                  |
|----------------------------------+-----------+----------+---------+-----------+------------------------------+--------+------+---------+------------+----------+-------+----------------------------------+----------------------------------|
|                                  |       5-0 | 64187173 | lac-028 | FAILED    | lig-10_contig-5-0_production |        |      |         |            |          |       |                                  |                                  |
|                                  |       3-0 | 63844030 | nvl-004 | FAILED    | lig-10_contig-3-0_production |        |      |         |            |          |       |                                  |                                  |
|                                  |       5-0 | 63844032 | lac-027 | FAILED    | lig-10_contig-5-0_production |        |      |         |            |          |       |                                  |                                  |
|                                  |       1-0 | 63844028 | nvl-002 | FAILED    | lig-10_contig-1-0_production |        |      |         |            |          |       |                                  |                                  |





*** 17

**** Current <2020-08-28 Fri>

***** tree

- root :: 9096091b25b2c978354b32f348fb2c71 (patched: yyy)
  - 0
    - 0-0 :: bb375a77d0065cdd4e8b86eb05a2cdde
    - 0-1 :: 381489ed16d72d74c9a3a30ea1d6f2b7
    - 0-2 :: 022637501a9ab0608fd25482e47b0299
  - 1
    - 1-0 :: f97c14eb333f86e24d92f760780294af
    - 1-1 :: 165370e5502ff14a337fb9b1014b4136
    - 1-2 :: 2f24aaa861680b6504ab2cd8e7041fad
  - 2
    - 2-0 :: 2451389c1a1d8af39fea7f16977f4e13
    - 2-1 :: d9dda1af96355f23dc8f3e5996512f5a
    - 2-2 :: 11dd2db78c3e7c9fc45ad14512ef26bf
  - 3
    - 3-0 :: 5a5ceb0e0dc581460d4d36f9accc2451
    - 3-1 :: ec044c9a6cbef04a9ee75e2b36d2ff7a
    - 3-2 :: 1f3144ad2a89337dcce408452bb27d3c
  - 4
    - 4-0 :: 1bdd219873e7163dccfb9f97be1e92eb
    - 4-1 :: 780a49884be084bcb2328298da863806
    - 4-2 :: 70dcd6563aea3385a8076f3e6a469009
  - 5
    - 5-0 :: f2cb52a2072e7e9ecb8097de1262c435
    - 5-1 :: 6268c249d2c5a88255b6b45ff8ff2e4d
    - 5-2 :: 8e0fc8e25c66535a87758b63640c96d6


***** table
     :PROPERTIES:
     :TABLE_EXPORT_FILE: data/sim_management/lig_17.csv
     :TABLE_EXPORT_FORMAT: orgtbl-to-csv
     :END:


|  job_ID | patched_start_hash               | end_hash                         | start_hash                       | contig_ID | node    | state    |   pmin | tag | cycles | exits | keep | success | reconciled | hdf5-join | backedup |
|---------+----------------------------------+----------------------------------+----------------------------------+-----------+---------+----------+--------+-----+--------+-------+------+---------+------------+-----------+----------|
| 3443135 | 9096091b25b2c978354b32f348fb2c71 | bb375a77d0065cdd4e8b86eb05a2cdde | 9096091b25b2c978354b32f348fb2c71 |       0-0 | lac-025 | FINISHED |  1e-12 |     |    687 |     0 | yes  | yes     |            |           |          |
| 3543550 | 9096091b25b2c978354b32f348fb2c71 | f97c14eb333f86e24d92f760780294af | 9096091b25b2c978354b32f348fb2c71 |       1-0 | nvl-004 | FINISHED |  1e-12 |     |   1763 |     0 | yes  | yes     |            |           |          |
| 3543551 | 9096091b25b2c978354b32f348fb2c71 | 2451389c1a1d8af39fea7f16977f4e13 | 9096091b25b2c978354b32f348fb2c71 |       2-0 | nvl-002 | FINISHED |  1e-12 |     |    820 |     0 | yes  | yes     |            |           |          |
| 3543552 | 9096091b25b2c978354b32f348fb2c71 | 5a5ceb0e0dc581460d4d36f9accc2451 | 9096091b25b2c978354b32f348fb2c71 |       3-0 | nvl-003 | FINISHED |  1e-12 |     |   1601 |     0 | yes  | yes     |            |           |          |
| 3543553 | 9096091b25b2c978354b32f348fb2c71 | 1bdd219873e7163dccfb9f97be1e92eb | 9096091b25b2c978354b32f348fb2c71 |       4-0 | lac-027 | FINISHED |  1e-12 |     |    684 |     0 | yes  | yes     |            |           |          |
| 3543554 | 9096091b25b2c978354b32f348fb2c71 | f2cb52a2072e7e9ecb8097de1262c435 | 9096091b25b2c978354b32f348fb2c71 |       5-0 | lac-028 | FINISHED |  1e-12 |     |    672 |     0 | yes  | yes     |            |           |          |
|---------+----------------------------------+----------------------------------+----------------------------------+-----------+---------+----------+--------+-----+--------+-------+------+---------+------------+-----------+----------|
| 3833877 | 60c5312a7a0676c8b4b08d4dd8aa8ca7 | 381489ed16d72d74c9a3a30ea1d6f2b7 | bb375a77d0065cdd4e8b86eb05a2cdde |       0-1 | nvl-002 | FINISHED |  1e-12 |     |    882 |     1 | yes  | yes     |            |           |          |
| 3833882 | da482a8e0ac5b467629fdb258641fed4 | 165370e5502ff14a337fb9b1014b4136 | f97c14eb333f86e24d92f760780294af |       1-1 | lac-027 | FINISHED |  1e-12 |     |    634 |     0 | yes  | yes     |            |           |          |
| 3833880 | 4eb04b4089eec8f1f43c0b30263be044 | d9dda1af96355f23dc8f3e5996512f5a | 2451389c1a1d8af39fea7f16977f4e13 |       2-1 | lac-028 | FINISHED |  1e-12 |     |    643 |     0 | yes  | yes     |            |           |          |
| 3833883 | 1dd0d2d0c0a691afd6cc9d8e15b5ef39 | ec044c9a6cbef04a9ee75e2b36d2ff7a | 5a5ceb0e0dc581460d4d36f9accc2451 |       3-1 | lac-029 | FINISHED |  1e-12 |     |    634 |     1 | yes  | yes     |            |           |          |
| 3833885 | 7b63f92ff8e76096084e1759b2b8f6fa | 780a49884be084bcb2328298da863806 | 1bdd219873e7163dccfb9f97be1e92eb |       4-1 | nvl-003 | FINISHED |  1e-12 |     |   1599 |     0 | yes  | yes     |            |           |          |
| 3833886 | 128b4e86d8683ae702a0fd5a0a1ba0fb | 6268c249d2c5a88255b6b45ff8ff2e4d | f2cb52a2072e7e9ecb8097de1262c435 |       5-1 | lac-025 | FINISHED |  1e-12 |     |    661 |     6 | yes  | yes     |            |           |          |
|---------+----------------------------------+----------------------------------+----------------------------------+-----------+---------+----------+--------+-----+--------+-------+------+---------+------------+-----------+----------|
| 4961500 | 43a78f2eb03b325dabe917f3a69baf89 | 022637501a9ab0608fd25482e47b0299 | 381489ed16d72d74c9a3a30ea1d6f2b7 |       0-2 | lac-143 | FINISHED | 11e-16 |     |    596 |    16 | yes  | yes     |            |           |          |
| 4961503 | 82b15e2487e961f282d606898afd6c33 | 2f24aaa861680b6504ab2cd8e7041fad | 165370e5502ff14a337fb9b1014b4136 |       1-2 | lac-025 | FINISHED | 11e-16 |     |    617 |     0 | yes  | yes     |            |           |          |
| 4961502 | 40edf9916e05da7c573943bc2d0041db | 11dd2db78c3e7c9fc45ad14512ef26bf | d9dda1af96355f23dc8f3e5996512f5a |       2-2 | lac-029 | FINISHED | 11e-16 |     |    603 |    12 | yes  | yes     |            |           |          |
| 4961505 | 5b8e612f27e94573177c2db3d71df30c | 1f3144ad2a89337dcce408452bb27d3c | ec044c9a6cbef04a9ee75e2b36d2ff7a |       3-2 | lac-028 | FINISHED | 11e-16 |     |    615 |     0 | yes  | yes     |            |           |          |
| 4961506 | 458c2afff38211f5f760f6bb2ad04e4e | 70dcd6563aea3385a8076f3e6a469009 | 780a49884be084bcb2328298da863806 |       4-2 | lac-027 | FINISHED | 11e-16 |     |    611 |     0 | yes  | yes     |            |           |          |
| 4961508 | 0d3697d0dcfb2b49d45c68acacdf9eaf | 8e0fc8e25c66535a87758b63640c96d6 | 6268c249d2c5a88255b6b45ff8ff2e4d |       5-2 | lac-348 | FINISHED | 11e-16 |     |    601 |    22 | yes  | yes     |            |           |          |
|---------+----------------------------------+----------------------------------+----------------------------------+-----------+---------+----------+--------+-----+--------+-------+------+---------+------------+-----------+----------|


***** Notes



**** Old
***** tree

- root :: 
  - 0
    - 0-0 :: 
  - 1
    - 1-0 :: 
  - 2
    - 2-0 :: 
  - 3
    - 3-0 :: 
  - 4
    - 4-0 :: 
  - 5
    - 5-0 :: 

***** table

| ! | run_idx | end_hash | contig_ID |   job_ID | state                  | keep | copied | success | cycles | exits | node | backedup | reconciled | exit weight | rate | restime |
|---+---------+----------+-----------+----------+------------------------+------+--------+---------+--------+-------+------+----------+------------+-------------+------+---------|
|   |         |          |       0-0 | 45911223 | FINISHED               | yes  | na     | yes     |    831 |     2 |      |          |            |             |      |         |
|   |         |          |       2-0 | 45911226 | FINISHED               | yes  | na     | yes     |    833 |     0 |      |          |            |             |      |         |
|   |         |          |       5-0 | 45911229 | FINISHED               | yes  | na     | yes     |    832 |     0 |      |          |            |             |      |         |
|---+---------+----------+-----------+----------+------------------------+------+--------+---------+--------+-------+------+----------+------------+-------------+------+---------|
|   |         |          |       2-0 | 45911227 | RUNNING,STALLED/BLOWUP | no   |        |         |        |       |      |          |            |             |      |         |
|   |         |          |       4-0 | 45911228 | FINISHED               | no   | na     | no      |   1200 |     1 |      |          |            |             |      |         |
|   |         |          |       1-0 | 47140978 | STALLED                | no   |        | no      |        |       |      |          |            |             |      |         |

#+TBLFM: $17=1/$rate



***** notes


*** 18

**** Current <2020-08-25 Tue>

***** tree

- root :: 2348d3b3c4c34fe405282157614d7bb8 (patched: 2348d3b3c4c34fe405282157614d7bb8)
  - 0
    - 0-0 :: 5ec12b6684505a507c2b349b9909d62d
    - 0-1 :: 5d4dd14c7746f87632393e5f664802f1
    - 0-2 :: 077cb438fa658a812f2e387564c012d2
  - 1
    - 1-0 :: 235f74cd69e33796c55e22bd25a13074
    - 1-1 :: 0a570c199fca47986fb613709cf8b597
    - 1-2 :: eb50b51892a9d59041171c30a55a60b2
  - 2
    - 2-0 :: e25400cdcdcf1b1a6edfd53cd14ac8cf
    - 2-1 :: f15c52ea4f35e1171d45423eb32e30e4
    - 2-2 :: 8e1505cb78145e27f2a447d20d82ff37
  - 3
    - 3-0 :: 16d9df4ba299d6a0fa35bca0f6a19a5d
    - 3-1 :: 1bc951bb16776609ff4c3643d2a2d9bf
    - 3-2 :: 7292a7a380361f14f423cf4a52361bca
  - 4
    - 4-0 :: c7c1b71673e8aa6cf46d3acd9472a16c
    - 4-1 :: 9529a931658f358b902cded4d34d56e5
    - 4-2 :: 9ae1bed65b880742cdcaf85245957110
  - 5
    - 5-0 :: b66aa93210241e691dd0159645a6724f
    - 5-1 :: 2e1dd92c6ccf3de5ca04461822ce0b51
    - 5-2 :: 65a02dc9a42ae020f28133b397b589e5


***** table
     :PROPERTIES:
     :TABLE_EXPORT_FILE: data/sim_management/lig_18.csv
     :TABLE_EXPORT_FORMAT: orgtbl-to-csv
     :END:


|  job_ID | patched_start_hash               | end_hash                         | contig_ID | node    | state    |  pmin | tag | cycles | exits | keep | success | reconciled | backedup | start_hash                       |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 3077399 | 2348d3b3c4c34fe405282157614d7bb8 | 5ec12b6684505a507c2b349b9909d62d |       0-0 | nvl-002 | FINISHED | 1e-12 |     |    795 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
| 3077400 | 2348d3b3c4c34fe405282157614d7bb8 | 235f74cd69e33796c55e22bd25a13074 |       1-0 | nvl-003 | FINISHED | 1e-12 |     |   1552 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
| 3546054 | 63e993710ae8eea759394455c1b334b2 | e25400cdcdcf1b1a6edfd53cd14ac8cf |       2-0 | lac-029 | FINISHED | 1e-12 |     |    674 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
| 3077402 | 2348d3b3c4c34fe405282157614d7bb8 | 16d9df4ba299d6a0fa35bca0f6a19a5d |       3-0 | lac-028 | FINISHED | 1e-12 |     |    696 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
| 3077403 | 2348d3b3c4c34fe405282157614d7bb8 | c7c1b71673e8aa6cf46d3acd9472a16c |       4-0 | lac-029 | FINISHED | 1e-12 |     |    675 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
| 3077404 | 2348d3b3c4c34fe405282157614d7bb8 | b66aa93210241e691dd0159645a6724f |       5-0 | lac-025 | FINISHED | 1e-12 |     |    674 |     0 | yes  | yes     | yes        | yes      | 2348d3b3c4c34fe405282157614d7bb8 |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 3546055 | 8a5591e9cf4c95776e4b1ac4f578f47b | 5d4dd14c7746f87632393e5f664802f1 |       0-1 | lac-025 | FINISHED | 1e-12 |     |    663 |     0 | yes  | yes     | yes        | yes      | 5ec12b6684505a507c2b349b9909d62d |
| 3546056 | e702de8835461c751bfb0790e1ff1e81 | 0a570c199fca47986fb613709cf8b597 |       1-1 | lac-292 | FINISHED | 1e-12 |     |    647 |     0 | yes  | yes     | yes        | yes      | 235f74cd69e33796c55e22bd25a13074 |
| 4961489 | 5ad26ee0faea3979b880dd8ea47b2bda | f15c52ea4f35e1171d45423eb32e30e4 |       2-1 | lac-197 | FINISHED | 1e-16 |     |    601 |     0 | yes  | yes     | yes        | yes      | e25400cdcdcf1b1a6edfd53cd14ac8cf |
| 3546057 | 1bbd943cacb998379ee693484a9564d3 | 1bc951bb16776609ff4c3643d2a2d9bf |       3-1 | lac-197 | FINISHED | 1e-12 |     |    654 |     0 | yes  | yes     | yes        | yes      | 16d9df4ba299d6a0fa35bca0f6a19a5d |
| 3546058 | 6446f2f53410f1cacf9d5302a802db19 | 9529a931658f358b902cded4d34d56e5 |       4-1 | nvl-006 | FINISHED | 1e-12 |     |   1608 |     0 | yes  | yes     | yes        | yes      | c7c1b71673e8aa6cf46d3acd9472a16c |
| 3546059 | be7a32c9b2afb763723df20abb824243 | 2e1dd92c6ccf3de5ca04461822ce0b51 |       5-1 | lac-288 | FINISHED | 1e-12 |     |    640 |     0 | yes  | yes     | yes        | yes      | b66aa93210241e691dd0159645a6724f |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 4961521 | 41081f59da077f9ae5ba8cff45db5dce | 077cb438fa658a812f2e387564c012d2 |       0-2 | lac-289 | FINISHED | 1e-16 |     |    616 |     0 | yes  | yes     | yes        |          | 5d4dd14c7746f87632393e5f664802f1 |
| 4961522 | 9e1246bfec7296b422fee1a40e4ebf2a | eb50b51892a9d59041171c30a55a60b2 |       1-2 | lac-195 | FINISHED | 1e-16 |     |    611 |     0 | yes  | yes     | yes        |          | 0a570c199fca47986fb613709cf8b597 |
| 5444902 | 0502137e831ac7c829813e47b769834f | 8e1505cb78145e27f2a447d20d82ff37 |       2-2 | lac-025 | FINISHED | 1e-16 |     |    618 |     0 | yes  | yes     | yes        |          | f15c52ea4f35e1171d45423eb32e30e4 |
| 4961523 | 0044c9f36a2b4a021d024db5c97063be | 7292a7a380361f14f423cf4a52361bca |       3-2 | lac-287 | FINISHED | 1e-16 |     |    606 |     0 | yes  | yes     | yes        |          | 1bc951bb16776609ff4c3643d2a2d9bf |
| 4961524 | 61222b6ba0d7e8d6ee19df61112d5a3c | 9ae1bed65b880742cdcaf85245957110 |       4-2 | lac-029 | FINISHED | 1e-16 |     |    612 |     0 | yes  | yes     | yes        |          | 9529a931658f358b902cded4d34d56e5 |
| 4961525 | fa552837c5e37672994c887d6d602282 | 65a02dc9a42ae020f28133b397b589e5 |       5-2 | lac-196 | FINISHED | 1e-16 |     |    609 |     0 | yes  | yes     | yes        |          | 2e1dd92c6ccf3de5ca04461822ce0b51 |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|


***** Notes

For Job 3546054 I have no idea why the start hash is that. But thats
what it is in the orchestrator... not a good sign but I'll roll with
it.

**** OLD
***** tree

- new root :: 72af38a7e982615cc0bf1bc70b501494
- root :: 84de06ee42824b533e2289f6962cf289
  - 0
    - 0-0 :: 65467c8e0531d84351e246db7b3bf46f
  - 1
    - 1-0 :: ba08d22cd1fc2f11b8871582fd5ae28d
  - 2
    - 2-0 :: 1ebba9d011b7af55fa637f86e77d3e7e
  - 3
    - 3-0 :: 
  - 4
    - 4-0 :: 
  - 5
    - 5-0 :: a08821e60b00cf8817abccdada49ef7f


***** table

| ! | run_idx | end_hash                         | contig_ID |   job_ID | state            | keep | copied | success | backedup | reconciled | cycles | exits | node | exit weight | rate | restime |
|---+---------+----------------------------------+-----------+----------+------------------+------+--------+---------+----------+------------+--------+-------+------+-------------+------+---------|
|   |         | 65467c8e0531d84351e246db7b3bf46f |       0-0 | 47201275 | FINISHED         | yes  | na     | yes     |          |            |    826 |     0 |      |             |      |         |
|   |         | ba08d22cd1fc2f11b8871582fd5ae28d |       1-0 | 45710691 | FINISHED         | yes  | na     | yes     |          |            |    807 |     0 |      |             |      |         |
|   |         | 1ebba9d011b7af55fa637f86e77d3e7e |       2-0 | 47201276 | FINISHED         | yes  | na     | yes     |          |            |    840 |     0 |      |             |      |         |
|   |         | a08821e60b00cf8817abccdada49ef7f |       5-0 | 47201279 | FINISHED         | yes  | na     | yes     |          |            |    840 |     0 |      |             |      |         |
|---+---------+----------------------------------+-----------+----------+------------------+------+--------+---------+----------+------------+--------+-------+------+-------------+------+---------|
|   |         |                                  |       3-0 | 47201277 | FINISHED,STALLED | no   |        | no      |          |            |    346 |       |      |             |      |         |
|   |         |                                  |       4-0 | 47201278 | FINISHED,STALLED | no   |        | no      |          |            |    324 |       |      |             |      |         |
|   |         |                                  |       0-0 | 46136262 | FAILED           |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-2 | 46136280 | FAILED           |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-3 | 46136283 | FAILED           |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-4 | 46136287 | FAILED           |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-5 | 46136292 | FAILED           |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-0 | 45710690 | RUNNING,BLOWUP   |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-2 | 45710692 | RUNNING,BLOWUP   |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-3 | 45710693 | RUNNING,BLOWUP   |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-4 | 45710694 | RUNNING,BLOWUP   |      |        |         |          |            |        |       |      |             |      |         |
|   |         |                                  |       0-5 | 45710695 | RUNNING,BLOWUP   |      |        |         |          |            |        |       |      |             |      |         |
|---+---------+----------------------------------+-----------+----------+------------------+------+--------+---------+----------+------------+--------+-------+------+-------------+------+---------|
#+TBLFM: $17=1/$rate

***** notes

*** 20

**** Current <2020-08-28 Fri>

***** tree

- root :: 46edaa6b9b187fac7e6d5426962c97d1 (patched: yyy)
  - 0
    - 0-0 :: 67607f3a767374eca477fa18c64370a2
    - 0-1 :: 92047914ba43c1dfadd28a33289c4a48
    - 0-2 :: c95acace0e8022932eb7f53505fb3ca9
  - 1
    - 1-0 :: 20e9b79e88a7066b5292e1d32c50ba89
    - 1-1 :: 193c7c5d6d62695149b64ddfa5d87aee
    - 1-2 :: 2346e56c9bf92a79a7cabba4c94d6563
  - 2
    - 2-0 :: 09012ee1199004878490ff6798a9226f
    - 2-1 :: 6ebbfda79da92c41c5f3aef37309bce9
    - 2-2 :: e798e2a210178e104f0fd17f790706c8
  - 3
    - 3-0 :: c722976b2739f7326b72e9ee42ac88aa
    - 3-1 :: e383051d3d434282121381dbd3cad3c8
    - 3-2 :: babaab8ec8289671841152746a403aaa
  - 4
    - 4-0 :: 0cc674b19036ca12d4a43e3b65026174
    - 4-1 :: 880e5cf325bf636555c5d5b545ff4713
    - 4-2 :: 19bc4b067ae88f7f4b7b8e427b5c8ce7
  - 5
    - 5-0 :: 7e61c0c04e9f9eeca4bfb675f213bb59
    - 5-1 :: a6e8df9432b76bda26da1743da700c4a
    - 5-2 :: 757672ab17e699f2e6e1327716bdabca


***** table
     :PROPERTIES:
     :TABLE_EXPORT_FILE: data/sim_management/lig_20.csv
     :TABLE_EXPORT_FORMAT: orgtbl-to-csv
     :END:


|  job_ID | patched_start_hash               | end_hash                         | contig_ID | node    | state    |  pmin | tag | cycles | exits | keep | success | reconciled | backedup | start_hash                       |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 3162998 | 46edaa6b9b187fac7e6d5426962c97d1 | 67607f3a767374eca477fa18c64370a2 |       0-0 |         | FINISHED | 1e-12 |     |   1539 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
| 3178012 | 46edaa6b9b187fac7e6d5426962c97d1 | 20e9b79e88a7066b5292e1d32c50ba89 |       1-0 |         | FINISHED | 1e-12 |     |    653 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
| 3178013 | 46edaa6b9b187fac7e6d5426962c97d1 | 09012ee1199004878490ff6798a9226f |       2-0 |         | FINISHED | 1e-12 |     |    657 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
| 3178014 | 46edaa6b9b187fac7e6d5426962c97d1 | c722976b2739f7326b72e9ee42ac88aa |       3-0 |         | FINISHED | 1e-12 |     |    649 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
| 3178015 | 46edaa6b9b187fac7e6d5426962c97d1 | 0cc674b19036ca12d4a43e3b65026174 |       4-0 |         | FINISHED | 1e-12 |     |    661 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
| 3178016 | 46edaa6b9b187fac7e6d5426962c97d1 | 7e61c0c04e9f9eeca4bfb675f213bb59 |       5-0 |         | FINISHED | 1e-12 |     |    652 |     0 | yes  | yes     | yes        | yes      | 46edaa6b9b187fac7e6d5426962c97d1 |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 3546061 | 892e307e4a9906d3749b7cd8c0e4a098 | 92047914ba43c1dfadd28a33289c4a48 |       0-1 | nvl-004 | FINISHED | 1e-12 |     |   1685 |     0 | yes  | yes     | yes        | yes      | 67607f3a767374eca477fa18c64370a2 |
| 3546062 | 4d9fef6f8ccbba88321553e5eb9f9431 | 6ebbfda79da92c41c5f3aef37309bce9 |       1-1 | lac-137 | FINISHED | 1e-12 |     |   1601 |     0 | yes  | yes     | yes        | yes      | 20e9b79e88a7066b5292e1d32c50ba89 |
| 3546063 | 29bd0171209e87d6fd5039ab8953add8 | 193c7c5d6d62695149b64ddfa5d87aee |       2-1 | nvl-005 | FINISHED | 1e-12 |     |    641 |     0 | yes  | yes     | yes        | yes      | 09012ee1199004878490ff6798a9226f |
| 3546064 | 4ebbcccd4945fb912909e218f2fc86e6 | e383051d3d434282121381dbd3cad3c8 |       3-1 | lac-290 | FINISHED | 1e-12 |     |    641 |     0 | yes  | yes     | yes        | yes      | c722976b2739f7326b72e9ee42ac88aa |
| 3546065 | f48a38dc14d00eee8aabef827ced2ea8 | 880e5cf325bf636555c5d5b545ff4713 |       4-1 | lac-348 | FINISHED | 1e-12 |     |    651 |     0 | yes  | yes     | yes        | yes      | 0cc674b19036ca12d4a43e3b65026174 |
| 3546066 | 37820a0dcdd0076451aa9202a35825c5 | a6e8df9432b76bda26da1743da700c4a |       5-1 | lac-143 | FINISHED | 1e-12 |     |    643 |     0 | yes  | yes     | yes        | yes      | 7e61c0c04e9f9eeca4bfb675f213bb59 |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|
| 4961510 | dc4fd59650fea2ca6ef2f9fc738727a2 | c95acace0e8022932eb7f53505fb3ca9 |       0-2 | lac-342 | FINISHED | 1e-16 |     |    599 |     0 | yes  | yes     |            |          | 92047914ba43c1dfadd28a33289c4a48 |
| 4961513 | 5e7ef60fd22e6a437d4700407decae50 | 2346e56c9bf92a79a7cabba4c94d6563 |       1-2 | nvl-004 | FINISHED | 1e-16 |     |   1674 |     0 | yes  | yes     |            |          | 193c7c5d6d62695149b64ddfa5d87aee |
| 4961512 | 8a6d6ab60fb4de7258b6534c6844a1ad | e798e2a210178e104f0fd17f790706c8 |       2-2 | nvl-003 | FINISHED | 1e-16 |     |   1653 |     0 | yes  | yes     |            |          | 6ebbfda79da92c41c5f3aef37309bce9 |
| 4961514 | 1fb3af10137b92e9ad2620c67f2c4c0a | babaab8ec8289671841152746a403aaa |       3-2 | nvl-002 | FINISHED | 1e-16 |     |    801 |     0 | yes  | yes     |            |          | e383051d3d434282121381dbd3cad3c8 |
| 4961516 | 650eed89d4d628eb78f7e0bc708363b8 | 19bc4b067ae88f7f4b7b8e427b5c8ce7 |       4-2 | lac-137 | FINISHED | 1e-16 |     |    607 |     0 | yes  | yes     |            |          | 880e5cf325bf636555c5d5b545ff4713 |
| 4961517 | 819377e39abbe029297e8f3d23c8796c | 757672ab17e699f2e6e1327716bdabca |       5-2 | lac-290 | FINISHED | 1e-16 |     |    608 |     0 | yes  | yes     |            |          | a6e8df9432b76bda26da1743da700c4a |
|---------+----------------------------------+----------------------------------+-----------+---------+----------+-------+-----+--------+-------+------+---------+------------+----------+----------------------------------|


***** Notes




**** Old
***** tree

 - new root :: 9c296f7853b0fbfcb3e7f91ca66bc4a5
 - root :: 46dc5ffd2ea9c4616a9ce75c2d1501b0
   - 0
     - 0-0 :: 83a13bf6f8132c367857f25859f56d09
   - 1
     - 1-0 :: 83c1b820bc9cc3446288d73dddda70c8
   - 2
     - 2-0 :: e70ad1d4751a6b9b7a5cb6a2ed7ead54
   - 3
     - 3-0 :: b94bf86e81b59c5902372fd5438ce0b5
   - 4
     - 4-0 :: 21141ca76235b6926a3aebb8e7e5d16a
   - 5
     - 5-0 :: 442ec2a8be828eb4436bcf121717a5c7



***** table

 | ! | run_idx | end_hash                         | contig_ID |   job_ID | state          | keep | copied | success | backedup | reconciled | cycles | exits | node | exit weight | rate | restime |
 |---+---------+----------------------------------+-----------+----------+----------------+------+--------+---------+----------+------------+--------+-------+------+-------------+------+---------|
 |   |         | 83a13bf6f8132c367857f25859f56d09 |       0-0 | 45710696 | FINISHED       | yes  | na     | yes     |          |            |    831 |     0 |      |             |      |         |
 |   |         | 83c1b820bc9cc3446288d73dddda70c8 |       1-0 | 47201463 | FINISHED       | yes  | na     | yes     |          |            |        |       |      |             |      |         |
 |   |         | e70ad1d4751a6b9b7a5cb6a2ed7ead54 |       2-0 | 45710698 | FINISHED       | yes  | na     | yes     |          |            |    831 |     0 |      |             |      |         |
 |   |         | b94bf86e81b59c5902372fd5438ce0b5 |       3-0 | 45710699 | FAILED,STALLED | yes  | na     | yes     |          |            |    817 |     0 |      |             |      |         |
 |   |         | 21141ca76235b6926a3aebb8e7e5d16a |       4-0 | 45710700 | FINISHED       | yes  | na     | yes     |          |            |   1762 |     0 |      |             |      |         |
 |   |         | 442ec2a8be828eb4436bcf121717a5c7 |       5-0 | 45710701 | FINISHED       | yes  | na     | yes     |          |            |    832 |     0 |      |             |      |         |
 |---+---------+----------------------------------+-----------+----------+----------------+------+--------+---------+----------+------------+--------+-------+------+-------------+------+---------|
 #+TBLFM: $17=1/$rate

***** notes

*** COMMENT Sim Management Archive
**** 3

***** span tree

This new root snapshot is suitable for things like test runs from the
root orches. To add runs to the started datasets use the original root hash

- new root snapshot hash :: 8e158c28c8c426f064d032823bb9aaf9

This is the tree from the simulations which were started orginally, if
you are to run simulations within this collective dataset use the
snapshots defined here. The root snapshot similar to the new root
snapshot, but somehow is a different hash from when we rebuilt things.

- root :: ed11f6caeabd40a7cf5d687a98f5a6b2
  - 0
    - 0-0 :: de390f00dac09f52f97d82ca5b8789fa
    - 0-1 :: 2a2888da6c4733be0328b22eb4b7af26
    - 0-2 :: d1317b41436d5d9c48b79c1b500d759f
    - 0-3 :: 055e58cbe616fc271fb25d3daa78b471
    - 0-4 :: 0cf1c1dbbdb8a8e91f0bfc5c090a0b74
    - 0-5 :: 7153db16782171f2534656644c98ffea
  - 1
    - 1-0 :: bb621a3fa5b36b7954a3d21745f203aa
    - 1-1 :: 8e97b2d28e2afd49db6b2e1c18d48608
    - 1-2 :: ec513e3d59a5b0d921bbe93a9b17857e
  - 2
    - 2-0 :: 12f792ca1ec034cac0de7d9ff6517c91
    - 2-1 :: 1dde5be5baf0faef2233201b68832245
    - 2-2 :: f5835aba9352e5d8c4bd73e22c0b74f5
  - 3
    - 3-0 :: 9b027c1488717712905b00fe3e8cb7a4
    - 3-1 :: 1ff9228c6da5cd22713a9d8ec42351ad
    - 3-2 :: 844000198b1d1c40a6a2a23bf726887b
  - 4
    - 4-0 :: eaab5d019e061a229dd07e387a242ffc
    - 4-1 :: 5756b809fe2f60269f46eff3407d76db
    - 4-2 :: 09706bed29a4b433302be2566848d861
  - 5
    - 5-0 :: ff4b43ac2bb8c4277fbc0cb3b08496c4
    - 5-1 :: 18dcfb9749be00a67e22f85105ee378d
    - 5-2 :: afbc501c4a89356fc68164cb15fc3ea8


****** Old hashes

- root :: 3a3eed6bb613a042d089547bcab8b1e6
  - 0 :: 1b0a5c2dc8d36ab234935a7c6b3cfc44
    - 0-0 :: 1b0a5c2dc8d36ab234935a7c6b3cfc44 
    - 0-1 :: 608107bdef0980187daa7c66c125e854
    - 0-2 :: 8cd97e8583f682845106ff1456916318
    - 0-3 :: b6119f14a8656e9b7099865a6bf443bd
    - 0-4 :: 2ade05295a691b7eb907128deba57198
  - 1 :: d005a8ff3725cd638d8d5f64d0106f39
    - 1-0 :: d005a8ff3725cd638d8d5f64d0106f39
    - 1-1 :: 2e37121d537d99fdcf6ad36283def254
  - 2 :: 81ec62755ca3eb30c38390eab5c33b2b
    - 2-0 :: 81ec62755ca3eb30c38390eab5c33b2b
    - 2-1 :: d9d4ac20026372f2b57a37bf99f2dc39
  - 3 :: 25b4dd9712ae9d113514d31bc4a69ade
    - 3-0 :: 25b4dd9712ae9d113514d31bc4a69ade
    - 3-1 :: 2847ddf56740caab77a9edfc55f03c71
  - 4 :: 8725e396deb0cad3904b05f64a73fe8f
    - 4-0 :: 8725e396deb0cad3904b05f64a73fe8f
    - 4-1 :: 530a1088efb3186496ef2fea6be4aab4
  - 5 :: b43cc17d8c17d88be1167be2515bd8ba
    - 5-0 :: b43cc17d8c17d88be1167be2515bd8ba
    - 5-1 :: d399d6b123c54eec7ea1e755329739a9


****** mapping old to new script

Here is a python datastructure with that relationship

NOTE WARNING: This doesn't produce the right hashes to match up with
the job produced orchs for runs. I.e. the table above was not produced
entirely with this.

#+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
  import itertools as it
  from collections import defaultdict

  snaphash_records = []

  lig_3_orch_hash_maps = [

      ('13326969',
      (('d005a8ff3725cd638d8d5f64d0106f39', 'bb621a3fa5b36b7954a3d21745f203aa'),
       ('2e37121d537d99fdcf6ad36283def254', '8e97b2d28e2afd49db6b2e1c18d48608'))),

      ('13326970',
       (('81ec62755ca3eb30c38390eab5c33b2b', '12f792ca1ec034cac0de7d9ff6517c91'),
        ('d9d4ac20026372f2b57a37bf99f2dc39', '1dde5be5baf0faef2233201b68832245'))),

      ('13326971',
       (('25b4dd9712ae9d113514d31bc4a69ade', '9b027c1488717712905b00fe3e8cb7a4'),
        ('2847ddf56740caab77a9edfc55f03c71', '1ff9228c6da5cd22713a9d8ec42351ad'))),

      ('13326972',
       (('8725e396deb0cad3904b05f64a73fe8f', 'eaab5d019e061a229dd07e387a242ffc'),
        ('530a1088efb3186496ef2fea6be4aab4', '5756b809fe2f60269f46eff3407d76db'))),

      ('13326973',
       (('b43cc17d8c17d88be1167be2515bd8ba', 'ff4b43ac2bb8c4277fbc0cb3b08496c4'),
        ('d399d6b123c54eec7ea1e755329739a9', '18dcfb9749be00a67e22f85105ee378d'))),

      ('13327704',
       (('b6119f14a8656e9b7099865a6bf443bd', '055e58cbe616fc271fb25d3daa78b471'),
        ('2ade05295a691b7eb907128deba57198', '0cf1c1dbbdb8a8e91f0bfc5c090a0b74'))),

      ('2503029',
       (('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'))),

      ('3075070',
       (('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'))),

      ('3722763',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'))),

      ('5940557',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('d005a8ff3725cd638d8d5f64d0106f39', 'cd3be07f43d753b6f66f329912239d32'))),

      ('5940559',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('b6119f14a8656e9b7099865a6bf443bd', '885203f0f586faf9e484089185082e12'))),

      ('5940766',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('81ec62755ca3eb30c38390eab5c33b2b', '429601ec8d38abbd21a060e1f82bb701'))),

      ('5940966',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('25b4dd9712ae9d113514d31bc4a69ade', 'e0b4eb8f2c51ab4c9545be850a5aecb3'))),

      ('5940967',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('8725e396deb0cad3904b05f64a73fe8f', '7d31bef63622aae7e364ff369d78e706'))),

      ('5940968',
       (('1454ab6acc61d826c29f7f77e1ad6c40', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('3a3eed6bb613a042d089547bcab8b1e6', 'acb5994e13b7f13f88f1afef68a6b11f'),
        ('1b0a5c2dc8d36ab234935a7c6b3cfc44', 'b2d6bc6ba87730e3efb19e15039796d5'),
        ('608107bdef0980187daa7c66c125e854', 'b9e6dba4fd77c1c6e8270cb65286a62b'),
        ('8cd97e8583f682845106ff1456916318', '4154720461e854302fc14fe35b3ad3b2'),
        ('b43cc17d8c17d88be1167be2515bd8ba', '96de1865ea30c868afe82340088ea4c2'))),
  ]


  # just ligand 3

  # combine them into one set of records with the job ids
  lig_3_records = list(it.chain.from_iterable(
        [[(orch_hash_map[0], old, new) for old, new in orch_hash_map[1]]
                for orch_hash_map in lig_3_orch_hash_maps]))

  # find the old hashes with multiple new ones, so we can choose one to
  # use
  lig_3_old_to_new = defaultdict(set)

  for job_id, old, new in lig_3_records:
      lig_3_old_to_new[old].add(new)

  lig_3_old_to_new_assgs = []
  for old_hash, new_hashes in lig_3_old_to_new.items():
      # just pop one off at random to use
      lig_3_old_to_new_assgs.append((old_hash, new_hashes.pop()))

  # print it out in an org-table like way
  print("\n")
  print("Ligand 3 all mappings table")
  print("| job id | old hash | new hash |")
  print(str(lig_3_records).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  print("Ligand 3 assigned mappings table")
  print("| old hash | new hash |")
  print(str(lig_3_old_to_new_assgs).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")


  snaphash_records.append(lig_3_records)
#+END_SRC

****** all mappings table

|   job id | old hash                         | new hash                         |
|----------+----------------------------------+----------------------------------|
| 13326969 | d005a8ff3725cd638d8d5f64d0106f39 | bb621a3fa5b36b7954a3d21745f203aa |
| 13326969 | 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 |
| 13326970 | 81ec62755ca3eb30c38390eab5c33b2b | 12f792ca1ec034cac0de7d9ff6517c91 |
| 13326970 | d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 |
| 13326971 | 25b4dd9712ae9d113514d31bc4a69ade | 9b027c1488717712905b00fe3e8cb7a4 |
| 13326971 | 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad |
| 13326972 | 8725e396deb0cad3904b05f64a73fe8f | eaab5d019e061a229dd07e387a242ffc |
| 13326972 | 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db |
| 13326973 | b43cc17d8c17d88be1167be2515bd8ba | ff4b43ac2bb8c4277fbc0cb3b08496c4 |
| 13326973 | d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d |
| 13327704 | b6119f14a8656e9b7099865a6bf443bd | 055e58cbe616fc271fb25d3daa78b471 |
| 13327704 | 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 |
|  2503029 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  2503029 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3075070 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3075070 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  3722763 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  3722763 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  3722763 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  3722763 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  3722763 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940557 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940557 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940557 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940557 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940557 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940557 | d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 |
|  5940559 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940559 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940559 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940559 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940559 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940559 | b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 |
|  5940766 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940766 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940766 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940766 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940766 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940766 | 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 |
|  5940966 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940966 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940966 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940966 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940966 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940966 | 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 |
|  5940967 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940967 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940967 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940967 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940967 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940967 | 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 |
|  5940968 | 1454ab6acc61d826c29f7f77e1ad6c40 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940968 | 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f |
|  5940968 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 |
|  5940968 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b |
|  5940968 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 |
|  5940968 | b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 |


****** assigned mappings

| old hash                         | new hash                         | redo new hash                    |
|----------------------------------+----------------------------------+----------------------------------|
| d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 | bb621a3fa5b36b7954a3d21745f203aa |
| 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 |                                  |
| 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 | 12f792ca1ec034cac0de7d9ff6517c91 |
| d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 |                                  |
| 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 | 9b027c1488717712905b00fe3e8cb7a4 |
| 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad |                                  |
| 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 | eaab5d019e061a229dd07e387a242ffc |
| 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db |                                  |
| b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 | ff4b43ac2bb8c4277fbc0cb3b08496c4 |
| d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d |                                  |
| b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 | 055e58cbe616fc271fb25d3daa78b471 |
| 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 |                                  |
| 3a3eed6bb613a042d089547bcab8b1e6 | acb5994e13b7f13f88f1afef68a6b11f | ed11f6caeabd40a7cf5d687a98f5a6b2 |
| 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 | de390f00dac09f52f97d82ca5b8789fa |
| 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b | 2a2888da6c4733be0328b22eb4b7af26 |
| 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 | d1317b41436d5d9c48b79c1b500d759f |




***** run table

|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
| ! | run_idx | contig_ID |   job_ID | end_hash                         | state     | exits |            exit_weight | success | keep | copied | reconciled | backedup |                   rate |   restime | old_end_hash                     | v2_end_hash                      | node    | cycles | avg. runner time | avg. BC time | avg. res time | avg. cycle time | avg seg time |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       0 |       0-0 |  2503029 | de390f00dac09f52f97d82ca5b8789fa | FINISHED  |     0 |                      0 | yes     | yes  | yes    | yes        | yes      |                      0 |       1/0 | 1b0a5c2dc8d36ab234935a7c6b3cfc44 | b2d6bc6ba87730e3efb19e15039796d5 | lac-027 |    241 |                  |              |               |                 |              |
|   |       1 |       0-1 |  3075070 | 2a2888da6c4733be0328b22eb4b7af26 | RECOVERED |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                      0 |       1/0 | 608107bdef0980187daa7c66c125e854 | b9e6dba4fd77c1c6e8270cb65286a62b | lac-027 |    155 |                  |              |               |                 |              |
|   |       2 |       0-2 |  3722763 | d1317b41436d5d9c48b79c1b500d759f | FINISHED  |     6 |  9.097017821291077e-12 | yes     | yes  | yes    | yes        | yes      | 1.3997134757033284e-05 | 71443.193 | 8cd97e8583f682845106ff1456916318 | 4154720461e854302fc14fe35b3ad3b2 | lac-025 |    677 |                  |              |               |                 |              |
|   |       3 |       0-3 |  5940559 | 055e58cbe616fc271fb25d3daa78b471 | FINISHED  |     5 |  6.995932729903008e-12 | yes     | yes  | yes    | yes        | yes      | 1.0638583835010646e-05 | 93997.473 | b6119f14a8656e9b7099865a6bf443bd | 885203f0f586faf9e484089185082e12 | lac-027 |    685 |                  |              |               |                 |              |
|   |       9 |       0-4 | 13327704 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 | CANCELLED |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                        |           | 2ade05295a691b7eb907128deba57198 | 0cf1c1dbbdb8a8e91f0bfc5c090a0b74 | lac-288 |    152 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       4 |       1-0 |  5940557 | bb621a3fa5b36b7954a3d21745f203aa | FINISHED  |     2 | 1.2983715742088229e-11 | yes     | yes  | yes    | yes        | yes      | 1.8942162322140815e-05 | 52792.283 | d005a8ff3725cd638d8d5f64d0106f39 | cd3be07f43d753b6f66f329912239d32 | lac-141 |    714 |                  |              |               |                 |              |
|   |      10 |       1-1 | 13326969 | 8e97b2d28e2afd49db6b2e1c18d48608 | CANCELLED |     8 |  5.309500795900743e-11 | yes     | yes  | yes    | yes        | yes      |                        |           | 2e37121d537d99fdcf6ad36283def254 | 8e97b2d28e2afd49db6b2e1c18d48608 | lac-028 |    557 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       5 |       2-0 |  5940766 | 12f792ca1ec034cac0de7d9ff6517c91 | FINISHED  |     0 |                      0 | no      | yes  | yes    | yes        | yes      |                      0 |       1/0 | 81ec62755ca3eb30c38390eab5c33b2b | 429601ec8d38abbd21a060e1f82bb701 | lac-028 |    738 |                  |              |               |                 |              |
|   |      11 |       2-1 | 13326970 | 1dde5be5baf0faef2233201b68832245 | CANCELLED |     1 | 1.4833536742875784e-12 | yes     | yes  | yes    | yes        | yes      |                        |           | d9d4ac20026372f2b57a37bf99f2dc39 | 1dde5be5baf0faef2233201b68832245 | lac-027 |    555 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       6 |       3-0 |  5940966 | 9b027c1488717712905b00fe3e8cb7a4 | FINISHED  |     6 |  8.041574068210327e-09 | yes     | yes  | yes    | yes        | yes      |   0.012087503108781742 | 82.730072 | 25b4dd9712ae9d113514d31bc4a69ade | e0b4eb8f2c51ab4c9545be850a5aecb3 | lac-028 |    693 |                  |              |               |                 |              |
|   |      12 |       3-1 | 13326971 | 1ff9228c6da5cd22713a9d8ec42351ad | CANCELLED |    29 | 3.4406083367984306e-08 | yes     | yes  | yes    | yes        | yes      |                        |           | 2847ddf56740caab77a9edfc55f03c71 | 1ff9228c6da5cd22713a9d8ec42351ad | nvl-003 |   1055 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       7 |       4-0 |  5940967 | eaab5d019e061a229dd07e387a242ffc | FINISHED  |    32 |  5.886065409321491e-09 | yes     | yes  | yes    | yes        | yes      |   0.008911799614403896 | 112.21078 | 8725e396deb0cad3904b05f64a73fe8f | 7d31bef63622aae7e364ff369d78e706 | lac-195 |    688 |                  |              |               |                 |              |
|   |      13 |       4-1 | 13326972 | 5756b809fe2f60269f46eff3407d76db | CANCELLED |    33 | 3.7826321754656087e-10 | yes     | yes  | yes    | yes        | yes      |                        |           | 530a1088efb3186496ef2fea6be4aab4 | 5756b809fe2f60269f46eff3407d76db | lac-140 |    553 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |       8 |       5-0 |  5940968 | ff4b43ac2bb8c4277fbc0cb3b08496c4 | FINISHED  |     2 |  3.400666086019177e-12 | yes     | yes  | yes    | yes        | yes      |  4.886014491406851e-06 | 204665.79 | b43cc17d8c17d88be1167be2515bd8ba | 96de1865ea30c868afe82340088ea4c2 | lac-025 |    725 |                  |              |               |                 |              |
|   |      14 |       5-1 | 13326973 | 18dcfb9749be00a67e22f85105ee378d | CANCELLED |     7 |  3.440377372541073e-09 | yes     | yes  | yes    | yes        | yes      |                        |           | d399d6b123c54eec7ea1e755329739a9 | 18dcfb9749be00a67e22f85105ee378d | nvl-005 |    992 |                  |              |               |                 |              |
|---+---------+-----------+----------+----------------------------------+-----------+-------+------------------------+---------+------+--------+------------+----------+------------------------+-----------+----------------------------------+----------------------------------+---------+--------+------------------+--------------+---------------+-----------------+--------------|
|   |         |       0-5 | 22160883 | 7153db16782171f2534656644c98ffea | FINISHED  |    31 | 3.4210872876361725e-09 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-028 |    856 |              632 |           33 |             2 |             667 |           68 |
|   |         |       1-2 | 22073108 | ec513e3d59a5b0d921bbe93a9b17857e | FINISHED  |   110 |  8.398273190754693e-06 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-002 |   1771 |              264 |           33 |             2 |             301 |            7 |
|   |         |       2-2 | 22073109 | f5835aba9352e5d8c4bd73e22c0b74f5 | FINISHED  |    49 |  7.692635141264896e-10 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-003 |   1791 |              260 |           33 |             2 |             296 |            7 |
|   |         |       3-2 | 22073110 | 844000198b1d1c40a6a2a23bf726887b | FINISHED  |    75 |  8.755167184270571e-08 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | nvl-004 |   1800 |              260 |           33 |             2 |             295 |            7 |
|   |         |       4-2 | 22073111 | 09706bed29a4b433302be2566848d861 | FINISHED  |    64 |  3.409734339522123e-10 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-195 |    848 |              631 |           37 |             2 |             671 |           70 |
|   |         |       5-2 | 22073112 | afbc501c4a89356fc68164cb15fc3ea8 | FINISHED  |    33 |  6.917655884624962e-09 | yes     | yes  | yes    |            |          |                        |           |                                  |                                  | lac-141 |    757 |              715 |           38 |             2 |             757 |           75 |
#+TBLFM: $15=1/$rate




***** calculating contig props

#+begin_src python :tangle hpcc/scripts/calc_contig_props.py
  import numpy as np
  import pandas as pd

  import simtk.unit as unit

  from seh_prep.parameters import STEP_TIME, N_CYCLE_STEPS, N_WALKERS, CYCLE_TIME

  run_cols = ['contig_idx', 'inter_contig_idx', 'n_cycles', 'n_exits', 'exit_weight']
  run_recs = [
      # contig 0
      (0, 0, 241, 0, 0.),
      (0, 1, 155, 0, 0.),
      (0, 2, 677, 6, 9.097017821291077e-12),
      (0, 3, 685, 5, 6.995932729903008e-12),
      (0, 4, 152, 0, 0.),
      (0, 5, 856, 31, 3.4210872876361725e-09),

      # contig 1
      (1, 0, 714, 2, 1.2983715742088229e-11),
      (1, 1, 557, 8, 5.309500795900743e-11),
      (1, 2, 1771, 110, 8.398273190754693e-06),

      # contig 2
      (2, 0, 738, 0, 0.),
      (2, 1, 555, 1, 1.4833536742875784e-12),
      (2, 2, 1791, 49, 7.692635141264896e-10),

      # contig 3
      (3, 0, 693, 6, 8.041574068210327e-09),
      (3, 1, 1055, 29, 3.4406083367984306e-08),
      (3, 2, 1800, 75, 8.755167184270571e-08),

      # contig 4
      (4, 0, 688, 32,  5.886065409321491e-09),
      (4, 1, 553, 33, 3.7826321754656087e-10),
      (4, 2, 848, 64, 3.409734339522123e-10),

      # contig 5
      (5, 0, 725, 2, 3.400666086019177e-12),
      (5, 1, 992, 7, 3.440377372541073e-09),
      (5, 2, 757, 33, 6.917655884624962e-09),
  ]

  run_df = pd.DataFrame(run_recs, columns=run_cols)


  run_agg_funcs = {
      'inter_contig_idx' : len,
      'n_cycles' : np.sum,
      'n_exits' : np.sum,
      'exit_weight' : np.sum,
  }

  contig_df = run_df.groupby('contig_idx')\
                    .agg(run_agg_funcs)\
                    .rename(columns={'inter_contig_idx' : 'n_runs'})

  CYCLE_SAMPLING_TIME = CYCLE_TIME * N_WALKERS

  # amount of sampling for each indivual walker, the longest possible
  # contiguous simulation
  contig_df['walker_sampling_time (ms)'] = [(n_cycles * CYCLE_TIME).value_in_unit(unit.microsecond)
                                            for n_cycles in contig_df['n_cycles']]

  # total aggregate ensemble simiulation time
  contig_df['ensemble_sampling_time (ms)'] = [(n_cycles * CYCLE_SAMPLING_TIME).value_in_unit(unit.microsecond)
                                           for n_cycles in contig_df['n_cycles']]

#+end_src

***** calculating performance

#+begin_src python :tangle hpcc/scripts/calc_run_perfs.py
  from copy import copy

  import numpy as np
  import pandas as pd

  import simtk.unit as unit

  from seh_prep.parameters import STEP_TIME, N_CYCLE_STEPS, N_WALKERS, CYCLE_TIME

  perf_rec_cols = [
      'job_id',
      'node_id',
      'n_cycles',
      'avg_runner_time_s',
      'avg_bc_time_s',
      'avg_resampling_time_s',
      'avg_cycle_time_s',
      'avg_segment_time_s',
  ]

  # TODO add total actual runtime



  run_recs = [

  ( '22160883' , 'lac-028' ,  856 , 632 , 33 , 2 , 667 , 68 ),
  ( '22073108' , 'nvl-002' , 1771 , 264 , 33 , 2 , 301 ,  7 ),
  ( '22073109' , 'nvl-003' , 1791 , 260 , 33 , 2 , 296 ,  7 ),
  ( '22073110' , 'nvl-004' , 1800 , 260 , 33 , 2 , 295 ,  7 ),
  ( '22073111' , 'lac-195' ,  848 , 631 , 37 , 2 , 671 , 70 ),
  ( '22073112' , 'lac-141' ,  757 , 715 , 38 , 2 , 757 , 75 ),

   ]

  perf_df = pd.DataFrame(run_recs, columns=perf_rec_cols)

  # then compute some things from this that are of interest

  # split up the node id into node types
  node_types_col = [node_id.split('-')[0] for node_id in perf_df['node_id']]

  perf_df['node_type'] = node_types_col

  # compute the avg sampling time from the cycles
  perf_df['avg_sampling_time_ns'] = [(n_cycles * CYCLE_TIME).value_in_unit(unit.nanosecond)
                                     for n_cycles in perf_df['n_cycles']]


  # group by the node type and average over all of the interesting columns

  run_agg_funcs = {
      'job_id' : len,
      'n_cycles' : np.mean,
      'avg_runner_time_s' : np.mean,
      'avg_bc_time_s' : np.mean,
      'avg_resampling_time_s' : np.mean,
      'avg_cycle_time_s' : np.mean,
      'avg_segment_time_s' : np.mean,
      'avg_sampling_time_ns' : np.mean,
  }

  node_perf_df = perf_df.drop(columns=['node_id'])\
                    .groupby('node_type')\
                    .agg(run_agg_funcs)\
                    .rename(columns={'job_id' : 'n_runs'})


  # calculate the speedups for each time column
  time_cols = (

      'avg_runner_time_s',
      'avg_bc_time_s',
      'avg_resampling_time_s',
      'avg_cycle_time_s',
      'avg_segment_time_s',
  )

  sampling_cols = (
      'avg_sampling_time_ns',
      'n_cycles',
  )

  speedup_rec = {}
  for col in node_perf_df.columns:
      if col in sampling_cols:
          speedup_rec[col] = node_perf_df.loc['nvl'][col] / node_perf_df.loc['lac'][col]
      elif col in time_cols:
          speedup_rec[col] = node_perf_df.loc['lac'][col] / node_perf_df.loc['nvl'][col]
      else:
          speedup_rec[col] = np.nan

  speedup_df = copy(node_perf_df)
  speedup_df.loc['speedup'] = speedup_rec


  #  calculate the nanoseconds per hour, per day and per week
  week_hours = 168
  week_days = 7
  num_gpus = 8

  node_perf_df['ns_per_node_week'] = node_perf_df['avg_sampling_time_ns']
  node_perf_df['ns_per_node_day'] = node_perf_df['avg_sampling_time_ns'] / week_days
  node_perf_df['ns_per_node_hour'] = node_perf_df['avg_sampling_time_ns'] / week_hours

  node_perf_df['ns_per_gpu_week'] = node_perf_df['avg_sampling_time_ns'] / num_gpus
  node_perf_df['ns_per_gpu_day'] = node_perf_df['avg_sampling_time_ns'] / week_days / num_gpus
  node_perf_df['ns_per_gpu_hour'] = node_perf_df['avg_sampling_time_ns'] / week_hours / num_gpus


  throughput_cols = [
      'ns_per_node_week',
      'ns_per_node_day',
      'ns_per_node_hour',
      'ns_per_gpu_week',
      'ns_per_gpu_day',
      'ns_per_gpu_hour',
  ]

  throughput_df = node_perf_df[throughput_cols]


  # computing overheads of regular MD with OpenMM
  node_perf_df['avg_total_segment_gpu_time_s'] = node_perf_df['avg_segment_time_s'] * N_WALKERS

  node_perf_df['avg_total_segment_time_s'] = node_perf_df['avg_total_segment_gpu_time_s'] / num_gpus

  # compute Runner overhead
  node_perf_df['runner_overhead_s'] = node_perf_df['avg_runner_time_s'] - \
                                    node_perf_df['avg_total_segment_time_s']


  # compute Wepy total overhead
  node_perf_df['cycle_overhead_s'] = node_perf_df['avg_cycle_time_s'] - \
                                    node_perf_df['avg_total_segment_time_s']


  # include the end time where we are waiting for the simulation to
  # end. I don't have the actual rn times available anyways. But the
  # idea would be to minimize this as well since we run on node time. To
  # report wepy as a library overhead use the cycle_overhead
  total_time = week_hours * 60 * 60
  node_perf_df['node_time_overhead_s'] = total_time - \
                               (node_perf_df['avg_total_segment_time_s'] *
                                node_perf_df['n_cycles'])


  # get the residual overhead (not accounted by the components timings)
  node_perf_df['node_time_residual_overhead_s'] = node_perf_df['node_time_overhead_s'] - \
                                                  (node_perf_df['cycle_overhead_s'] *
                                                   node_perf_df['n_cycles'])

  node_perf_df['node_time_overhead_h'] = node_perf_df['node_time_overhead_s'] / (60**2)
  node_perf_df['node_time_residual_overhead_h'] = node_perf_df['node_time_residual_overhead_s'] / (60**2)

  overhead_cols = [
      'avg_total_segment_time_s',
      'runner_overhead_s',
      'cycle_overhead_s',
      'node_time_overhead_s',
      'node_time_residual_overhead_s',
      'node_time_overhead_h',
      'node_time_residual_overhead_h',
  ]


  # calculate the percentage of total time spent not doing MD
  node_perf_df['md_fraction'] = node_perf_df['avg_total_segment_time_s'] /\
                                node_perf_df['avg_cycle_time_s']

  node_perf_df['wepy_fraction'] = node_perf_df['cycle_overhead_s'] /\
                                  node_perf_df['avg_cycle_time_s']

  node_perf_df['runner_fraction'] = node_perf_df['runner_overhead_s'] /\
                                    node_perf_df['avg_cycle_time_s']

  node_perf_df['resampling_fraction'] = node_perf_df['avg_resampling_time_s'] /\
                                        node_perf_df['avg_cycle_time_s']

  node_perf_df['bc_fraction'] = node_perf_df['avg_bc_time_s'] /\
                                    node_perf_df['avg_cycle_time_s']


  fractions_cols = [
      'md_fraction',
      'wepy_fraction',
      'runner_fraction',
      'resampling_fraction',
      'bc_fraction'
  ]

  # calculate the total slowdown
  node_perf_df['slowdown'] = 1 / node_perf_df['md_fraction']

#+end_src

***** contig table
 | contig | n_runs | n_cycles | n_exits |  exit weight | walker time (us) | sampling time (us) |
 |--------+--------+----------+---------+--------------+------------------+--------------------|
 |      0 |      6 |     2766 |      42 | 3.437180e-09 |          0.05532 |            2.65536 |
 |      1 |      3 |     3042 |     120 | 8.398339e-06 |          0.06084 |            2.92032 |
 |      2 |      3 |     3084 |      50 | 7.707469e-10 |          0.06168 |            2.96064 |
 |      3 |      3 |     3548 |     110 | 1.299993e-07 |          0.07096 |            3.40608 |
 |      4 |      3 |     2089 |     129 | 6.605302e-09 |          0.04178 |            2.00544 |
 |      5 |      3 |     2474 |      42 | 1.036143e-08 |          0.04948 |            2.37504 |

***** performace table

Performance times:

| node_type | n_runs |    n_cycles | avg_runner_time_s | avg_bc_time_s | avg_resampling_time_s | avg_cycle_time_s | avg_segment_time_s | avg_sampling_time_ns |
|-----------+--------+-------------+-------------------+---------------+-----------------------+------------------+--------------------+----------------------|
| lac       |    3.0 |  820.333333 |        659.333333 |     36.000000 |                   2.0 |       698.333333 |          71.000000 |            16.406667 |
| nvl       |    3.0 | 1787.333333 |        261.333333 |     33.000000 |                   2.0 |       297.333333 |           7.000000 |            35.746667 |
|-----------+--------+-------------+-------------------+---------------+-----------------------+------------------+--------------------+----------------------|
| speedup   |        |    2.178789 |          2.522959 |      1.090909 |                   1.0 |         2.348655 |          10.142857 |             2.178789 |




Throughput:

| node_type | ns_per_node_week | ns_per_node_day | ns_per_node_hour | ns_per_gpu_week | ns_per_gpu_day | ns_per_gpu_hour |
|-----------+------------------+-----------------+------------------+-----------------+----------------+-----------------|
| lac       |        16.406667 |        2.343810 |         0.097659 |        2.050833 |       0.292976 |        0.012207 |
| nvl       |        35.746667 |        5.106667 |         0.212778 |        4.468333 |       0.638333 |        0.026597 |



Slowdowns:

| node_type | md_fraction | wepy_fraction | runner_fraction | resampling_fraction | bc_fraction |
|-----------+-------------+---------------+-----------------+---------------------+-------------|
| lac       |    0.610024 |      0.389976 |        0.334129 |            0.002864 |    0.051551 |
| nvl       |    0.141256 |      0.858744 |        0.737668 |            0.006726 |    0.110987 |

 cycle time fraction &       MD & wepy total &   runner & resampling &       bc \\
 lac                 & 0.610024 &   0.389976 & 0.334129 &   0.002864 & 0.051551 \\
 nvl                 & 0.141256 &   0.858744 & 0.737668 &   0.006726 & 0.110987 \\


**** 10

***** V1
****** tree


 This new root snapshot is suitable for things like test runs from the
 root orches. To add runs to the started datasets use the original root hash

 - new root snapshot hash :: 6258387e90a2517b19adfd15ee4ba07f

 This is the tree from the simulations which were started orginally, if
 you are to run simulations within this collective dataset use the
 snapshots defined here. The root snapshot similar to the new root
 snapshot, but somehow is a different hash from when we rebuilt things.

 - root :: 1b1ef610fddb4fd86e514eb1acd96a3f
   - 0
     - 0-0 :: 5ff88e59dff891973bd459224813a539
     - 0-1 :: 56ed345df9cfd1bef9c6ee646bcf2b5b
     - 0-2 :: a4bd4a55dd4fb99fe1ad86002911945f
     - 0-3 :: 
   - 1
     - 1-0 :: 5a9ca6bae5b48dad00ee452e896aa61b
     - 1-1 :: 7125793c770e23d596bb82973bf5a26b
     - 1-2 :: 2e6c8ad8f3bc58dbb935f1c91b32c345
     - 1-3 :: 
   - 2
     - 2-0 :: b2303d633641de4b6cb12eed548f87dd
     - 2-1 :: f43da7bcb5ec077e1d71a58efb59b275
     - 2-2 :: 5f0b05d01960da72b3c1a5a55bf408fd
     - 2-3 :: 
   - 3
     - 3-0 :: 0e081557e6e3082183a14c671f76ad86
     - 3-1 :: 
   - 4
     - 4-0 :: 34e64c97669757a13440f19b394622c1
     - 4-1 :: 342f7011e584f7a5db3428acb04b32ea
     - 4-2 :: fa74ecc7bd2e1d8ba50bb4d566003742
     - 4-3 :: 
   - 5
     - 5-0 :: 4ecde27bd33391126d5706041b28e550
     - 5-1 :: efd25ab7fe51c42551e754d9f6a593cd
     - 5-2 :: 

******* old hashes
 - root :: 6593f31b3db4442a0444c368157b2199
   - 0 :: ce2a2734998ccf791b4215a9445c2b6a
     - 0-0 :: ce2a2734998ccf791b4215a9445c2b6a
     - 0-1 :: c9ec78f6860688ae95c975843ba5f140
     - 0-2 :: 423d28970abdcd7e1c5d39448361e705
   - 1 :: f8747a492a5290bda59a38de9839e018
     - 1-0 :: f8747a492a5290bda59a38de9839e018
     - 1-1 :: d3dbfc34da5bef418549574e7ab1dafe
   - 2 :: 91ab23ca0bc0309a1ce83ad88835d1a0
     - 2-0 :: 91ab23ca0bc0309a1ce83ad88835d1a0
     - 2-1 :: 2743e83bc8f603d8c17cb18358d5ee37
   - 3 :: ce87c6ceb1ea44bb833af5a94c286cc8
     - 3-0 :: ce87c6ceb1ea44bb833af5a94c286cc8
     - 3-1 :: 
   - 4 :: ab3a8db0ae33557ac935c3928ed65287
     - 4-0 :: ab3a8db0ae33557ac935c3928ed65287
     - 4-1 :: 1508582d7c1c5191d3cf394b6c874e22
   - 5 :: fa1d93b340c79c40fcc5a89608362f38
     - 5-0 :: fa1d93b340c79c40fcc5a89608362f38
     - 5-1 :: 0f623c601c626e78fbb493cc45fa1aeb

******* COMMENT mapping old to new script

 Basically this is wildly inaccurate. I was using it as a comparison to
 update the mappings by hand, since the automated way was getting too
 screwed up.

 #+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
   import itertools as it

   lig_10_orch_hash_maps = [

       ('13326974',
       (('c9ec78f6860688ae95c975843ba5f140', '56ed345df9cfd1bef9c6ee646bcf2b5b'),
        ('423d28970abdcd7e1c5d39448361e705', 'a4bd4a55dd4fb99fe1ad86002911945f'))),

       ('13326976',
       (('91ab23ca0bc0309a1ce83ad88835d1a0', 'b2303d633641de4b6cb12eed548f87dd'),
        ('2743e83bc8f603d8c17cb18358d5ee37', 'f43da7bcb5ec077e1d71a58efb59b275'))),

       ('13326978',
       (('ab3a8db0ae33557ac935c3928ed65287', '34e64c97669757a13440f19b394622c1'),
        ('1508582d7c1c5191d3cf394b6c874e22', '342f7011e584f7a5db3428acb04b32ea'))),

       ('13326979',
       (('fa1d93b340c79c40fcc5a89608362f38', '4ecde27bd33391126d5706041b28e550'),
        ('0f623c601c626e78fbb493cc45fa1aeb', 'efd25ab7fe51c42551e754d9f6a593cd'))),

       ('13459588',
       (('f8747a492a5290bda59a38de9839e018', '5a9ca6bae5b48dad00ee452e896aa61b'),
        ('d3dbfc34da5bef418549574e7ab1dafe', '7125793c770e23d596bb82973bf5a26b'))),

       ('14786752',
       (('d3dbfc34da5bef418549574e7ab1dafe', '7125793c770e23d596bb82973bf5a26b'),
        ('47f5271ecedfe40cd6422f15780a9672', '2e6c8ad8f3bc58dbb935f1c91b32c345'))),

       ('14786753',
       (('2743e83bc8f603d8c17cb18358d5ee37', 'f43da7bcb5ec077e1d71a58efb59b275'),
        ('2adc8d74e51f51a1d11ad84ce50c135e', '5f0b05d01960da72b3c1a5a55bf408fd'))),

       ('14786754',
       (('1508582d7c1c5191d3cf394b6c874e22', '342f7011e584f7a5db3428acb04b32ea'),
        ('f0d901c1b5a4be552ef7f4f866f1d09b', 'fa74ecc7bd2e1d8ba50bb4d566003742'))),

       ('2577899',
       (('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce2a2734998ccf791b4215a9445c2b6a', '83689c1333f61c7998110ad5994ccdbf'))),

       ('5941385',
       (('654389bc9b06807113c48e08249eebc7', '513caa7123dde7c3111131207ee5228b'),
        ('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce2a2734998ccf791b4215a9445c2b6a', '83689c1333f61c7998110ad5994ccdbf'),
        ('c9ec78f6860688ae95c975843ba5f140', '94f01461847c6cb149e0a9774966e77e'))),

       ('5942786',
       (('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('f8747a492a5290bda59a38de9839e018', 'f766b95dd0699ac8d862f9c9a0a46a6f'))),

       ('5942788',
       (('654389bc9b06807113c48e08249eebc7', '513caa7123dde7c3111131207ee5228b'),
        ('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce2a2734998ccf791b4215a9445c2b6a', '83689c1333f61c7998110ad5994ccdbf'),
        ('91ab23ca0bc0309a1ce83ad88835d1a0', '673a6cc642944ce1e4b52e2e74ea901b'))),

       ('5942790',
       (('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce87c6ceb1ea44bb833af5a94c286cc8', '8893a03887c2642d3a2644f76c07ff27'))),

       ('5942793',
       (('654389bc9b06807113c48e08249eebc7', '513caa7123dde7c3111131207ee5228b'),
        ('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce2a2734998ccf791b4215a9445c2b6a', '83689c1333f61c7998110ad5994ccdbf'),
        ('ab3a8db0ae33557ac935c3928ed65287', 'b5361aab3d3c6bd34a6209dc8fca9616'))),

       ('5942795',
       (('654389bc9b06807113c48e08249eebc7', '513caa7123dde7c3111131207ee5228b'),
        ('6593f31b3db4442a0444c368157b2199', '513caa7123dde7c3111131207ee5228b'),
        ('ce2a2734998ccf791b4215a9445c2b6a', '83689c1333f61c7998110ad5994ccdbf'),
        ('fa1d93b340c79c40fcc5a89608362f38', 'b2203faf66ade63b5c3d297d2854d1cf'))),

   ]

   # just ligand 10

   lig_10_records = list(it.chain.from_iterable(
         [[(orch_hash_map[0], old, new) for old, new in orch_hash_map[1]]
                 for orch_hash_map in lig_10_orch_hash_maps]))

   # find the old hashes with multiple new ones, so we can choose one to
   # use
   lig_10_old_to_new = defaultdict(set)

   for job_id, old, new in lig_10_records:
       lig_10_old_to_new[old].add(new)

   lig_10_old_to_new_assgs = []
   for old_hash, new_hashes in lig_10_old_to_new.items():
       # just pop one off at random to use
       lig_10_old_to_new_assgs.append((old_hash, new_hashes.pop()))

   # print it out in an org-table like way
   print("\n")
   print("Ligand 10 all mappings table")
   print("| job id | old hash | new hash |")
   print(str(lig_10_records).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
   print("\n")

   print("Ligand 10 assigned mappings table")
   print("| old hash | new hash |")
   print(str(lig_10_old_to_new_assgs).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
   print("\n")

   snaphash_records.append(lig_10_records)
 #+END_SRC



******* all mappings table

 |   job id | old hash                         | new hash                         |
 |----------+----------------------------------+----------------------------------|
 | 13326974 | c9ec78f6860688ae95c975843ba5f140 | 56ed345df9cfd1bef9c6ee646bcf2b5b |
 | 13326974 | 423d28970abdcd7e1c5d39448361e705 | a4bd4a55dd4fb99fe1ad86002911945f |
 | 13326976 | 91ab23ca0bc0309a1ce83ad88835d1a0 | b2303d633641de4b6cb12eed548f87dd |
 | 13326976 | 2743e83bc8f603d8c17cb18358d5ee37 | f43da7bcb5ec077e1d71a58efb59b275 |
 | 13326978 | ab3a8db0ae33557ac935c3928ed65287 | 34e64c97669757a13440f19b394622c1 |
 | 13326978 | 1508582d7c1c5191d3cf394b6c874e22 | 342f7011e584f7a5db3428acb04b32ea |
 | 13326979 | fa1d93b340c79c40fcc5a89608362f38 | 4ecde27bd33391126d5706041b28e550 |
 | 13326979 | 0f623c601c626e78fbb493cc45fa1aeb | efd25ab7fe51c42551e754d9f6a593cd |
 | 13459588 | f8747a492a5290bda59a38de9839e018 | 5a9ca6bae5b48dad00ee452e896aa61b |
 | 13459588 | d3dbfc34da5bef418549574e7ab1dafe | 7125793c770e23d596bb82973bf5a26b |
 |  1478675 | d3dbfc34da5bef418549574e7ab1dafe | 7125793c770e23d596bb82973bf5a26b |
 |  1478675 | 47f5271ecedfe40cd6422f15780a9672 | 2e6c8ad8f3bc58dbb935f1c91b32c345 |
 | 14786753 | 2743e83bc8f603d8c17cb18358d5ee37 | f43da7bcb5ec077e1d71a58efb59b275 |
 | 14786753 | 2adc8d74e51f51a1d11ad84ce50c135e | 5f0b05d01960da72b3c1a5a55bf408fd |
 | 14786754 | 1508582d7c1c5191d3cf394b6c874e22 | 342f7011e584f7a5db3428acb04b32ea |
 | 14786754 | f0d901c1b5a4be552ef7f4f866f1d09b | fa74ecc7bd2e1d8ba50bb4d566003742 |
 |  2577899 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  2577899 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |  5941385 | 654389bc9b06807113c48e08249eebc7 | 513caa7123dde7c3111131207ee5228b |
 |  5941385 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5941385 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |  5941385 | c9ec78f6860688ae95c975843ba5f140 | 94f01461847c6cb149e0a9774966e77e |
 |  5942786 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5942786 | f8747a492a5290bda59a38de9839e018 | f766b95dd0699ac8d862f9c9a0a46a6f |
 |  5942788 | 654389bc9b06807113c48e08249eebc7 | 513caa7123dde7c3111131207ee5228b |
 |  5942788 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5942788 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |  5942788 | 91ab23ca0bc0309a1ce83ad88835d1a0 | 673a6cc642944ce1e4b52e2e74ea901b |
 |  5942790 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5942790 | ce87c6ceb1ea44bb833af5a94c286cc8 | 8893a03887c2642d3a2644f76c07ff27 |
 |  5942793 | 654389bc9b06807113c48e08249eebc7 | 513caa7123dde7c3111131207ee5228b |
 |  5942793 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5942793 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |  5942793 | ab3a8db0ae33557ac935c3928ed65287 | b5361aab3d3c6bd34a6209dc8fca9616 |
 |  5942795 | 654389bc9b06807113c48e08249eebc7 | 513caa7123dde7c3111131207ee5228b |
 |  5942795 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b |
 |  5942795 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |  5942795 | fa1d93b340c79c40fcc5a89608362f38 | b2203faf66ade63b5c3d297d2854d1cf |

******* assigned mappings table



 | old hash                         | new hash                         | redo new hash                    |
 |----------------------------------+----------------------------------+----------------------------------|
 | c9ec78f6860688ae95c975843ba5f140 | 94f01461847c6cb149e0a9774966e77e | 56ed345df9cfd1bef9c6ee646bcf2b5b |
 | 423d28970abdcd7e1c5d39448361e705 | a4bd4a55dd4fb99fe1ad86002911945f |                                  |
 | 91ab23ca0bc0309a1ce83ad88835d1a0 | 673a6cc642944ce1e4b52e2e74ea901b | b2303d633641de4b6cb12eed548f87dd |
 | 2743e83bc8f603d8c17cb18358d5ee37 | f43da7bcb5ec077e1d71a58efb59b275 |                                  |
 | ab3a8db0ae33557ac935c3928ed65287 | 34e64c97669757a13440f19b394622c1 |                                  |
 | 1508582d7c1c5191d3cf394b6c874e22 | 342f7011e584f7a5db3428acb04b32ea |                                  |
 | fa1d93b340c79c40fcc5a89608362f38 | b2203faf66ade63b5c3d297d2854d1cf | 4ecde27bd33391126d5706041b28e550 |
 | 0f623c601c626e78fbb493cc45fa1aeb | efd25ab7fe51c42551e754d9f6a593cd |                                  |
 | f8747a492a5290bda59a38de9839e018 | 5a9ca6bae5b48dad00ee452e896aa61b |                                  |
 | d3dbfc34da5bef418549574e7ab1dafe | 7125793c770e23d596bb82973bf5a26b |                                  |
 | 47f5271ecedfe40cd6422f15780a9672 | 2e6c8ad8f3bc58dbb935f1c91b32c345 |                                  |
 | 2adc8d74e51f51a1d11ad84ce50c135e | 5f0b05d01960da72b3c1a5a55bf408fd |                                  |
 | f0d901c1b5a4be552ef7f4f866f1d09b | fa74ecc7bd2e1d8ba50bb4d566003742 |                                  |
 | 6593f31b3db4442a0444c368157b2199 | 513caa7123dde7c3111131207ee5228b | 1b1ef610fddb4fd86e514eb1acd96a3f |
 | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf | 5ff88e59dff891973bd459224813a539 |
 | ce87c6ceb1ea44bb833af5a94c286cc8 | 8893a03887c2642d3a2644f76c07ff27 | 0e081557e6e3082183a14c671f76ad86 |




****** table

 | ! | run_idx | contig_ID |   job_ID |            | redo end_hash                    | state     | keep | copied | success | cycles | exits | node    | backedup | reconciled | exit weight | rate | restime | old_end_hash                     | end_hash                         |
 |---+---------+-----------+----------+------------+----------------------------------+-----------+------+--------+---------+--------+-------+---------+----------+------------+-------------+------+---------+----------------------------------+----------------------------------|
 |   |       0 |       0-0 |  2577899 |            | 5ff88e59dff891973bd459224813a539 | FINISHED  | yes  | yes    | no      |    236 |     0 | lac-287 | yes      | yes        |           0 |    0 |         | ce2a2734998ccf791b4215a9445c2b6a | 83689c1333f61c7998110ad5994ccdbf |
 |   |       1 |       0-1 |  5941385 |            | 56ed345df9cfd1bef9c6ee646bcf2b5b | FINISHED  | yes  | yes    | no      |    721 |     0 |         | yes      | yes        |           0 |    0 |         | c9ec78f6860688ae95c975843ba5f140 | 94f01461847c6cb149e0a9774966e77e |
 |   |       7 |       0-2 | 13326974 | throw away |                                  | CANCELLED | yes  | yes    | no      |     84 |     0 |         | yes      | yes        |             |      |         | 423d28970abdcd7e1c5d39448361e705 | a4bd4a55dd4fb99fe1ad86002911945f |
 |   |       2 |       1-0 |  5942786 |            | 5a9ca6bae5b48dad00ee452e896aa61b | WALLTIME  | yes  | yes    | no      |    667 |     0 | lac-196 | yes      | yes        |           0 |    0 |         | f8747a492a5290bda59a38de9839e018 | f766b95dd0699ac8d862f9c9a0a46a6f |
 |   |       8 |       1-1 | 13459588 |            |                                  | CANCELLED | yes  | yes    | no      |     36 |     0 |         | yes      | yes        |             |      |         | d3dbfc34da5bef418549574e7ab1dafe | 7125793c770e23d596bb82973bf5a26b |
 |   |      12 |       1-2 | 14786752 |            |                                  | CANCELLED | yes  | yes    | no      |    299 |     0 |         | yes      | yes        |             |      |         | 47f5271ecedfe40cd6422f15780a9672 | 2e6c8ad8f3bc58dbb935f1c91b32c345 |
 |   |       3 |       2-0 |  5942788 |            | b2303d633641de4b6cb12eed548f87dd | FINISHED  | yes  | yes    | no      |    699 |     0 | lac-195 | yes      | yes        |           0 |    0 |         | 91ab23ca0bc0309a1ce83ad88835d1a0 | 673a6cc642944ce1e4b52e2e74ea901b |
 |   |       9 |       2-1 | 13326976 |            |                                  | CANCELLED | yes  | yes    | no      |    548 |     0 |         | yes      | yes        |             |      |         | 2743e83bc8f603d8c17cb18358d5ee37 | f43da7bcb5ec077e1d71a58efb59b275 |
 |   |      13 |       2-2 | 14786753 |            |                                  | CANCELLED | yes  | yes    | no      |    183 |     0 |         | yes      | yes        |             |      |         | 2adc8d74e51f51a1d11ad84ce50c135e | 5f0b05d01960da72b3c1a5a55bf408fd |
 |   |       4 |       3-0 |  5942790 |            | 0e081557e6e3082183a14c671f76ad86 | FAILED    | yes  | yes    | no      |    727 |     0 | lac-293 | yes      | yes        |           0 |    0 |         | ce87c6ceb1ea44bb833af5a94c286cc8 | 8893a03887c2642d3a2644f76c07ff27 |
 |   |       5 |       4-0 |  5942793 |            | 34e64c97669757a13440f19b394622c1 | FINISHED  | yes  | yes    | no      |    645 |     0 | lac-289 | yes      | yes        |           0 |    0 |         | ab3a8db0ae33557ac935c3928ed65287 | b5361aab3d3c6bd34a6209dc8fca9616 |
 |   |      10 |       4-1 | 13326978 |            |                                  | CANCELLED | yes  | yes    | no      |    245 |     0 |         | yes      | yes        |             |      |         | 1508582d7c1c5191d3cf394b6c874e22 | 342f7011e584f7a5db3428acb04b32ea |
 |   |       6 |       5-0 |  5942795 |            | 4ecde27bd33391126d5706041b28e550 | FINISHED  | yes  | yes    | no      |    725 |     0 | lac-288 | yes      | yes        |           0 |    0 |         | fa1d93b340c79c40fcc5a89608362f38 | b2203faf66ade63b5c3d297d2854d1cf |
 |   |      14 |       4-2 | 14786754 |            |                                  | CANCELLED | yes  | yes    | no      |    290 |     0 |         | yes      | yes        |             |      |         | f0d901c1b5a4be552ef7f4f866f1d09b | fa74ecc7bd2e1d8ba50bb4d566003742 |
 |   |      11 |       5-1 | 13326979 |            |                                  | CANCELLED | yes  | yes    | no      |    489 |     0 |         | yes      | yes        |             |      |         | 0f623c601c626e78fbb493cc45fa1aeb | efd25ab7fe51c42551e754d9f6a593cd |
 |---+---------+-----------+----------+------------+----------------------------------+-----------+------+--------+---------+--------+-------+---------+----------+------------+-------------+------+---------+----------------------------------+----------------------------------|
 |   |         |       0-2 | 22110765 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       0-3 | 22073113 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       1-3 | 22073114 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       2-3 | 22073115 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       3-1 | 22073116 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       4-3 | 22073117 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 |   |         |       5-2 | 22073118 |            |                                  |           |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
 #+TBLFM: $18=1/$rate



****** notes
******* 5942786

 Walltime wsa reached for this one.

 The segments started taking a long time right at the end of the
 simulation...

******* 5942790

 Node failure caused this to end.

***** V2
****** tree

  start, end

  - new root :: 6a33f4a4746dc7a1e83296a727c1f8b5
  - root :: 6258387e90a2517b19adfd15ee4ba07f
    - 0
      - 0-0 :: 6d2c47827d6c6bfcc7974fda080f4378
      - 0-1 :: 0fe3b4450344a149fdf6778b013dcd79
      - 0-2 :: 7680a05e926708603aa88f30841756e2 978f37c41a8d880a45e8a35b91ab5be3
      - 0-3 :: 
    - 1
      - 1-0 :: e160f24a6155bf7664b2e6a41b8b2da7
      - 1-1 :: 9b1483a3fe8776431bec6e4863661fc0 e4c77e375e5bc2bc8e9d7357d3c16ce7
      - 1-2 :: 
    - 2
      - 2-0 :: af45df1ec10ad6286f3c7ca37401ddef
      - 2-1 :: f9d613627fdd07fd714afb6a025084e8
      - 2-2 :: 9b1483a3fe8776431bec6e4863661fc0 e4c77e375e5bc2bc8e9d7357d3c16ce7
      - 2-3 :: 
    - 3
      - 3-0 :: 8d219d1788b9c8d3c5f8f649755f1221
      - 3-1 :: 64c6723e262b80a4a99b4f69a2ebabb9
      - 3-2 :: 6522772c2320c24a5e5aac0e17ef0bb5 dc74d28358ef2361690698ff966b96db
      - 3-3 :: 
    - 4
      - 4-0 :: 1bf3e875f58e8ea7996efb2a3dbce966
      - 4-1 :: 7f24b6fe68739af9536e1eebf14d870f
      - 4-2 :: ee5515f439567f3787c87bd5ac3d63ea f54d00819eb8ff9db67336ac198853a2
      - 4-3 :: 
    - 5
      - 5-0 :: d6c98acf0b786c8dd94cf957469d0dcf
      - 5-1 :: 978f37c41a8d880a45e8a35b91ab5be3 cca05a1850609c6c488b295a36604317
      - 5-2 :: 

****** table
       :PROPERTIES:
       :TABLE_EXPORT_FILE: data/sim_management/lig_10.csv
       :TABLE_EXPORT_FORMAT: orgtbl-to-csv
       :END:

  | end_hash                         | contig_ID |   job_ID | state     | cycles | keep | success | reconciled | backedup | exits |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  | 6d2c47827d6c6bfcc7974fda080f4378 |       0-0 | 45710799 | FINISHED  |    831 | yes  | yes     | yes        |          |     0 |
  | e160f24a6155bf7664b2e6a41b8b2da7 |       1-0 | 45710800 | FINISHED  |    836 | yes  | yes     | yes        |          |     0 |
  | af45df1ec10ad6286f3c7ca37401ddef |       2-0 | 45710801 | FINISHED  |    835 | yes  | yes     | yes        |          |     0 |
  | 8d219d1788b9c8d3c5f8f649755f1221 |       3-0 | 45688333 | FINISHED  |    847 | yes  | yes     | yes        |          |     0 |
  | 1bf3e875f58e8ea7996efb2a3dbce966 |       4-0 | 45710802 | FINISHED  |    833 | yes  | yes     | yes        |          |     0 |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  | 0fe3b4450344a149fdf6778b013dcd79 |       0-1 | 49459812 | FAILED    |     50 | yes  | yes     | yes        |          |       |
  | f9d613627fdd07fd714afb6a025084e8 |       2-1 | 49415681 | FAILED    |    397 | yes  | yes     | yes        |          |       |
  | 64c6723e262b80a4a99b4f69a2ebabb9 |       3-1 | 49415682 | FAILED    |    297 | yes  | yes     | yes        |          |       |
  | 7f24b6fe68739af9536e1eebf14d870f |       4-1 | 49415683 | FAILED    |    402 | yes  | yes     | yes        |          |       |
  | d6c98acf0b786c8dd94cf957469d0dcf |       5-0 | 49459810 | FAILED    |     94 | yes  | yes     | yes        |          |       |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  | 978f37c41a8d880a45e8a35b91ab5be3 |       0-2 | 50642727 | FAILED    |   1069 | yes  | yes     | yes        |          |       |
  | e4c77e375e5bc2bc8e9d7357d3c16ce7 |       1-1 | 50642728 | CANCELLED |        | yes  | yes     | yes        |          |       |
  | e4c77e375e5bc2bc8e9d7357d3c16ce7 |       2-2 | 50642729 | CANCELLED |        | yes  | yes     | yes        |          |       |
  | dc74d28358ef2361690698ff966b96db |       3-2 | 50642730 | CANCELLED |        | yes  | yes     | yes        |          |       |
  | f54d00819eb8ff9db67336ac198853a2 |       4-2 | 50642731 | CANCELLED |        | yes  | yes     | yes        |          |       |
  | cca05a1850609c6c488b295a36604317 |       5-1 | 50642732 | CANCELLED |        | yes  | yes     | yes        |          |       |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  |                                  |       1-2 | 51003402 | RUNNING   |        |      |         |            |          |       |
  |                                  |       2-3 | 51003403 | RUNNING   |        |      |         |            |          |       |
  |                                  |       4-3 | 51003405 | RUNNING   |        |      |         |            |          |       |
  |                                  |       5-2 | 51003406 | RUNNING   |        |      |         |            |          |       |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  |                                  |       0-3 |          |           |        |      |         |            |          |       |
  |                                  |       3-3 |          |           |        |      |         |            |          |       |
  |----------------------------------+-----------+----------+-----------+--------+------+---------+------------+----------+-------|
  |                                  |       3-3 | 51003404 | FAILED    |        | no   |         |            |          |       |
  |                                  |       1-1 | 49459813 | STALLED   |    198 | no   | no      |            |          |       |
  |                                  |       0-1 | 49575547 | STALLED   |    144 | no   | no      |            |          |       |


****** notes



**** 18
***** tree

This new root snapshot is suitable for things like test runs from the
root orches. To add runs to the started datasets use the original root hash

- new root snapshot hash :: 84de06ee42824b533e2289f6962cf289

This is the tree from the simulations which were started orginally, if
you are to run simulations within this collective dataset use the
snapshots defined here. The root snapshot similar to the new root
snapshot, but somehow is a different hash from when we rebuilt things.

- root :: 31f88cf33b945c8c4d732dac6944e0d6
  - 0 
    - 0-0* :: 3ce746e9adf69469f6e0582aeae269fc
    - 0-1* :: bd8e6362dc7a4e61fc98502c5972180d
    - 0-2 :: eba2781a1f49a59133144cedcffec35b
    - 0-3 :: dacddf49609e3cc47e29daca52caa5d8
    - 0-4 :: 4bc9e83f424237c023a0cc868face740
    - 0-5 :: 
  - 1
    - 1-0 :: 91bbd7a0cc82a270123004b7f9479c9d
    - 1-1 :: 
  - 2
    - 2-0 :: fd61ace5e31482a8c07fd86ec84a1480
    - 2-1 :: 71dfce792cff9046ebd99e7f4e10a448
    - 2-2 :: 96b540d815a500374bd66d20fd7d18af
    - 2-3 :: 
  - 3
    - 3-0 :: f99b643595dffa5d54ba730506409fff
    - 3-1 :: 07614b623256fa7c1e54aa99d2b4c2d3
    - 3-2 :: 23a311b9d9cc45f5df44b929916e566a
    - 3-3 :: 
  - 4
    - 4-0 :: 0a7fa88bd1e0ef72000281d761d2559a
    - 4-1 :: 7646fd603713084c822060f9adcb011c
    - 4-2 :: b94290d18cf7c9282d7518a56fe6bd66
    - 4-3 :: 
  - 5
    - 5-0 :: 64c497938e30085ef905ca9d096da265
    - 5-1 :: bd192c5c4da3973076411f37ea1a044b
    - 5-2 :: 


****** old hashes
- root :: 6812d64700106bd832351b5e0a1ffa53
  - 0 :: 616d9451c3ad5325dcf672545a14c169
    - 0-0* :: 9146738e382e8bd242e2c3e0a697405e
    - 0-1* :: 6f312aec9af87c012bffd75184947923
    - 0-2 :: 616d9451c3ad5325dcf672545a14c169
    - 0-3 :: 5ac4e9a53d1cda46acf3da5796807eed
  - 1 :: 00a9082b272c62d28a5c724548f0583a
    - 1-0 :: 00a9082b272c62d28a5c724548f0583a
    - 1-1 :: 
  - 2 :: 72fdb473eb9eb511dc69328caf35281a
    - 2-0 :: 72fdb473eb9eb511dc69328caf35281a
    - 2-1 :: 31573b377d97b727628f2affcb53eaf4
  - 3 :: 9198cc6c498ea77037f64353beebfee5
    - 3-0 :: 9198cc6c498ea77037f64353beebfee5
    - 3-1 :: 12cb479452cef27052ad0dd20da58a04
  - 4 :: 5e0c9be0d6e118a897b525b4924339b9
    - 4-0 :: 5e0c9be0d6e118a897b525b4924339b9
    - 4-1 :: b0ef7637562d736708ed7934063e50e1
  - 5 :: e886aa40fcb5f78d8bbe521dd6769c0d
    - 5-0 :: e886aa40fcb5f78d8bbe521dd6769c0d
    - 5-1 :: 

****** mapping old to new script

#+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
  import itertools as it

  lig_18_orch_hash_maps = [

      ('2965088',
       (('9146738e382e8bd242e2c3e0a697405e', 'ceb98cebb6b8661dd3024f32f3d9099c'),
        ('6f312aec9af87c012bffd75184947923', '782b4d432ccdad1c85328733407e8c16'))),

      ('13326982',
      (('9198cc6c498ea77037f64353beebfee5', 'f99b643595dffa5d54ba730506409fff'),
       ('12cb479452cef27052ad0dd20da58a04', '07614b623256fa7c1e54aa99d2b4c2d3'))),

      ('13326983',
      (('5e0c9be0d6e118a897b525b4924339b9', '0a7fa88bd1e0ef72000281d761d2559a'),
       ('b0ef7637562d736708ed7934063e50e1', '7646fd603713084c822060f9adcb011c'))),

       ('13326981',
       (('72fdb473eb9eb511dc69328caf35281a', 'fd61ace5e31482a8c07fd86ec84a1480'), 
        ('31573b377d97b727628f2affcb53eaf4', '71dfce792cff9046ebd99e7f4e10a448'))),

      ('13327296',
      (('616d9451c3ad5325dcf672545a14c169', 'eba2781a1f49a59133144cedcffec35b'),
       ('5ac4e9a53d1cda46acf3da5796807eed', 'dacddf49609e3cc47e29daca52caa5d8'))),

      ('14699913',
      (('e886aa40fcb5f78d8bbe521dd6769c0d', '64c497938e30085ef905ca9d096da265'),
       ('6cafafc702cad795b733d0d5fdc86e4f', 'bd192c5c4da3973076411f37ea1a044b'))),

      ('14786755',
      (('5ac4e9a53d1cda46acf3da5796807eed', 'dacddf49609e3cc47e29daca52caa5d8'),
       ('6d99726e7fa5fda52fe53a004d0109fe', '4bc9e83f424237c023a0cc868face740'))),

      ('14786756',
      (('31573b377d97b727628f2affcb53eaf4', '71dfce792cff9046ebd99e7f4e10a448'),
       ('049957ca3948549f3402466193355a29', '96b540d815a500374bd66d20fd7d18af'))),

      ('14786757',
      (('12cb479452cef27052ad0dd20da58a04', '07614b623256fa7c1e54aa99d2b4c2d3'),
       ('9c2f73c2485667746c551195b9bd5577', '23a311b9d9cc45f5df44b929916e566a'))),

      ('14786758',
      (('b0ef7637562d736708ed7934063e50e1', '7646fd603713084c822060f9adcb011c'),
       ('c2312cbf4e69d54e2c59a48cb6a61458', 'b94290d18cf7c9282d7518a56fe6bd66'))),

      ('2577900',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('9146738e382e8bd242e2c3e0a697405e', 'ceb98cebb6b8661dd3024f32f3d9099c'))),

      ('5941434',
      (('6f312aec9af87c012bffd75184947923', '782b4d432ccdad1c85328733407e8c16'),
       ('616d9451c3ad5325dcf672545a14c169', '0077586e4b46577e4627f6c5130f5a61'))),

      ('5943360',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('00a9082b272c62d28a5c724548f0583a', 'a93128912b29c8be04d09bb9f72f2fc3'))),

      ('5943391',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('72fdb473eb9eb511dc69328caf35281a', '9c62ff1dde10b5909efd3c2fe7fda5b3'))),

      ('5943392',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('9198cc6c498ea77037f64353beebfee5', '3d9d40105d933ecf5d0af5541ef7cb0c'))),

      ('5943394',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('5e0c9be0d6e118a897b525b4924339b9', '48430da0a76a7de3d72654e4c52e1e7e'))),

      ('5943400',
      (('6812d64700106bd832351b5e0a1ffa53', '3af1ced8483bd4dd7cd0d83ebdeb8575'),
       ('e886aa40fcb5f78d8bbe521dd6769c0d', '95a253b3049de8d2f7ee1ccead7b72ce'))),

  ]

  # just ligand 18

  lig_18_records = list(it.chain.from_iterable(
        [[(orch_hash_map[0], old, new) for old, new in orch_hash_map[1]]
                for orch_hash_map in lig_18_orch_hash_maps]))


  # find the old hashes with multiple new ones, so we can choose one to
  # use
  lig_18_old_to_new = defaultdict(set)

  for job_id, old, new in lig_18_records:
      lig_18_old_to_new[old].add(new)

  lig_18_old_to_new_assgs = []
  for old_hash, new_hashes in lig_18_old_to_new.items():
      # just pop one off at random to use
      lig_18_old_to_new_assgs.append((old_hash, new_hashes.pop()))

  # print it out in an org-table like way
  print("\n")
  print("Ligand 18 all mappings table")
  print("| job id | old hash | new hash |")
  print(str(lig_18_records).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  print("Ligand 18 assigned mappings table")
  print("| old hash | new hash |")
  print(str(lig_18_old_to_new_assgs).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  snaphash_records.append(lig_18_records)


#+END_SRC


****** all mappings table

|   job id | old hash                         | new hash                         |
|----------+----------------------------------+----------------------------------|
| 14699913 | 72fdb473eb9eb511dc69328caf35281a | fd61ace5e31482a8c07fd86ec84a1480 |
| 14699913 | 31573b377d97b727628f2affcb53eaf4 | 71dfce792cff9046ebd99e7f4e10a448 |
| 13326982 | 9198cc6c498ea77037f64353beebfee5 | f99b643595dffa5d54ba730506409fff |
| 13326982 | 12cb479452cef27052ad0dd20da58a04 | 07614b623256fa7c1e54aa99d2b4c2d3 |
| 13326983 | 5e0c9be0d6e118a897b525b4924339b9 | 0a7fa88bd1e0ef72000281d761d2559a |
| 13326983 | b0ef7637562d736708ed7934063e50e1 | 7646fd603713084c822060f9adcb011c |
| 13327296 | 616d9451c3ad5325dcf672545a14c169 | eba2781a1f49a59133144cedcffec35b |
| 13327296 | 5ac4e9a53d1cda46acf3da5796807eed | dacddf49609e3cc47e29daca52caa5d8 |
| 14699913 | e886aa40fcb5f78d8bbe521dd6769c0d | 64c497938e30085ef905ca9d096da265 |
| 14699913 | 6cafafc702cad795b733d0d5fdc86e4f | bd192c5c4da3973076411f37ea1a044b |
| 14786755 | 5ac4e9a53d1cda46acf3da5796807eed | dacddf49609e3cc47e29daca52caa5d8 |
| 14786755 | 6d99726e7fa5fda52fe53a004d0109fe | 4bc9e83f424237c023a0cc868face740 |
| 14786756 | 31573b377d97b727628f2affcb53eaf4 | 71dfce792cff9046ebd99e7f4e10a448 |
| 14786756 | 049957ca3948549f3402466193355a29 | 96b540d815a500374bd66d20fd7d18af |
| 14786757 | 12cb479452cef27052ad0dd20da58a04 | 07614b623256fa7c1e54aa99d2b4c2d3 |
| 14786757 | 9c2f73c2485667746c551195b9bd5577 | 23a311b9d9cc45f5df44b929916e566a |
| 14786758 | b0ef7637562d736708ed7934063e50e1 | 7646fd603713084c822060f9adcb011c |
| 14786758 | c2312cbf4e69d54e2c59a48cb6a61458 | b94290d18cf7c9282d7518a56fe6bd66 |
|  2577900 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  2577900 | 9146738e382e8bd242e2c3e0a697405e | ceb98cebb6b8661dd3024f32f3d9099c |
|  5941434 | 9146738e382e8bd242e2c3e0a697405e | ceb98cebb6b8661dd3024f32f3d9099c |
|  5941434 | 6f312aec9af87c012bffd75184947923 | 782b4d432ccdad1c85328733407e8c16 |
|  5941434 | 6f312aec9af87c012bffd75184947923 | 782b4d432ccdad1c85328733407e8c16 |
|  5941434 | 616d9451c3ad5325dcf672545a14c169 | 0077586e4b46577e4627f6c5130f5a61 |
|  5943360 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  5943360 | 00a9082b272c62d28a5c724548f0583a | a93128912b29c8be04d09bb9f72f2fc3 |
|  5943391 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  5943391 | 72fdb473eb9eb511dc69328caf35281a | 9c62ff1dde10b5909efd3c2fe7fda5b3 |
|  5943392 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  5943392 | 9198cc6c498ea77037f64353beebfee5 | 3d9d40105d933ecf5d0af5541ef7cb0c |
|  5943394 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  5943394 | 5e0c9be0d6e118a897b525b4924339b9 | 48430da0a76a7de3d72654e4c52e1e7e |
|  5943400 | 6812d64700106bd832351b5e0a1ffa53 | 3af1ced8483bd4dd7cd0d83ebdeb8575 |
|  5943400 | e886aa40fcb5f78d8bbe521dd6769c0d | 95a253b3049de8d2f7ee1ccead7b72ce |

****** assigned mappings table

| old hash                          | new hash                         | redo new hash                    |
|-----------------------------------+----------------------------------+----------------------------------|
| 72fdb473eb9eb511dc69328caf35281a  | 9c62ff1dde10b5909efd3c2fe7fda5b3 | fd61ace5e31482a8c07fd86ec84a1480 |
| 31573b377d97b727628f2affcb53eaf4  | 71dfce792cff9046ebd99e7f4e10a448 |                                  |
| 9198cc6c498ea77037f64353beebfee5p | 3d9d40105d933ecf5d0af5541ef7cb0c | f99b643595dffa5d54ba730506409fff |
| 12cb479452cef27052ad0dd20da58a04  | 07614b623256fa7c1e54aa99d2b4c2d3 |                                  |
| 5e0c9be0d6e118a897b525b4924339b9  | 0a7fa88bd1e0ef72000281d761d2559a |                                  |
| b0ef7637562d736708ed7934063e50e1  | 7646fd603713084c822060f9adcb011c |                                  |
| 616d9451c3ad5325dcf672545a14c169  | 0077586e4b46577e4627f6c5130f5a61 | eba2781a1f49a59133144cedcffec35b |
| 5ac4e9a53d1cda46acf3da5796807eed  | dacddf49609e3cc47e29daca52caa5d8 |                                  |
| e886aa40fcb5f78d8bbe521dd6769c0d  | 95a253b3049de8d2f7ee1ccead7b72ce | 64c497938e30085ef905ca9d096da265 |
| 6cafafc702cad795b733d0d5fdc86e4f  | bd192c5c4da3973076411f37ea1a044b |                                  |
| 6d99726e7fa5fda52fe53a004d0109fe  | 4bc9e83f424237c023a0cc868face740 |                                  |
| 049957ca3948549f3402466193355a29  | 96b540d815a500374bd66d20fd7d18af | 96b540d815a500374bd66d20fd7d18af |
| 9c2f73c2485667746c551195b9bd5577  | 23a311b9d9cc45f5df44b929916e566a |                                  |
| c2312cbf4e69d54e2c59a48cb6a61458  | b94290d18cf7c9282d7518a56fe6bd66 |                                  |
| 6812d64700106bd832351b5e0a1ffa53  | 3af1ced8483bd4dd7cd0d83ebdeb8575 | 31f88cf33b945c8c4d732dac6944e0d6 |
| 9146738e382e8bd242e2c3e0a697405e  | ceb98cebb6b8661dd3024f32f3d9099c | 3ce746e9adf69469f6e0582aeae269fc |
| 6f312aec9af87c012bffd75184947923  | 782b4d432ccdad1c85328733407e8c16 | bd8e6362dc7a4e61fc98502c5972180d |
| 00a9082b272c62d28a5c724548f0583a  | a93128912b29c8be04d09bb9f72f2fc3 | 91bbd7a0cc82a270123004b7f9479c9d |


***** table

|---+---------+-----------+----------+--------+---------------+------+--------+---------+--------+-------+----------------------------------+----------+------------+---------+-------------+------+---------+----------------------------------+----------------------------------|
| ! | run_idx | contig_ID |   job_ID | blowup | state         | keep | copied | success | cycles | exits | redo new hash                    | backedup | reconciled | node    | exit_weight | rate | restime | end_hash                         | old_end_hash                     |
|---+---------+-----------+----------+--------+---------------+------+--------+---------+--------+-------+----------------------------------+----------+------------+---------+-------------+------+---------+----------------------------------+----------------------------------|
|   |       0 |       0-0 |  2577900 |        | FAILED (rec)  | yes  | yes    | no      |    105 |       | 3ce746e9adf69469f6e0582aeae269fc | yes      | yes        | lac-141 |             |      |         | ceb98cebb6b8661dd3024f32f3d9099c | 9146738e382e8bd242e2c3e0a697405e |
|   |       1 |       0-1 |  2965088 |        | CANCELLED rec | yes  | yes    | no      |     29 |       | bd8e6362dc7a4e61fc98502c5972180d | yes      | yes        | lac-142 |             |      |         | 782b4d432ccdad1c85328733407e8c16 | 6f312aec9af87c012bffd75184947923 |
|   |       2 |       0-2 |  5941434 |        | WALLTIME      | yes  | yes    | no      |    295 |     0 | eba2781a1f49a59133144cedcffec35b | yes      | yes        | lac-027 |           0 |    0 |         | 0077586e4b46577e4627f6c5130f5a61 | 616d9451c3ad5325dcf672545a14c169 |
|   |       8 |       0-3 | 13327296 |        | CANCELLED     | yes  | yes    | no      |    150 |     0 | dacddf49609e3cc47e29daca52caa5d8 | yes      | yes        |         |             |      |         | dacddf49609e3cc47e29daca52caa5d8 | 5ac4e9a53d1cda46acf3da5796807eed |
|   |      13 |       0-4 | 14786755 |        | CANCELLED     | yes  | yes    | no      |     59 |     0 | 4bc9e83f424237c023a0cc868face740 | yes      | yes        |         |             |      |         | 4bc9e83f424237c023a0cc868face740 | 6d99726e7fa5fda52fe53a004d0109fe |
|   |       3 |       1-0 |  5943360 |        | WALLTIME      | yes  | yes    | no      |    404 |     0 | 91bbd7a0cc82a270123004b7f9479c9d | yes      | yes        | lac-197 |           0 |    0 |         | a93128912b29c8be04d09bb9f72f2fc3 | 00a9082b272c62d28a5c724548f0583a |
|   |       4 |       2-0 |  5943391 |        | FINISHED      | yes  | yes    | no      |    727 |     0 | fd61ace5e31482a8c07fd86ec84a1480 | yes      | yes        | lac-199 |           0 |    0 |         | 9c62ff1dde10b5909efd3c2fe7fda5b3 | 72fdb473eb9eb511dc69328caf35281a |
|   |       9 |       2-1 | 13326981 |        | CANCELLED     | yes  | yes    | no      |    482 |     0 | 71dfce792cff9046ebd99e7f4e10a448 | yes      | yes        |         |             |      |         | 71dfce792cff9046ebd99e7f4e10a448 | 31573b377d97b727628f2affcb53eaf4 |
|   |      14 |       2-2 | 14786756 |        | CANCELLED     | yes  | yes    | no      |    289 |     0 | 96b540d815a500374bd66d20fd7d18af | yes      | yes        |         |             |      |         | 96b540d815a500374bd66d20fd7d18af | 049957ca3948549f3402466193355a29 |
|   |       5 |       3-0 |  5943392 |        | FINISHED      | yes  | yes    | no      |    738 |     0 | f99b643595dffa5d54ba730506409fff | yes      | yes        | lac-193 |           0 |    0 |         | 3d9d40105d933ecf5d0af5541ef7cb0c | 9198cc6c498ea77037f64353beebfee5 |
|   |      10 |       3-1 | 13326982 |        | CANCELLED     | yes  | yes    | no      |    436 |     0 | 07614b623256fa7c1e54aa99d2b4c2d3 | yes      | yes        |         |             |      |         | 07614b623256fa7c1e54aa99d2b4c2d3 | 12cb479452cef27052ad0dd20da58a04 |
|   |      15 |       3-2 | 14786757 |        | CANCELLED     | yes  | yes    | no      |    289 |     0 | 23a311b9d9cc45f5df44b929916e566a | yes      | yes        |         |             |      |         | 23a311b9d9cc45f5df44b929916e566a | 9c2f73c2485667746c551195b9bd5577 |
|   |       6 |       4-0 |  5943394 |        | FINISHED      | yes  | yes    | no      |    733 |     0 | 0a7fa88bd1e0ef72000281d761d2559a | yes      | yes        | lac-140 |           0 |    0 |         | 48430da0a76a7de3d72654e4c52e1e7e | 5e0c9be0d6e118a897b525b4924339b9 |
|   |      11 |       4-1 | 13326983 |        | CANCELLED     | yes  | yes    | no      |    464 |     0 | 7646fd603713084c822060f9adcb011c | yes      | yes        |         |             |      |         | 7646fd603713084c822060f9adcb011c | b0ef7637562d736708ed7934063e50e1 |
|   |      16 |       4-2 | 14786758 |        | CANCELLED     | yes  | yes    | no      |     59 |     0 | b94290d18cf7c9282d7518a56fe6bd66 | yes      | yes        |         |             |      |         | b94290d18cf7c9282d7518a56fe6bd66 | c2312cbf4e69d54e2c59a48cb6a61458 |
|   |       7 |       5-0 |  5943400 |        | WALLTIME      | yes  | yes    | no      |     11 |     0 | 64c497938e30085ef905ca9d096da265 | yes      | yes        | lac-139 |           0 |    0 |         | 95a253b3049de8d2f7ee1ccead7b72ce | e886aa40fcb5f78d8bbe521dd6769c0d |
|   |      12 |       5-1 | 14699913 |        | CANCELLED     | yes  | yes    | no      |    332 |     0 | bd192c5c4da3973076411f37ea1a044b | yes      | yes        |         |             |      |         | bd192c5c4da3973076411f37ea1a044b | 6cafafc702cad795b733d0d5fdc86e4f |
|---+---------+-----------+----------+--------+---------------+------+--------+---------+--------+-------+----------------------------------+----------+------------+---------+-------------+------+---------+----------------------------------+----------------------------------|
|   |         |       0-5 | 22073119 | no     | CANCELLED     |      |        |         |        |       |                                  |          |            | lac-027 |             |      |         |                                  |                                  |
|   |         |       1-1 | 22123236 | yes    | CANCELLED     |      |        |         |        |       |                                  |          |            | lac-293 |             |      |         |                                  |                                  |
|   |         |       2-3 | 22073121 | yes    | CANCELLED     |      |        |         |        |       |                                  |          |            | lac-142 |             |      |         |                                  |                                  |
|   |         |       3-3 | 22073122 | no     | CANCELLED     |      |        |         |        |       |                                  |          |            | lac-199 |             |      |         |                                  |                                  |
|   |         |       4-3 | 22123237 | yes    | CANCELLED     |      |        |         |        |       |                                  |          |            | nvl-005 |             |      |         |                                  |                                  |
|   |         |       4-3 | 22101677 | yes    | CANCELLED     |      |        |         |        |       |                                  |          |            |         |             |      |         |                                  |                                  |
|   |         |       5-2 | 22073124 | yes    | CANCELLED     |      |        |         |        |       |                                  |          |            | lac-194 |             |      |         |                                  |                                  |
|---+---------+-----------+----------+--------+---------------+------+--------+---------+--------+-------+----------------------------------+----------+------------+---------+-------------+------+---------+----------------------------------+----------------------------------|
|   |         |       1-1 | 22101676 | yes    | CANCELLED     | no   |        |         |        |       |                                  |          |            |         |             |      |         |                                  |                                  |
|   |         |       1-1 | 22073120 | yes    | CANCELLED     | no   |        |         |        |       |                                  |          |            | lac-289 |             |      |         |                                  |                                  |
|   |         |       4-3 | 22073123 | yes    | CANCELLED     | no   |        |         |        |       |                                  |          |            | lac-198 |             |      |         |                                  |                                  |

***** notes

****** 5941434

This job went over the wall time because the segments starting taking
a really long time to run.

This one notably had a very small number of total cycles in it though.


****** 5943400

This only had 11 cycles finished which is very strange.

The segment times started out okay but gradually a lot of them
becamevery slow to the level the rest were seeing... Should have had
more than 11 though, really not sure why that is all.


**** 20
***** tree
This new root snapshot is suitable for things like test runs from the
root orches. To add runs to the started datasets use the original root hash

- new root snapshot hash :: 46dc5ffd2ea9c4616a9ce75c2d1501b0

This is the tree from the simulations which were started orginally, if
you are to run simulations within this collective dataset use the
snapshots defined here. The root snapshot similar to the new root
snapshot, but somehow is a different hash from when we rebuilt things.

- root :: 243924f8b642a1b69e9ff83ef466e2d2
  - 0
    - 0-0 :: 18f87753e7646610f97b3ad801adde5b
    - 0-1 :: 3ed328662a6729de4828df4b6c25d915 * 08088e9a40eb53e98876403d2e4a1387
    - 0-2 :: 79248ae08705a5e9dc524331d5b1879a
    - 0-3 :: 77925f2d0870263ee41aad604ecaa285
    - 0-4 :: 
  - 1
    - 1-0 :: 6571e3f1d6100215c7711b1a69f06f6f
    - 1-1 :: db0db3d06713accc3e5e3befb6aa0ac9
    - 1-2 :: 65c6fc950375b4de5be05c1a81fbe777
    - 1-3 :: 
  - 2
    - 2-0 :: c5682961b7fd63b2b1d66b7601d0b49e
    - 2-1 :: 
  - 3
    - 3-0 :: a1563856befefce7970e322cd2d8740c
    - 3-1 :: fce412e170a916dd34938ef682fdad82
    - 3-2 :: 4c381d53f1e8b432f11c7ed3595f9d5e
    - 3-3 :: 
  - 4
    - 4-0 :: b5ef16134e028f176fd11177ada8ec77
    - 4-1 :: 
  - 5
    - 5-0 :: 6a058bdf469dd0bde2d121de461f28ad
    - 5-1 :: bfcac3fcea48ad651ce7aa783c10b9ad
    - 5-2 :: f6aa72c339471089ebf3829a103eab56
    - 5-3 :: 


****** old hashes

- root :: 2fcfc4f36e674ba42edfa71ced4a8595
  - 0 :: 46d469e48c99453e826f324ee36bee2d
    - 0-0 :: 46d469e48c99453e826f324ee36bee2d
    - 0-1 :: 488bf6840eb0c3e07d313e3d735a84e9
    - 0-2 :: 8e60380b318869f265598ce6f3c2d03a
  - 1 :: 36cac1e918ff24123c8371c4330c579b
    - 1-0 :: 36cac1e918ff24123c8371c4330c579b
    - 1-1 :: af51b916e340fd8f283007d44773da3c
  - 2 :: 40b780f64c331e9d736a8f30a05c473b
    - 2-0 :: 40b780f64c331e9d736a8f30a05c473b
    - 2-1 :: 
  - 3 :: 2fa1e6b19b3ca5b89c56d5534ed02bbc
    - 3-0 :: 2fa1e6b19b3ca5b89c56d5534ed02bbc
    - 3-1 :: 9c9d76a28bfa283a125cd39faed713b7
  - 4 :: 53e0a9f0d952864d7003d64121feb082
    - 4-0 :: 53e0a9f0d952864d7003d64121feb082
    - 4-1 :: 
  - 5 :: 8dcee38988335bf42a3ef9825d166166
    - 5-0 :: 8dcee38988335bf42a3ef9825d166166
    - 5-1 :: 7fe55b6b8d1063252028be7d73a646bc


****** mapping old to new script

#+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
  import itertools as it

  lig_20_orch_hash_maps = [

      ('13326985',
      (('36cac1e918ff24123c8371c4330c579b', '21d236bee8e1bac6c79af23029a8b772'),
       ('af51b916e340fd8f283007d44773da3c', 'db0db3d06713accc3e5e3befb6aa0ac9'))),

      ('13326987',
      (('2fa1e6b19b3ca5b89c56d5534ed02bbc', '59200b09f7acdb3b54085b9565def2aa'),
       ('9c9d76a28bfa283a125cd39faed713b7', 'fce412e170a916dd34938ef682fdad82'))),

      ('13326988',
      (('53e0a9f0d952864d7003d64121feb082', 'b42c0899f742b53f0bfa00a3ab756369'),
       ('3ba073023a92ed9a71bc0ac74eb6839f', '17e8193ab5ecb9db4949a53389df5303'))),

      ('13326989',
      (('8dcee38988335bf42a3ef9825d166166', '6a058bdf469dd0bde2d121de461f28ad'),
       ('7fe55b6b8d1063252028be7d73a646bc', 'bfcac3fcea48ad651ce7aa783c10b9ad'))),

      ('13327455',
      (('488bf6840eb0c3e07d313e3d735a84e9', '3ed328662a6729de4828df4b6c25d915'),
       ('8e60380b318869f265598ce6f3c2d03a', '79248ae08705a5e9dc524331d5b1879a'))),

      ('14786759',
      (('8e60380b318869f265598ce6f3c2d03a', '79248ae08705a5e9dc524331d5b1879a'),
       ('56ae763c3425e39fce34f14ca011d65a', '77925f2d0870263ee41aad604ecaa285'))),

      ('14786760',
      (('af51b916e340fd8f283007d44773da3c', 'db0db3d06713accc3e5e3befb6aa0ac9'),
       ('8065627f4fcf83c848f12e6b6465dbea', '65c6fc950375b4de5be05c1a81fbe777'))),

      ('14786761',
      (('9c9d76a28bfa283a125cd39faed713b7', 'fce412e170a916dd34938ef682fdad82'),
       ('b1f23aff5413b5138bdaea73dc9bac8e', '4c381d53f1e8b432f11c7ed3595f9d5e'))),

      ('14786763',
      (('7fe55b6b8d1063252028be7d73a646bc', 'bfcac3fcea48ad651ce7aa783c10b9ad'),
       ('5cc060d218a9aba3c26646588c78afb9', 'f6aa72c339471089ebf3829a103eab56'))),

      ('2577901',
      (('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'))),

      ('5942751',
      (('16c906e28c32641e44e499e52ecff92a', '7df4102c428d8f858429e342940483c5'),
       ('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'),
       ('488bf6840eb0c3e07d313e3d735a84e9', '08088e9a40eb53e98876403d2e4a1387'))),

      ('5943436',
      (('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('40b780f64c331e9d736a8f30a05c473b', 'a917ca540a208d988348592364ff9d5f'))),

      ('5943537',
      (('16c906e28c32641e44e499e52ecff92a', '7df4102c428d8f858429e342940483c5'),
       ('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'),
       ('2fa1e6b19b3ca5b89c56d5534ed02bbc', 'a1563856befefce7970e322cd2d8740c'))),

      ('5943613',
      (('16c906e28c32641e44e499e52ecff92a', '7df4102c428d8f858429e342940483c5'),
       ('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'),
       ('53e0a9f0d952864d7003d64121feb082', 'b5ef16134e028f176fd11177ada8ec77'))),

      ('5943737',
      (('16c906e28c32641e44e499e52ecff92a', '7df4102c428d8f858429e342940483c5'),
       ('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'),
       ('8dcee38988335bf42a3ef9825d166166', 'b7c787ab33e0392651fb13cc8872dd19'))),

      ('7630942',
      (('16c906e28c32641e44e499e52ecff92a', '7df4102c428d8f858429e342940483c5'),
       ('2fcfc4f36e674ba42edfa71ced4a8595', '7df4102c428d8f858429e342940483c5'),
       ('46d469e48c99453e826f324ee36bee2d', '7de7d11fd9e16ed1e6f015357f07ef53'),
       ('488bf6840eb0c3e07d313e3d735a84e9', '08088e9a40eb53e98876403d2e4a1387'),
       ('40b780f64c331e9d736a8f30a05c473b', 'a917ca540a208d988348592364ff9d5f'),
       ('2fa1e6b19b3ca5b89c56d5534ed02bbc', 'a1563856befefce7970e322cd2d8740c'),
       ('53e0a9f0d952864d7003d64121feb082', 'b5ef16134e028f176fd11177ada8ec77'),
       ('8dcee38988335bf42a3ef9825d166166', 'b7c787ab33e0392651fb13cc8872dd19'),
       ('36cac1e918ff24123c8371c4330c579b', '6571e3f1d6100215c7711b1a69f06f6f'))),

  ]

  # just ligand 20

  lig_20_records = list(it.chain.from_iterable(
        [[(orch_hash_map[0], old, new) for old, new in orch_hash_map[1]]
                for orch_hash_map in lig_20_orch_hash_maps]))

  # find the old hashes with multiple new ones, so we can choose one to
  # use
  lig_20_old_to_new = defaultdict(set)

  for job_id, old, new in lig_20_records:
      lig_20_old_to_new[old].add(new)

  lig_20_old_to_new_assgs = []
  for old_hash, new_hashes in lig_20_old_to_new.items():
      # just pop one off at random to use
      lig_20_old_to_new_assgs.append((old_hash, new_hashes.pop()))

  # print it out in an org-table like way
  print("\n")
  print("Ligand 20 all mappings table")
  print("| job id | old hash | new hash |")
  print(str(lig_20_records).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  print("Ligand 20 assigned mappings table")
  print("| old hash | new hash |")
  print(str(lig_20_old_to_new_assgs).replace('[', '').replace(']', '').replace("'", '').replace('(', '|').replace('),', '|\n').replace(',', '|').replace(')', '|'))
  print("\n")

  snaphash_records.append(lig_20_records)

#+END_SRC

****** all mappings table

|   job id | old hash                         | new hash                         |
|----------+----------------------------------+----------------------------------|
| 13326985 | 36cac1e918ff24123c8371c4330c579b | 21d236bee8e1bac6c79af23029a8b772 |
| 13326985 | af51b916e340fd8f283007d44773da3c | db0db3d06713accc3e5e3befb6aa0ac9 |
| 13326987 | 2fa1e6b19b3ca5b89c56d5534ed02bbc | 59200b09f7acdb3b54085b9565def2aa |
| 13326987 | 9c9d76a28bfa283a125cd39faed713b7 | fce412e170a916dd34938ef682fdad82 |
| 13326988 | 53e0a9f0d952864d7003d64121feb082 | b42c0899f742b53f0bfa00a3ab756369 |
| 13326988 | 3ba073023a92ed9a71bc0ac74eb6839f | 17e8193ab5ecb9db4949a53389df5303 |
| 13326989 | 8dcee38988335bf42a3ef9825d166166 | 6a058bdf469dd0bde2d121de461f28ad |
| 13326989 | 7fe55b6b8d1063252028be7d73a646bc | bfcac3fcea48ad651ce7aa783c10b9ad |
| 13327455 | 488bf6840eb0c3e07d313e3d735a84e9 | 3ed328662a6729de4828df4b6c25d915 |
| 13327455 | 8e60380b318869f265598ce6f3c2d03a | 79248ae08705a5e9dc524331d5b1879a |
| 14786759 | 8e60380b318869f265598ce6f3c2d03a | 79248ae08705a5e9dc524331d5b1879a |
| 14786759 | 56ae763c3425e39fce34f14ca011d65a | 77925f2d0870263ee41aad604ecaa285 |
| 14786760 | af51b916e340fd8f283007d44773da3c | db0db3d06713accc3e5e3befb6aa0ac9 |
| 14786760 | 8065627f4fcf83c848f12e6b6465dbea | 65c6fc950375b4de5be05c1a81fbe777 |
| 14786761 | 9c9d76a28bfa283a125cd39faed713b7 | fce412e170a916dd34938ef682fdad82 |
| 14786761 | b1f23aff5413b5138bdaea73dc9bac8e | 4c381d53f1e8b432f11c7ed3595f9d5e |
| 14786763 | 7fe55b6b8d1063252028be7d73a646bc | bfcac3fcea48ad651ce7aa783c10b9ad |
| 14786763 | 5cc060d218a9aba3c26646588c78afb9 | f6aa72c339471089ebf3829a103eab56 |
|  2577901 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  2577901 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  5942751 | 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 |
|  5942751 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  5942751 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  5942751 | 488bf6840eb0c3e07d313e3d735a84e9 | 08088e9a40eb53e98876403d2e4a1387 |
|  5943436 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  5943436 | 40b780f64c331e9d736a8f30a05c473b | a917ca540a208d988348592364ff9d5f |
|  5943537 | 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 |
|  5943537 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  5943537 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  5943537 | 2fa1e6b19b3ca5b89c56d5534ed02bbc | a1563856befefce7970e322cd2d8740c |
|  5943613 | 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 |
|  5943613 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  5943613 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  5943613 | 53e0a9f0d952864d7003d64121feb082 | b5ef16134e028f176fd11177ada8ec77 |
|  5943737 | 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 |
|  5943737 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  5943737 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  5943737 | 8dcee38988335bf42a3ef9825d166166 | b7c787ab33e0392651fb13cc8872dd19 |
|  7630942 | 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 |
|  7630942 | 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 |
|  7630942 | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|  7630942 | 488bf6840eb0c3e07d313e3d735a84e9 | 08088e9a40eb53e98876403d2e4a1387 |
|  7630942 | 40b780f64c331e9d736a8f30a05c473b | a917ca540a208d988348592364ff9d5f |
|  7630942 | 2fa1e6b19b3ca5b89c56d5534ed02bbc | a1563856befefce7970e322cd2d8740c |
|  7630942 | 53e0a9f0d952864d7003d64121feb082 | b5ef16134e028f176fd11177ada8ec77 |
|  7630942 | 8dcee38988335bf42a3ef9825d166166 | b7c787ab33e0392651fb13cc8872dd19 |
|  7630942 | 36cac1e918ff24123c8371c4330c579b | 6571e3f1d6100215c7711b1a69f06f6f |


****** assigned mappings table

| old hash                         | new hash                         | redo new hash                    |
|----------------------------------+----------------------------------+----------------------------------|
| 36cac1e918ff24123c8371c4330c579b | 21d236bee8e1bac6c79af23029a8b772 |                                  |
| af51b916e340fd8f283007d44773da3c | db0db3d06713accc3e5e3befb6aa0ac9 |                                  |
| 2fa1e6b19b3ca5b89c56d5534ed02bbc | 59200b09f7acdb3b54085b9565def2aa |                                  |
| 9c9d76a28bfa283a125cd39faed713b7 | fce412e170a916dd34938ef682fdad82 |                                  |
| 53e0a9f0d952864d7003d64121feb082 | b42c0899f742b53f0bfa00a3ab756369 |                                  |
| 3ba073023a92ed9a71bc0ac74eb6839f | 17e8193ab5ecb9db4949a53389df5303 |                                  |
| 8dcee38988335bf42a3ef9825d166166 | b7c787ab33e0392651fb13cc8872dd19 | 6a058bdf469dd0bde2d121de461f28ad |
| 7fe55b6b8d1063252028be7d73a646bc | bfcac3fcea48ad651ce7aa783c10b9ad |                                  |
| 488bf6840eb0c3e07d313e3d735a84e9 | 3ed328662a6729de4828df4b6c25d915 |                                  |
| 8e60380b318869f265598ce6f3c2d03a | 79248ae08705a5e9dc524331d5b1879a |                                  |
| 56ae763c3425e39fce34f14ca011d65a | 77925f2d0870263ee41aad604ecaa285 |                                  |
| 8065627f4fcf83c848f12e6b6465dbea | 65c6fc950375b4de5be05c1a81fbe777 |                                  |
| b1f23aff5413b5138bdaea73dc9bac8e | 4c381d53f1e8b432f11c7ed3595f9d5e |                                  |
| 5cc060d218a9aba3c26646588c78afb9 | f6aa72c339471089ebf3829a103eab56 |                                  |
| 2fcfc4f36e674ba42edfa71ced4a8595 | 7df4102c428d8f858429e342940483c5 | 243924f8b642a1b69e9ff83ef466e2d2 |
| 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 | 18f87753e7646610f97b3ad801adde5b |
| 16c906e28c32641e44e499e52ecff92a | 7df4102c428d8f858429e342940483c5 | 243924f8b642a1b69e9ff83ef466e2d2 |
| 40b780f64c331e9d736a8f30a05c473b | a917ca540a208d988348592364ff9d5f | c5682961b7fd63b2b1d66b7601d0b49e |



***** table


| ! | run_idx | contig_ID |   job_ID | blowup? | job_state | redo end_hash                    | keep | copied | success | cycles | exits | node    | backedup | reconciled | exit_weight | rate | restime | old_end_hash                     | end_hash                         |
|---+---------+-----------+----------+---------+-----------+----------------------------------+------+--------+---------+--------+-------+---------+----------+------------+-------------+------+---------+----------------------------------+----------------------------------|
|   |       0 |       0-0 |  2577901 |         | FINISHED  | 18f87753e7646610f97b3ad801adde5b | yes  | yes    | no      |    232 |     0 | lac-289 | yes      | yes        |           0 |    0 |         | 46d469e48c99453e826f324ee36bee2d | 7de7d11fd9e16ed1e6f015357f07ef53 |
|   |       1 |       0-1 |  5942751 |         | FINISHED  | 3ed328662a6729de4828df4b6c25d915 | yes  | yes    | no      |    756 |     0 | lac-028 | yes      | yes        |           0 |    0 |         | 488bf6840eb0c3e07d313e3d735a84e9 | 08088e9a40eb53e98876403d2e4a1387 |
|   |       7 |       0-2 | 13327455 |         | CANCELLED | 79248ae08705a5e9dc524331d5b1879a | yes  | yes    | no      |    153 |     0 |         | yes      | yes        |             |      |         | 8e60380b318869f265598ce6f3c2d03a | 79248ae08705a5e9dc524331d5b1879a |
|   |      12 |       0-3 | 14786759 |         | CANCELLED | 77925f2d0870263ee41aad604ecaa285 | yes  | yes    | no      |    238 |     0 |         | yes      | yes        |             |      |         | 56ae763c3425e39fce34f14ca011d65a | 77925f2d0870263ee41aad604ecaa285 |
|   |       2 |       1-0 |  7630942 |         | FINISHED  | 6571e3f1d6100215c7711b1a69f06f6f | yes  | yes    | no      |    730 |     0 | lac-027 | yes      | yes        |           0 |    0 |         | 36cac1e918ff24123c8371c4330c579b | 6571e3f1d6100215c7711b1a69f06f6f |
|   |       8 |       1-1 | 13326985 |         | CANCELLED | db0db3d06713accc3e5e3befb6aa0ac9 | yes  | yes    | no      |    446 |     0 |         | yes      | yes        |             |      |         | af51b916e340fd8f283007d44773da3c | db0db3d06713accc3e5e3befb6aa0ac9 |
|   |      13 |       1-2 | 14786760 |         | CANCELLED | 65c6fc950375b4de5be05c1a81fbe777 | yes  | yes    | no      |    287 |     0 |         | yes      | yes        |             |      |         | 8065627f4fcf83c848f12e6b6465dbea | 65c6fc950375b4de5be05c1a81fbe777 |
|   |       3 |       2-0 |  5943436 |         | FAILED    | c5682961b7fd63b2b1d66b7601d0b49e | yes  | yes    | no      |    710 |     0 | lac-141 | yes      | yes        |           0 |    0 |         | 40b780f64c331e9d736a8f30a05c473b | a917ca540a208d988348592364ff9d5f |
|   |       4 |       3-0 |  5943537 |         | FINISHED  | a1563856befefce7970e322cd2d8740c | yes  | yes    | no      |    743 |     0 | lac-029 | yes      | yes        |           0 |    0 |         | 2fa1e6b19b3ca5b89c56d5534ed02bbc | a1563856befefce7970e322cd2d8740c |
|   |       9 |       3-1 | 13326987 |         | CANCELLED | fce412e170a916dd34938ef682fdad82 | yes  | yes    | no      |    464 |     0 |         | yes      | yes        |             |      |         | 9c9d76a28bfa283a125cd39faed713b7 | fce412e170a916dd34938ef682fdad82 |
|   |      14 |       3-2 | 14786761 |         | CANCELLED | 4c381d53f1e8b432f11c7ed3595f9d5e | yes  | yes    | no      |    201 |     0 |         | yes      | yes        |             |      |         | b1f23aff5413b5138bdaea73dc9bac8e | 4c381d53f1e8b432f11c7ed3595f9d5e |
|   |       5 |       4-0 |  5943613 |         | FINISHED  | b42c0899f742b53f0bfa00a3ab756369 | yes  | yes    | no      |    747 |     0 | lac-025 | yes      | yes        |           0 |    0 |         | 53e0a9f0d952864d7003d64121feb082 | b5ef16134e028f176fd11177ada8ec77 |
|   |      10 |       4-1 | 13326988 |         | CANCELLED | 17e8193ab5ecb9db4949a53389df5303 | yes  | yes    | no      |    557 |     0 |         | yes      | yes        |             |      |         | 3ba073023a92ed9a71bc0ac74eb6839f | 17e8193ab5ecb9db4949a53389df5303 |
|   |      11 |       5-1 | 13326989 |         | CANCELLED | bfcac3fcea48ad651ce7aa783c10b9ad | yes  | yes    | no      |    153 |     0 |         | yes      | yes        |             |      |         | 7fe55b6b8d1063252028be7d73a646bc | bfcac3fcea48ad651ce7aa783c10b9ad |
|   |       6 |       5-0 |  5943737 |         | FINISHED  | 6a058bdf469dd0bde2d121de461f28ad | yes  | yes    | no      |    724 |     0 | lac-290 | yes      | yes        |           0 |    0 |         | 8dcee38988335bf42a3ef9825d166166 | b7c787ab33e0392651fb13cc8872dd19 |
|   |      15 |       5-2 | 14786763 |         | CANCELLED | f6aa72c339471089ebf3829a103eab56 | yes  | yes    | no      |    181 |     0 |         | yes      | yes        |             |      |         | 5cc060d218a9aba3c26646588c78afb9 | f6aa72c339471089ebf3829a103eab56 |
|---+---------+-----------+----------+---------+-----------+----------------------------------+------+--------+---------+--------+-------+---------+----------+------------+-------------+------+---------+----------------------------------+----------------------------------|
|   |         |       0-4 | 22073125 |         | RUNNING   |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|   |         |       1-3 | 22073126 |         | RUNNING   |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|   |         |       2-1 | 22073127 | yes     | CANCELLED |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|   |         |       3-3 | 22073128 |         | RUNNING   |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|   |         |       4-1 | 22160310 |         | RUNNING   |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|   |         |       5-3 | 22073130 |         | RUNNING   |                                  |      |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
|---+---------+-----------+----------+---------+-----------+----------------------------------+------+--------+---------+--------+-------+---------+----------+------------+-------------+------+---------+----------------------------------+----------------------------------|
|   |         |       4-1 | 22073129 |         | FAILED    |                                  | no   |        |         |        |       |         |          |            |             |      |         |                                  |                                  |
#+TBLFM: $18=1/$rate::@6$11=vsum($)

***** notes
****** 5942786

Walltime wsa reached for this one.

The segments started taking a long time right at the end of the
simulation...

****** 5942790

**** all

#+BEGIN_SRC python :tangle hpcc/scripts/new_orch_hash_map.py
  # all of them
  all_snaphash_records = it.chain.from_iterable(snaphash_records)

  # lig_3_old_to_new = defaultdict(set)

  # for job_id, old, new in lig_3_records:
  #     lig_3_old_to_new[old].add(new)

  # for old_hash, new_hashes in lig_3_old_to_new.items():
  #     if len(new_hashes) > 1:
  #         print("Multiple new hashes for: ", old_hash)
  #         for new_hash in new_hashes:
  #             print(new_hash)
  #         print("\n")
#+END_SRC


* COMMENT Scrapyard

Things you don't want to throw away but you don't want to keep in the
clean sections above.

** TODO COMMENT Traversal Traj

Make a trajectory that is a preorder depth-first traversal of a contig
which will give a single way of looking at the whole thing.

#+begin_src python :tangle src/seh_pathway_hopping/_tasks.py
  import os
  import os.path as osp
  import pickle

  import mdtraj as mdj

  from run.setup_paths import data_dir, traversal_traj_dir, md_systems_dir, top_dir

  from run.load_results import gexps_contigtree
  from run.get_spans_data import lig_spans_data
  from run.func_recenter_superimpose_bs import recenter_superimpose_traj

  for lig_id, lig_span_data in lig_spans_data.items():
      print("Ligand: {}".format(lig_id))

      contigtree = dict(gexps_contigtree)[lig_id]

      # make and get path to the directory to save this ligands
      # exit trajectories
      traversal_traj_dir = osp.join(traversal_traj_dir, str(lig_id))
      os.makedirs(traversal_traj_dir, exist_ok=True)

      # remove the existing files in this dir
      for filename in os.listdir(traversal_traj_dir):
          os.remove(osp.join(traversal_traj_dir, filename))

      for span_id, span_data in lig_span_data.items():

          print("Span: {}".format(span_id))

          trace = span_data['preorder_traversal_trace']
          # get the raw trace fields for the trajectories
          trace_fields = contigtree.wepy_h5.get_trace_fields(trace[0:10000], ['positions', 'box_vectors'])

          pos, centered_ref_pos = recenter_superimpose_traj(trace_fields, lig_id, 'main_rep')
          trace_fields['positions'] = pos

          # then make the mdtraj trajectory
          traj = contigtree.wepy_h5.traj_fields_to_mdtraj(trace_fields)

          traj_filename = "span-{}_traversal_lig-{}.dcd".format(span_id, lig_id)
          traj_path = osp.join(traversal_traj_dir, traj_filename)

          # write the trajectory
          traj.save_dcd(traj_path)

          break
#+end_src




***** Prefect Pipeline






** Prefect Workflows

*** Header

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_pipelines.py
  """Generated file from the analysis.org file. Do not edit directly."""
  from copy import copy
  import inspect

  from prefect import Flow
  import prefect

  import seh_pathway_hopping._tasks as tasks_module

  # these helper functions are for automatically listing all of the
  # functions defined in the tasks module
  def is_mod_function(mod, func):
      return inspect.isfunction(func) and inspect.getmodule(func) == mod

  def get_functions(mod):

      # get only the functions that aren't module functions and that
      # aren't private
      return {func.__name__ : func for func in mod.__dict__.values()
              if (is_mod_function(mod, func) and
                  not func.__name__.startswith('_')) }

  # get the task functions and wrap them as prefect tasks
  tasks = {name : prefect.task(func)
           for name, func in get_functions(tasks_module).items()}

#+END_SRC


*** Flows

**** Test

#+BEGIN_SRC python :tangle src/seh_pathway_hopping/_pipelines.py

  @prefect.task
  def say_hello(person: str) -> None:
      print("hello world, {}".format(person))

  @prefect.task
  def add(x, y=1):
      return x + y

  test_flow = prefect.Flow("flow for testing")

  with test_flow as flow:
      a = add(1, 2)
      b = add(100, 100)
      c = say_hello(str(a))
      d = say_hello(str(b))

#+END_SRC


** Scratch

** Analysis

** Invocation

* COMMENT Local Variables

# Local Variables:
# mode: org
# org-table-export-default-format: orgtbl-to-csv
# End:
